[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ITSC 2214 - Data Structures and Algorithms - Fall 2023",
    "section": "",
    "text": "Preface\nThis website contains a set of readings for ITSC 2214 - Data Structures and Algorithms.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "labs/00_installing_req_software.html",
    "href": "labs/00_installing_req_software.html",
    "title": "1  Installing Required Software",
    "section": "",
    "text": "1.1 One-Liner Install (Experimental)\nYou will need to install the following software on your laptop to work on labs in this course -",
    "crumbs": [
      "Programming Labs",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installing Required Software</span>"
    ]
  },
  {
    "objectID": "labs/00_installing_req_software.html#one-liner-install-experimental",
    "href": "labs/00_installing_req_software.html#one-liner-install-experimental",
    "title": "1  Installing Required Software",
    "section": "",
    "text": "1.1.1 Windows\nFollow the instructions in the first comment on this gist - https://gist.github.com/DhruvDh/71d90635a528db1318a36ea35da6d56d\nTo run the script on Windows:\n\nOpen a PowerShell window.\nGo to the gist and click the Raw button next to the file name at the top.\nCopy the URL from your browser’s address bar (this is the PASTE_RAW_URL).\nReplace PASTE_RAW_URL in the command below with the URL you copied.\nRun the command in PowerShell:\n\nInvoke-RestMethod -Uri PASTE_RAW_URL | iex\nFor the current version (at the time of writing), you can use this command:\nInvoke-RestMethod -Uri https://gist.githubusercontent.com/DhruvDh/71d90635a528db1318a36ea35da6d56d/raw/38aa59f4c8ee3698dd2f266321c74f81c171858e/lab-prep-windows.ps1 | iex\n\n\n1.1.2 MacOS\nFollow the instructions in the first comment on this gist - https://gist.github.com/DhruvDh/f313bf5edd929949a31995e247e28c53\nTo run the script on macOS:\n\nOpen a Terminal window.\nGo to the gist and click the Raw button next to the file name at the top.\nCopy the URL from your browser’s address bar (this is the PASTE_RAW_URL).\nReplace PASTE_RAW_URL in the command below with the URL you copied.\nRun the command in Terminal:\n\ncurl -sL PASTE_RAW_URL | bash\nFor the current version (at the time of writing), you can use this command:\ncurl -sL https://gist.githubusercontent.com/DhruvDh/f313bf5edd929949a31995e247e28c53/raw/8be0c2ff7f8e28763944497e41114a69492e3a9f/lab-prep-macos.sh | bash\nThis will download and execute the script, installing all necessary software and configurations for your lab preparation.",
    "crumbs": [
      "Programming Labs",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installing Required Software</span>"
    ]
  },
  {
    "objectID": "labs/00_installing_req_software.html#ide---visual-studio-code",
    "href": "labs/00_installing_req_software.html#ide---visual-studio-code",
    "title": "1  Installing Required Software",
    "section": "1.2 IDE - Visual Studio Code",
    "text": "1.2 IDE - Visual Studio Code\nDownload and install Visual Studio Code from here.\nPlease go through the following videos to get familiar with Visual Studio Code - https://code.visualstudio.com/docs\nThen, install the following extensions -\n\nhttps://marketplace.visualstudio.com/items?itemName=vscjava.vscode-java-pack\nhttps://marketplace.visualstudio.com/items?itemName=MS-vsliveshare.vsliveshare\nhttps://marketplace.visualstudio.com/items?itemName=Codeium.codeium\n\nFinally, you may be collaborating in labs with other students. Please go to https://code.visualstudio.com/learn/collaboration/live-share and learn about how to do the same.",
    "crumbs": [
      "Programming Labs",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installing Required Software</span>"
    ]
  },
  {
    "objectID": "labs/00_installing_req_software.html#windows-1",
    "href": "labs/00_installing_req_software.html#windows-1",
    "title": "1  Installing Required Software",
    "section": "1.3 Windows",
    "text": "1.3 Windows\n\n1.3.1 Git\n\nWe will be using scoop again to install git. Ensure you’ve gone through the previous steps. Copy and paste the following command into the terminal and press enter:\n\nscoop install git\n\nVerify that git is installed. Copy and paste the following command into a NEW terminal window and press enter:\n\ngit --version\nIf you don’t get an error, you should be good to go.\n\n\n1.3.2 Java (JDK 17)\nWe will be using a “package manager” to install the software we need, called scoop. A package manager is a tool that allows you to install software from the command line. It is similar to the apt package manager on Ubuntu, or brew on MacOS.\n\nTo start, please open a terminal by opening the start menu, typing Terminal, and selecting the Windows Terminal app.\n\nYou should see a Powershell window open up. If you see a different shell, please click the down arrow in the top bar and select Powershell. You should see a prompt like this:\nPS C:\\Users\\yourusername&gt;\n\nIf you see a different prompt, please let the instructor know.\n\nNow, copy and paste the following command into the terminal and press enter:\nSet-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser\nThis will allow the “current user” (you) to run scripts that you download from the internet.\n\nNext, copy and paste the following command into the terminal and press enter:\n\nInvoke-RestMethod -Uri https://get.scoop.sh | Invoke-Expression\nThis will download and run the installer for the scoop package manager.\n\nWe will now add a “bucket” to scoop, which is a “bucket” of software scoop can install. Java is not in the default “main” bucket. Copy and paste the following command into the terminal and press enter:\n\nscoop bucket add java\n\nNow, we can install Java. Copy and paste the following command into the terminal and press enter:\n\nscoop install openjdk\n\nFinally, we can verify that Java is installed. Copy and paste the following command into a NEW terminal window and press enter:\n\njavac --version\nIf you don’t get an error, you should be good to go.\n\n\n1.3.3 Autograder\n\nWe will need to install a compiler for a different programming language, called rust. The compiler will download, compile, and finally install the autograder. Before we begin, we need to install a few dependencies. Copy and paste the following command into the terminal and press enter:\n\nscoop install mingw\n\nNow, let’s install rust. Go to https://rustup.rs/ and download and run rustup-init.exe. This will open a terminal window, and you should see a window that asks you whether or not you want to install pre-requisites. The third option should say Don’t install pre-requisites and that is the one you choose by pressing 3 and then Enter.\n\ninfo: downloading installer\n\nWelcome to Rust!\n\nThis will download and install the official compiler for the Rust\nprogramming language, and its package manager, Cargo.\n\n...\n...\n\nCurrent installation options:\n\n\n  default host triple: ...\n    default toolchain: ...\n               profile: ...\n  modify PATH variable: ...\n\n1) Proceed with installation (default)\n2) Customize installation\n3) Cancel installation\n\nThis is a CLI menu-driven installer. Please configure it as follows, by first pressing 2 to customize installation.\n\nYou should then see -\nI'm going to ask you the value of each of these installation options.\nYou may simply press the Enter key to leave unchanged.\n\nDefault host triple? [aarch64-apple-darwin]\n\nPlease type x86_64-pc-windows-gnu and press enter.\n\nYou should then see -\nDefault toolchain? [stable]\n\nPlease type nightly and press enter.\n\nYou should then see -\nProfile (minimal default, default, complete)? [default]\n\nPlease type minimal and press enter.\n\nYou should then see -\nModify PATH variable? (y/n) [y]\n\nPlease type y and press enter.\n\nYou should then see -\nCurrent installation options:\n\n default host triple: x86_64-pc-windows-gnu\n   default toolchain: nightly\n              profile: minimal\nmodify PATH variable: yes\n\n1) Proceed with installation (default)\n2) Customize installation\n3) Cancel installation\n\n&gt; \n\nPlease press enter or 1 to proceed with installation as configured.\nOpen a NEW terminal window, type cargo --version and press enter. You should see something similar to -\n\ncargo 1.77.0-nightly (2ce45605d 2024-01-04)\nThe version number may be different, but as long as you don’t see an error, you should be good to go.\n\nFinally, we can install the autograder. Copy and paste the following command into the terminal and press enter:\n\ncargo install --git=https://github.com/DhruvDh/umm.git\n\nYou can now run umm --help to check if it was installed correctly. If you don’t see any errors, you should be good to go.",
    "crumbs": [
      "Programming Labs",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installing Required Software</span>"
    ]
  },
  {
    "objectID": "labs/00_installing_req_software.html#macos-1",
    "href": "labs/00_installing_req_software.html#macos-1",
    "title": "1  Installing Required Software",
    "section": "1.4 MacOS",
    "text": "1.4 MacOS\n\n1.4.1 Java (JDK 17)\nWe will be using a “package manager” to install the software we need, called brew. A package manager is a tool that allows you to install software from the command line. It is similar to the apt package manager on Ubuntu, or scoop on Windows.\n\nTo start, please open a terminal by opening the spotlight search and typing Terminal and selecting the Terminal app.\n\nYou should see a terminal window open up. You should see a prompt like this:\nyourusername@yourcomputername ~ %\nIf you see a different prompt, please let the instructor know.\n\nBefore we begin, we need to install a few dependencies. Copy and paste the following command into the terminal and press enter:\n\nxcode-select --install\n\nNext, copy and paste the following command into the terminal and press enter:\n\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nThis will download and run the installer for the brew package manager.\nOnce the installation is complete, we need to add brew to the path, so all future terminals will know where to find it. You can do this by running -\necho 'eval \"$(/opt/homebrew/bin/brew shellenv)\"' &gt;&gt; ~/.zprofile\nFollowed by -\neval \"$(/opt/homebrew/bin/brew shellenv)\"\n\nNex, try opening a new terminal window and typing brew --version` and pressing enter. You should see something similar to -\n\nHomebrew 3.2.12\nHomebrew/homebrew-core (git revision 3c2b; last commit 2021-10-13)\nThe version number may be different, but as long as you don’t see an error, you should be good to go.\n\nWe will now install Java. Copy and paste the following command into the terminal and press enter:\n\nbrew install openjdk\n\nFinally, we can verify that Java is installed. Copy and paste the following command into a NEW terminal window and press enter:\n\njavac --version\nIf you don’t get an error, you should be good to go.\n\n\n1.4.2 Git\n\nWe will be using brew again to install git. Ensure you’ve gone through the previous steps. Copy and paste the following command into the terminal and press enter:\n\nbrew install git\n\nVerify that git is installed. Copy and paste the following command into a NEW terminal window and press enter:\n\ngit --version\nIf you don’t get an error, you should be good to go.\n\n\n1.4.3 Autograder\n\nWe will need to install a compiler for a different programming language, called rust. The compiler will download, compile, and finally install the autograder.\nNow, let’s install rust. You can do so by running -\n\ncurl https://sh.rustup.rs -sSf | sh -s -- --default-toolchain nightly\n\nAfter that, you need to add rust to path, so all future terminals will know where to find it. You can do this by running -\n\nsource $HOME/.cargo/env\n\nOpen a NEW terminal window, type cargo --version and press enter. You should see something similar to -\n\ncargo 1.77.0-nightly (2ce45605d 2024-01-04)\nThe version number may be different, but as long as you don’t see an error, you should be good to go.\n\nFinally, we can install the autograder. Copy and paste the following command into the terminal and press enter:\n\ncargo install --git=https://github.com/DhruvDh/umm.git\n\nYou can now run umm --help to check if it was installed correctly. If you don’t see any errors, you should be good to go.",
    "crumbs": [
      "Programming Labs",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installing Required Software</span>"
    ]
  },
  {
    "objectID": "labs/01_using_autograder.html",
    "href": "labs/01_using_autograder.html",
    "title": "2  Using the Autograder",
    "section": "",
    "text": "2.1 Installing Required Software\nThese instructions will guide you on how to use the autograder for all your labs. The autograder helps you check your code and provides feedback on your assignments. Follow these steps for a smooth grading experience.\nEnsure you have all the necessary software installed. Follow the instructions in the Installing Required Software chapter or the instructions provided in class.",
    "crumbs": [
      "Programming Labs",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Using the Autograder</span>"
    ]
  },
  {
    "objectID": "labs/01_using_autograder.html#downloading-project-files",
    "href": "labs/01_using_autograder.html#downloading-project-files",
    "title": "2  Using the Autograder",
    "section": "2.2 Downloading Project Files",
    "text": "2.2 Downloading Project Files\nFor each lab, you will be provided with a link to download the project files. Unzip these files into a designated folder, preferably this folder will be placed inside a ITSC 2214 folder for all labs for this class.",
    "crumbs": [
      "Programming Labs",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Using the Autograder</span>"
    ]
  },
  {
    "objectID": "labs/01_using_autograder.html#opening-the-project-in-visual-studio-code",
    "href": "labs/01_using_autograder.html#opening-the-project-in-visual-studio-code",
    "title": "2  Using the Autograder",
    "section": "2.3 Opening the Project in Visual Studio Code",
    "text": "2.3 Opening the Project in Visual Studio Code\n\nOpen Visual Studio Code.\nOpen the Project Folder:\n\nGo to File &gt; Open Folder.\nSelect the folder where you extracted the project files.",
    "crumbs": [
      "Programming Labs",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Using the Autograder</span>"
    ]
  },
  {
    "objectID": "labs/01_using_autograder.html#writing-your-code",
    "href": "labs/01_using_autograder.html#writing-your-code",
    "title": "2  Using the Autograder",
    "section": "2.4 Writing Your Code",
    "text": "2.4 Writing Your Code\nEach lab will contain specific instructions on what code you need to write.",
    "crumbs": [
      "Programming Labs",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Using the Autograder</span>"
    ]
  },
  {
    "objectID": "labs/01_using_autograder.html#checking-autograder-feedback",
    "href": "labs/01_using_autograder.html#checking-autograder-feedback",
    "title": "2  Using the Autograder",
    "section": "2.5 Checking Autograder Feedback",
    "text": "2.5 Checking Autograder Feedback\nYou can check the feedback from the autograder using the following steps:\nUsing the Terminal:\n\nOpen a terminal in Visual Studio Code.\nRun the command: umm grade ./script.rhai\n\nUsing the Command Palette:\n\nOpen the command palette with Ctrl + Shift + P on Windows or Cmd + Shift + P on Mac.\nType “run task” and select “Tasks: Run Task”. (In future use, you can select “Tasks: Rerun Last Task”.)\nSelect the “Grade Assignment” task.\nChoose script.rhai and press Enter.\n\nNote: The first few labs might not have tasks correctly configured.",
    "crumbs": [
      "Programming Labs",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Using the Autograder</span>"
    ]
  },
  {
    "objectID": "labs/01_using_autograder.html#viewing-generated-feedback",
    "href": "labs/01_using_autograder.html#viewing-generated-feedback",
    "title": "2  Using the Autograder",
    "section": "2.6 Viewing Generated Feedback",
    "text": "2.6 Viewing Generated Feedback\nAfter running the grading command, the autograder will provide feedback on the tests that have failed, and tell if you have been penalized for any reason. You can view the feedback in the terminal or the feedback folder in the project directory.\nIt will also generate links that will further explain the reason for penalty, or explain the test that failed. You can click on these links to view the detailed feedback. Here is an example of a generated feedback link: https://feedback.dhruvdh.com/be5593a1-2b8f-49fc-a4ec-981095ae0702",
    "crumbs": [
      "Programming Labs",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Using the Autograder</span>"
    ]
  },
  {
    "objectID": "labs/01_using_autograder.html#creating-a-submission",
    "href": "labs/01_using_autograder.html#creating-a-submission",
    "title": "2  Using the Autograder",
    "section": "2.7 Creating a Submission",
    "text": "2.7 Creating a Submission\nWhen you are ready to submit your assignment:\n\nRun the command: umm create-submission in the terminal.\nThis will generate a zip file with a name like submission-2024-05-20-07-50-41.zip.\nSubmit the generated zip file to Gradescope.",
    "crumbs": [
      "Programming Labs",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Using the Autograder</span>"
    ]
  },
  {
    "objectID": "labs/01_using_autograder.html#additional-resources",
    "href": "labs/01_using_autograder.html#additional-resources",
    "title": "2  Using the Autograder",
    "section": "2.8 Additional Resources",
    "text": "2.8 Additional Resources\n\nAutograder Commands: The autograder provides various commands to help you manage your project. You can see a list of available commands by running umm --help in the terminal.",
    "crumbs": [
      "Programming Labs",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Using the Autograder</span>"
    ]
  },
  {
    "objectID": "labs/01_using_autograder.html#ai-feedback",
    "href": "labs/01_using_autograder.html#ai-feedback",
    "title": "2  Using the Autograder",
    "section": "2.9 AI Feedback",
    "text": "2.9 AI Feedback\nWhen you check your code with the autograder, you will receive feedback from an AI teaching assistant. If you are not satisfied with the response, you can request new feedback by clicking on one of the buttons that best describes your situation. These buttons include options like:\n\nYour suggestions are too broad and vague.\nI don’t understand.\nYour suggestions work, but I don’t understand why.\nThe changes you suggested are unnecessary or already implemented in my submission.\nRequest alternate explanation/solution.\nThis explanation seems incorrect.\n\nYou can also provide additional notes in the text box below these buttons. This interaction helps tailor the feedback to better suit your needs.\nHere is an example of the feedback interface: https://feedback.dhruvdh.com/be5593a1-2b8f-49fc-a4ec-981095ae0702\nThe AI feedback generator will prioritize key issues to help you make progress, provide relevant examples, and encourage critical thinking. It will avoid sharing direct solutions to foster independent problem-solving skills.",
    "crumbs": [
      "Programming Labs",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Using the Autograder</span>"
    ]
  },
  {
    "objectID": "labs/02_1l_java_ide_practice-1.html",
    "href": "labs/02_1l_java_ide_practice-1.html",
    "title": "3  1L. Java and IDE Practice - 1",
    "section": "",
    "text": "3.1 Prerequisites\nPlease follow the instructions from the Installing Required Software textbook chapter or the instructions presented in class.\nAlso, please go through the Using the Autograder to understand how to use the autograder for your labs.",
    "crumbs": [
      "Programming Labs",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1L. Java and IDE Practice - 1</span>"
    ]
  },
  {
    "objectID": "labs/02_1l_java_ide_practice-1.html#project-files",
    "href": "labs/02_1l_java_ide_practice-1.html#project-files",
    "title": "3  1L. Java and IDE Practice - 1",
    "section": "3.2 Project Files",
    "text": "3.2 Project Files\nDownload the project files from here",
    "crumbs": [
      "Programming Labs",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1L. Java and IDE Practice - 1</span>"
    ]
  },
  {
    "objectID": "labs/02_1l_java_ide_practice-1.html#opening-in-visual-studio-code",
    "href": "labs/02_1l_java_ide_practice-1.html#opening-in-visual-studio-code",
    "title": "3  1L. Java and IDE Practice - 1",
    "section": "3.3 Opening in Visual Studio Code",
    "text": "3.3 Opening in Visual Studio Code\n\nUnzip the files to preferably an ITSC 2214 folder for this class.\nOpen Visual Studio Code.\nGo to File &gt; Open Folder. Choose the folder where you extracted the files.",
    "crumbs": [
      "Programming Labs",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1L. Java and IDE Practice - 1</span>"
    ]
  },
  {
    "objectID": "labs/02_1l_java_ide_practice-1.html#description",
    "href": "labs/02_1l_java_ide_practice-1.html#description",
    "title": "3  1L. Java and IDE Practice - 1",
    "section": "3.4 Description",
    "text": "3.4 Description\nThere is a Main class with 12 methods. Methods are called part0, part1, etc.\nThe body of each method has comments that explain what the method needs to return. Your job is to write code to make sure it returns the correct value. There are tests you can run that will verify your method is working properly. Your grade will be determined by how many tests you pass. Each test or part is worth ~6 points.",
    "crumbs": [
      "Programming Labs",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1L. Java and IDE Practice - 1</span>"
    ]
  },
  {
    "objectID": "labs/02_1l_java_ide_practice-1.html#checking-autograder-feedback",
    "href": "labs/02_1l_java_ide_practice-1.html#checking-autograder-feedback",
    "title": "3  1L. Java and IDE Practice - 1",
    "section": "3.5 Checking Autograder Feedback",
    "text": "3.5 Checking Autograder Feedback\n\nOpen a terminal.\nRun the command: umm grade ./script.rhai\n\nOR\n\nOpen the command palette with Ctrl + Shift + P on Windows and Cmd + Shift + P on Mac.\nType “run task” and select “Tasks: Run Task”. (or “Tasks: Rerun Last Task” in the future).\nSelect the “Grade Assignment” task. Use the other ones as needed but avoid the “Update” task unless told to do so.\nSelect “script.rhai” and press Enter.\n\nYou may not have tasks correctly configured for the first few labs.",
    "crumbs": [
      "Programming Labs",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1L. Java and IDE Practice - 1</span>"
    ]
  },
  {
    "objectID": "labs/02_1l_java_ide_practice-1.html#submission",
    "href": "labs/02_1l_java_ide_practice-1.html#submission",
    "title": "3  1L. Java and IDE Practice - 1",
    "section": "3.6 Submission",
    "text": "3.6 Submission\n\nYou can copy and run umm create-submission command in the terminal, and that should create a zip file with a name similar to submission-2024-05-20-07-50-41.zip.\nSubmit the submission-2024... .zip file to Gradescope.",
    "crumbs": [
      "Programming Labs",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1L. Java and IDE Practice - 1</span>"
    ]
  },
  {
    "objectID": "01_intro_to_cs_dsa.html",
    "href": "01_intro_to_cs_dsa.html",
    "title": "4  Background on Computer Science and Data Structures",
    "section": "",
    "text": "4.1 Computing Capabilities\nLet’s break down the term “Computer Science” into its two parts: computer and science. What do these words mean?\nThe word science means “knowledge” in Latin. Science is a way of finding out things about the world by asking questions, doing experiments, and looking for evidence.\nThe word computer means “to calculate” in Latin. A computer is a machine that can do math problems and store information very fast. The word computing means using computers or other machines to solve problems or do tasks.\nBut did you know that computers were not always machines? Before electronic computers were invented, there were people who did math problems by hand for scientists and engineers. They were called human computers. They followed fixed rules and had no authority to deviate from them in any detail. They worked in teams and checked each other’s results for accuracy.\nComputers, in the broadest sense, are devices that can perform calculations or manipulate information. Throughout history, humans have invented and used various types of computers, each with increasing capabilities and complexity. Here are some examples of how computing capabilities have evolved over time:\nHere is a summary of the computing capabilities of some of these devices -",
    "crumbs": [
      "On Computers and Computing",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Background on Computer Science and Data Structures</span>"
    ]
  },
  {
    "objectID": "01_intro_to_cs_dsa.html#computing-capabilities",
    "href": "01_intro_to_cs_dsa.html#computing-capabilities",
    "title": "4  Background on Computer Science and Data Structures",
    "section": "",
    "text": "Tally stick - One of the earliest forms of computers, dating back to prehistoric times. A tally stick is a piece of wood or bone with notches carved into it to record numbers or events. The computations it could do were incrementing and retrieving one piece of information. For example, a shepherd could use a tally stick to keep track of his sheep by making a notch for each one.\nAbacus - A manual device used for calculations by sliding counters along rods or in grooves. The abacus was invented in ancient times and is still used today in some parts of the world. It could store one set of numbers, add, subtract, multiply, divide, and perform other arithmetic operations to the stored information which can later be retrieved. For example, a merchant could use an abacus to keep track of his transactions and profits.\nAstrolabe - A sophisticated instrument used for astronomy and navigation by measuring the positions and movements of celestial bodies. The astrolabe was developed in ancient Greece and reached its peak in the Islamic Golden Age. It could perform complex calculations such as determining the time, latitude, longitude, and direction based on the observation of stars and planets. For example, a sailor could use an astrolabe to find his way across the sea by aligning it with the sun or the pole star.\nAntikythera mechanism - A mechanical device that simulated the motions of the sun, moon, and planets according to a geocentric model. The Antikythera mechanism was discovered in a shipwreck near the Greek island of Antikythera in 1901. It is estimated to date back to the 2nd century BC and is considered one of the first analog computers. It could predict astronomical phenomena such as eclipses, phases of the moon, and positions of the zodiac signs. For example, a priest could use the Antikythera mechanism to plan religious ceremonies and festivals based on the celestial calendar.\nDifference engine - A mechanical calculator that could compute polynomial functions using the method of finite differences. The difference engine was designed by Charles Babbage in the early 19th century but was never fully completed due to technical and financial difficulties. It could generate accurate tables of values for various mathematical functions such as logarithms, trigonometry, and navigation. For example, a mathematician could use the difference engine to check his calculations and avoid errors.\nAnalytical engine - A proposed mechanical computer that could perform any calculation given a set of instructions or a program. The analytical engine was also designed by Charles Babbage in the mid-19th century but was never built due to his death and lack of funding. It is considered the first general-purpose computer and the precursor of modern computers. It could store data in memory, process data using arithmetic and logical operations, control the flow of execution using conditional branching and looping, and output data using a printer or a punch card. For example, Ada Lovelace, who wrote the first algorithm for the analytical engine, envisioned that it could compose music based on mathematical rules.\n\n\n\nTally stick: Store and retrieve one piece of data\nAbacus: Store and retrieve one piece of data, and perform basic arithmetical operations on them with another operand.\nSystem of Gears: Store and retrieve one piece of data. Each gear can use stored data and scale it up or down (multiply or divide) by a fixed constant determined by the gear ratio.​\nDifference engine: Can perform complex, but non-programmable computations. Produced tables of input-output pairs.\nAnalytical engine: Can perform complex, programmable computations. Can store data in memory, process data using arithmetic and logical operations, control the flow of execution using conditional branching and looping, and output data using a printer or a punch card.",
    "crumbs": [
      "On Computers and Computing",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Background on Computer Science and Data Structures</span>"
    ]
  },
  {
    "objectID": "01_intro_to_cs_dsa.html#what-is-computer-science-then",
    "href": "01_intro_to_cs_dsa.html#what-is-computer-science-then",
    "title": "4  Background on Computer Science and Data Structures",
    "section": "4.2 What is Computer Science then?",
    "text": "4.2 What is Computer Science then?\nThe science of computers, or Computer Science, seeks to answer fundamental questions like: What are the essential parts of a computer? What can be computed, and what cannot? What determines the ease and speed of computation? This field provides the foundation for understanding the principles that drive computational systems.",
    "crumbs": [
      "On Computers and Computing",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Background on Computer Science and Data Structures</span>"
    ]
  },
  {
    "objectID": "01_intro_to_cs_dsa.html#data-structures-and-algorithms",
    "href": "01_intro_to_cs_dsa.html#data-structures-and-algorithms",
    "title": "4  Background on Computer Science and Data Structures",
    "section": "4.3 Data Structures and Algorithms",
    "text": "4.3 Data Structures and Algorithms\nThe field of Data Structures and Algorithms expands upon the principles of Computer Science. It determines the efficiency of computation by answering the question: How easily or quickly can something be computed, and what factors influence these metrics? It outlines the necessary building blocks or tools required for programming, and explores the common patterns and problems in programs, offering known ways to enhance their speed. It is this branch of Computer Science that gives us the skills to design, write and analyze the efficiency of our programs. This understanding is crucial to becoming proficient in the broader field of Computer Science.\nIn essence, the intertwined journey of computers and computing science is a testament to human ingenuity and the relentless pursuit of understanding and harnessing the principles that underlie our world. As we delve deeper into the concepts of data structures and algorithms, we continue to contribute to this exciting journey.",
    "crumbs": [
      "On Computers and Computing",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Background on Computer Science and Data Structures</span>"
    ]
  },
  {
    "objectID": "01_intro_to_cs_dsa.html#review",
    "href": "01_intro_to_cs_dsa.html#review",
    "title": "4  Background on Computer Science and Data Structures",
    "section": "4.4 Review",
    "text": "4.4 Review\nWe talked a little about what computers are, and what capabilities a device needs to have to be able to compute certain types of problems.​ The next chapter will be about “general purpose, programmable computers”, and how they work.\nHere’s a fun exercise for you -\n\nAre analog wristwatches computers? What do they compute?\nCan you design a system of gears that can convert Fahrenheit to Celcius?",
    "crumbs": [
      "On Computers and Computing",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Background on Computer Science and Data Structures</span>"
    ]
  },
  {
    "objectID": "02_general_purpose_computers.html",
    "href": "02_general_purpose_computers.html",
    "title": "5  Modern General-Purpose, Programmable Computers",
    "section": "",
    "text": "5.1 The Working of a Processor\nIn this chapter, we will explore the functioning of modern, general-purpose computers. We will approach this topic by drawing parallels with a relatable example from an alternate universe, where two boys, Cory and Jamal, develop a unique system of communication.\nIn this parallel universe, Cory and Jamal are two teenage neighbors that enjoy playing Scrabble, but there’s a hitch. Cory’s parents are quite religious and in this universe, there exists a prophecy from their scriptures that a game played by youngsters will one day end the world; and so they disapprove of Cory playing Scrabble.\nTo get around this, Cory and Jamal come up with a clever plan. Cory happens to have two desk lamps that are visible from Jamal’s window. They decide that if Cory’s mother isn’t home, Cory will switch on the right lamp, and if his father isn’t home, he will turn on the left one. If both lamps are on, it essentially signals to Jamal that he can come over to play Scrabble.\nCory also has a sister who is usually at the University, but also at home on some weekends. When she is home, she tends to wake up for late-night snacks around 1 am. And so when Cory’s sister is home, they can only play until 1 am. To work around this, they add another signal. If Cory switches on his ceiling fan, it will signal that he can come to play, but only until 1 am.\nAs we see, each additional piece of information that we need to share requires an additional indicator.\nThe way Cory and Jamal communicate parallels how a computer’s processor operates. A processor has a set of operations it can perform - like copying, moving, adding, subtracting data, jumping to another instruction, conditional branching, and more.\nTo tell a computer to perform a certain operation, we need several indicators, just like Cory and Jamal’s system. Computers use tiny wires that may or may not have electricity running through them. The presence or absence of electricity indicates what the processor should do.\nWe refer to the absence as a 0 and the presence of electricity as a 1. Each wire conveying either a 0 or a 1 is said to convey one bit of information.\nThis is a fixed group of wires going into the processor to convey such information on what needs to be done. This grouping is known as “Instruction.” The number of bits in an instruction tells us the `size`` of the instruction.\nA processor usually has a fixed instruction size (64 for a 64-bit processor, 32 for a 32-bit processor, etc.).\nSometimes even more information is needed. For example, for operations like jumping to a different instruction, the processor needs to know where to jump to. For operations like addition, two numbers are needed. Sometimes, along with the operation, the processor also needs to be instructed about where to store the result. We call these pieces of data on which we perform instructions as operands. We often refer to instructions as operations.\nWe cannot have an extra set of wires for each time we need to share another operand with the processor, so we share this data in sequence.\nSay 0010 1010 is the code for the add operation, 0000 0000`` is the number 0, and0000 0001` is the number 1. If we want an 8-bit processor to add these together, we might share the following data through the wires -\nThere is a “clock” mechanism within the processor to signal the start and end of “cycles”. Generally the processor “pipelines” its operation such that it can complete one instruction in each cycle. However, many complex instructions like division or square roots take &gt;20 cycles to complete.\nA computer processor can be compared to a complex Rube Goldberg machine, with wires that take in the presence or absence of electricity similar to how many Rube Goldberg machines’ mechanism is kicked off by a rolling ball or marbles. The “marbles” of electricity roll through the processor, along each wire, triggering complex chain reactions or side effects that are designed to elicit the intended operation. Consider that a processor, as small as it is, frequently contains billions of transistors - use this information to imagine how complex these “Rube Goldberg machines” are.",
    "crumbs": [
      "On Computers and Computing",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Modern General-Purpose, Programmable Computers</span>"
    ]
  },
  {
    "objectID": "02_general_purpose_computers.html#the-working-of-a-processor",
    "href": "02_general_purpose_computers.html#the-working-of-a-processor",
    "title": "5  Modern General-Purpose, Programmable Computers",
    "section": "",
    "text": "...\n0000 0001 // third cycle - second operand\n0000 0000 // second cycle - first operand\n0010 1010 // first cycle - add operation",
    "crumbs": [
      "On Computers and Computing",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Modern General-Purpose, Programmable Computers</span>"
    ]
  },
  {
    "objectID": "02_general_purpose_computers.html#memory",
    "href": "02_general_purpose_computers.html#memory",
    "title": "5  Modern General-Purpose, Programmable Computers",
    "section": "5.2 Memory",
    "text": "5.2 Memory\nLet’s compare the computer’s memory to a vast grid filled with tiny cells. These cells can each store some electricity - and again the presence or absence of electricity conveys a bit of information. If we continue the previous analogy, it can be compared to a large grid storing balls (data).\nThe processor has instructions that allow it to select which cells to “read” and transfer those bits to the processor - to be interpreted either as data or instructions. The processor also has instructions that allow it to write to memory.\nIt is this interaction of a processor being able to “write” instructions and data to memory, and then conditionally “read” and execute them, and then write new instructions or data that makes a processor “programmable”.\nWhat we describe in this section is Active memory or RAM. It is the data in RAM that is readily available for the processor to read and write.",
    "crumbs": [
      "On Computers and Computing",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Modern General-Purpose, Programmable Computers</span>"
    ]
  },
  {
    "objectID": "02_general_purpose_computers.html#programs",
    "href": "02_general_purpose_computers.html#programs",
    "title": "5  Modern General-Purpose, Programmable Computers",
    "section": "5.3 Programs",
    "text": "5.3 Programs\nPrograms are essentially detailed sets of instructions that command a computer to execute specific operations. They can be likened to a recipe that a computer follows to achieve a particular task. Programs dictate what steps the computer must take and in what sequence to reach the desired outcome.\nThe program is stored on disk in a format that is standard for that operating system, and where the first instruction is stored in this format is always known - say, the first instruction in a “main” section. When a program is executed, it is loaded from the computer’s storage into RAM, and then the processor reads and executes the first instruction; and so the execution begins.\nThe complexity of programs can vary greatly, from a simple one that performs basic arithmetic to an intricate operating system like Windows or Linux that manages every aspect of a computer. However, the core characteristic of all programs is the same: they are sequences of instructions that the computer follows.",
    "crumbs": [
      "On Computers and Computing",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Modern General-Purpose, Programmable Computers</span>"
    ]
  },
  {
    "objectID": "02_general_purpose_computers.html#the-java-compiler",
    "href": "02_general_purpose_computers.html#the-java-compiler",
    "title": "5  Modern General-Purpose, Programmable Computers",
    "section": "5.4 The Java Compiler",
    "text": "5.4 The Java Compiler\nAs instructions are hard to write directly, we make use of programs called compilers that take in “code” in human-readable text format and output a list of instructions. The program is designed in such a way that the produced list of valid instructions always carries out the task described in “code” faithfully.\nThe Java compiler is such a program for Java code. When a Java program is compiled, the compiler reviews the code for syntax errors and then translates it into bytecode, a type of intermediate language closer to machine language. The Java Virtual Machine (JVM) then interprets this bytecode into machine code that your computer’s processor can execute.\nSyntax errors or other kinds of errors essentially refer to situations where the compiler doesn’t know how to, or cannot produce a valid set of instructions that can carry out what the code is describing.\nCompilers like the Java compiler don’t just translate code. They also optimize it, making it more efficient so that the resulting program runs faster and consumes less memory. For example, they may look at your for loop that is adding up the first N natural numbers and decide to replace the loop with the formula for this computation instead.",
    "crumbs": [
      "On Computers and Computing",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Modern General-Purpose, Programmable Computers</span>"
    ]
  },
  {
    "objectID": "02_general_purpose_computers.html#operating-systems",
    "href": "02_general_purpose_computers.html#operating-systems",
    "title": "5  Modern General-Purpose, Programmable Computers",
    "section": "5.5 Operating Systems",
    "text": "5.5 Operating Systems\nWith a sound understanding of the fundamental components of modern computing, it’s important to highlight the role of the operating system.\nAn operating system (OS) is a type of system software that manages computer hardware and software resources and provides various services for computer programs. It acts as a mediator between users and the computer hardware. Users interact with the operating system through user interfaces such as a command-line interface (CLI) or a graphical user interface (GUI).\nOperating systems bear the responsibility of managing the computer’s resources, including the processor, memory, disk space, and input/output devices. They coordinate tasks, ensuring that the processor’s time is used judiciously, and manage the memory, keeping track of which parts are in use and which are available.\nIn essence, the operating system provides the platform on which all other software runs. It is the environment in which programs, written in languages like Java and then compiled, operate.\nThis fundamental understanding of modern computing components helps elucidate the intricate operations that are continuously happening within our laptops, desktops, and even our smartphones. These fundamental aspects form the backbone of the digital age we live in.",
    "crumbs": [
      "On Computers and Computing",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Modern General-Purpose, Programmable Computers</span>"
    ]
  },
  {
    "objectID": "02_1_compilers_types.html",
    "href": "02_1_compilers_types.html",
    "title": "6  Java Compiler and Types",
    "section": "",
    "text": "6.1 What is a Compiler?\nThe compilation process is a fundamental aspect of Java development. It represents the bridge between human-readable code and machine-executable instructions. This section will explore the nature of compilers, specifically focusing on the Java compiler, which plays a critical role in translating source code into an intermediate form that can be understood by the Java Virtual Machine (JVM).\nA compiler is a specialized software program that translates source code written in a high-level programming language, like Java, into a lower-level, machine-readable code. This translation is essential for enabling the machine to execute the code, as computers can only understand binary instructions.\nIn the context of Java, this translation doesn’t result in machine code specific to a particular computer’s hardware. Instead, the Java compiler translates source code into an intermediate form called “bytecode.” Here’s how it works:\nThe use of bytecode allows Java to achieve its “write once, run anywhere” philosophy, as the bytecode is platform-independent and can be run on any device with an appropriate JVM.",
    "crumbs": [
      "Java Generics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Java Compiler and Types</span>"
    ]
  },
  {
    "objectID": "02_1_compilers_types.html#what-is-a-compiler",
    "href": "02_1_compilers_types.html#what-is-a-compiler",
    "title": "6  Java Compiler and Types",
    "section": "",
    "text": "Note on Bytecode: Think of bytecode as a universal language that the Java compiler translates your code into. It’s like a middleman language that can be understood by any computer, regardless of its specific hardware. This is what makes Java work the same way on different devices.\n\n\nSource Code: Programmers write Java code using text editors or integrated development environments (IDEs). This code is written in a high-level, human-readable language.\nCompilation: The Java compiler translates this high-level code into bytecode. This process includes parsing the code to check for syntax errors, performing various optimizations, and finally translating it into the platform-independent bytecode.\nExecution: The Java Virtual Machine (JVM) interprets or compiles this bytecode at runtime into native machine code tailored to the specific hardware architecture of the host machine.\n\n\nNote on JVM: Imagine the JVM as a special program that understands the universal bytecode and translates it into instructions your computer can directly execute. It’s like having a personal interpreter that makes sure your Java code runs smoothly on any device.",
    "crumbs": [
      "Java Generics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Java Compiler and Types</span>"
    ]
  },
  {
    "objectID": "02_1_compilers_types.html#role-of-the-compiler-in-java",
    "href": "02_1_compilers_types.html#role-of-the-compiler-in-java",
    "title": "6  Java Compiler and Types",
    "section": "6.2 Role of the Compiler in Java",
    "text": "6.2 Role of the Compiler in Java\nThe Java compiler’s role extends beyond mere translation of code. Let’s delve into its multifaceted functions:\n\nSyntax Checking: The compiler checks the source code for any syntax errors, such as missing semicolons, mismatched brackets, or incorrect keywords. If any such errors are found, the compilation process halts, and the errors must be fixed before proceeding.\nType Checking: Java is a strongly-typed language, meaning that every variable and expression has a type, and these types must be compatible. The compiler ensures that variables are used correctly, and that method calls are valid. For instance, it checks if the types of arguments in a method call match the method’s signature.\n\n\nNote on Strong Typing: In Java, you have to be clear about what type (like integer, string, etc.) a variable is. The compiler is strict about this, and it helps catch mistakes early. Think of it like labeling boxes when you move; being specific helps you know what’s inside without opening them.\n\n\nOptimization: Compilers can also perform optimizations to make the code more efficient. These optimizations may include removing unnecessary code, reordering instructions, or inlining methods.\n\n\nNote on Optimization: The compiler doesn’t just translate your code; it also looks for ways to make it run faster and smoother. It’s like reorganizing your closet to make it easier to find what you need. The compiler rearranges the code in a way that lets the computer process it more efficiently.\n\n\nBytecode Generation: Finally, the compiler generates the bytecode. This bytecode is a compact representation of the source code, suitable for execution by the JVM. It includes instructions for the JVM, along with metadata like class names, method names, and variable types.\n\nThe compiler’s thorough analysis and transformation of Java code into bytecode lay the foundation for Java’s robustness and cross-platform capabilities. Understanding the compiler’s function provides insights into how Java source code is interpreted and executed, and it sets the stage for the exploration of more advanced topics such as inheritance, polymorphism, and generics from the compiler’s perspective.\n\n\n\n\n\n\n\n\nG\n\n\n\nSourceCode\n\nSource Code\n\n\n\nCompiler\n\nJava Compiler\n\n\n\nSourceCode-&gt;Compiler\n\n\n Compilation\n\n\n\nBytecode\n\nBytecode\n\n\n\nCompiler-&gt;Bytecode\n\n\n Bytecode Generation\n\n\n\nJVM\n\nJVM\n\n\n\nBytecode-&gt;JVM\n\n\n Initial Interpretation\n\n\n\nJIT\n\nJIT Compiler\n\n\n\nJVM-&gt;JIT\n\n\n Just-In-Time Compilation\n\n\n\nNativeCode\n\nNative Machine Code\n\n\n\nJIT-&gt;NativeCode\n\n\n Native Code Generation\n\n\n\nNativeCode-&gt;JVM\n\n\n Execution\n\n\n\n\n\n\nFigure 6.1: Flow diagram illustrating the journey of Java source code from compilation into bytecode, followed by optimization and transpilation to native machine code by the Java Virtual Machine (JVM). This comprehensive view underscores the multifaceted roles played by the Java compiler and the JVM.\n\n\n\n\n\nThe diagram illustrates the lifecycle of Java code from the moment it is written until it is executed by the JVM.\n\nThe first node, “Source Code,” represents the initial Java code written by programmers.\nThe second node, “Java Compiler,” is where this code gets translated into an intermediate form, known as bytecode.\n“Bytecode,” the third node, is this intermediate form, which is crucial for Java’s portability.\nThe bytecode then moves to the “JVM,” where it is either interpreted or further compiled.\nThe “JIT Compiler” (Just-In-Time Compiler) represents the component of the JVM that compiles the bytecode into native machine code just before execution. This enhances performance.\nFinally, the “Native Machine Code” is the end result, ready to be executed by the computer’s hardware.\n\nEach arrow between the nodes represents a transformation or action, such as “Compilation” between “Source Code” and “Java Compiler” and “Bytecode Generation” between “Java Compiler” and “Bytecode.” These actions are what make Java a powerful and portable language.",
    "crumbs": [
      "Java Generics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Java Compiler and Types</span>"
    ]
  },
  {
    "objectID": "02_1_compilers_types.html#type-checking-and-the-compiler",
    "href": "02_1_compilers_types.html#type-checking-and-the-compiler",
    "title": "6  Java Compiler and Types",
    "section": "6.3 Type Checking and the Compiler",
    "text": "6.3 Type Checking and the Compiler\nType checking is not merely about confirming if an int is an int. It’s about ensuring that variables, methods, and objects adhere to certain expectations or “contracts.” The compiler uses these contracts to ensure your code is logically coherent before it even gets run.\n// Simple type checking example\nint a = 5;  // Declaring integer variable\n// a = \"hello\";  // Compiler Error: Incompatible types\n\nA Variable as a Promise: Imagine telling your friend you will bring a pen to class. You’ve now made a promise, or set an expectation. Similarly, when you declare a variable as an int, you’re promising the compiler that this variable will only store integer values.\n\n\n6.3.1 Role of Variable Declarations in Type Checking\n// Type checking with variable declaration\ndouble b = 4.5;\n// b = a + \"some text\";  // Compiler Error: Incompatible types\nb = a + 2.0;  // No error\nIn Java, declaring a variable with a specific type serves as a basic form of contract with the compiler. This informs the compiler what kinds of operations are valid or invalid on that variable. Essentially, type declarations are like basic rules that you agree to follow.\n\n\n6.3.2 Role of Method Declarations in Type Checking\n// Method declaration and calling\npublic int add(int x, int y) {\n    return x + y;\n}\n\n// add(5, \"10\");  // Compiler Error: Incompatible types\nMethod declarations build upon the foundation laid by variable declarations. They specify not just what actions (methods) are permissible on an object but also what kinds of values can be passed as arguments and what kind of value will be returned.\n\nMethods as a Job Description: Think of method declarations like a job description. It lists the qualifications (parameter types) you need to apply (call the method) and what you’ll get in return (return type).\n\n\n\n6.3.3 Role of Class Declarations in Type Checking\n// Class declaration with variables and methods\npublic class Dog {\n    String name;\n    int age;\n    \n    public void bark() {\n        System.out.println(\"Woof!\");\n    }\n}\n\nDog myDog = new Dog();\n// myDog.fly();  // Compiler Error: Cannot find symbol\nFinally, class declarations act as overarching contracts that encompass both variable and method declarations. These define the blueprint for objects and provide the compiler with a comprehensive understanding of what an object is capable of and what it contains.\n\nA Class as a Contract: Consider a class as a more complex agreement or contract that spells out what variables and methods are available for objects of that class.\n\n\n\n6.3.4 Inheritance and Type Checking\n// Class inheritance example\npublic class Animal {\n    public void makeSound() {\n        System.out.println(\"Some generic animal sound\");\n    }\n}\n\npublic class Cat extends Animal {\n    @Override\n    public void makeSound() {\n        System.out.println(\"Meow\");\n    }\n}\n\nAnimal myAnimal = new Cat();\nmyAnimal.makeSound();  // Outputs \"Meow\"\nThe concept of inheritance allows a subclass to inherit fields and methods from its superclass. This effectively expands the initial contract made by the superclass, and the compiler recognizes this extended contract during type checking.\n\n\n6.3.5 Abstract Classes and Interfaces in Type Checking\n// Abstract class and interface example\npublic abstract class Shape {\n    abstract void draw();\n}\n\npublic interface Drawable {\n    void draw();\n}\n\npublic class Circle extends Shape implements Drawable {\n    public void draw() {\n        System.out.println(\"Drawing circle\");\n    }\n}\n\nShape shape = new Circle();\nDrawable drawable = new Circle();\nshape.draw();  // Outputs \"Drawing circle\"\ndrawable.draw();  // Outputs \"Drawing circle\"\nAbstract classes and interfaces serve as abstract contracts. While they set certain expectations, they allow flexibility in how these expectations are met. They are like general guidelines or protocols that subclasses or implementing classes must follow.",
    "crumbs": [
      "Java Generics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Java Compiler and Types</span>"
    ]
  },
  {
    "objectID": "03_algorithmic_analysis.html",
    "href": "03_algorithmic_analysis.html",
    "title": "7  Algorithmic Analysis",
    "section": "",
    "text": "7.1 Problems, Algorithms, and Programs\nIn this chapter, we will delve into the exciting world of algorithmic analysis. The objectives of this chapter are to learn to communicate the speed of an algorithm effectively and to appraise the performance of an algorithm from pseudocode or Java code.\nBefore understanding algorithmic analysis, it’s essential to differentiate between problems, algorithms, and computer programs. These are three distinct concepts that are interrelated.",
    "crumbs": [
      "Algorithmic Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Algorithmic Analysis</span>"
    ]
  },
  {
    "objectID": "03_algorithmic_analysis.html#problems-algorithms-and-programs",
    "href": "03_algorithmic_analysis.html#problems-algorithms-and-programs",
    "title": "7  Algorithmic Analysis",
    "section": "",
    "text": "7.1.1 Problems\nA problem in computer science refers to a specific task that needs to be solved. It can be thought of in terms of inputs and matching outputs. For instance, to solve the problem of finding the youngest student in our class, the input would be the names and ages of all students in the class. The output would be the name of the youngest student.\nIt’s helpful to perceive problems as functions in a mathematical sense. In mathematics, a function is a relationship or correspondence between two sets — the input set (domain) and the output set (range).\n\n\n7.1.2 Algorithms\nAn algorithm, on the other hand, is a method or a process followed to solve a problem. If we perceive the problem as a function, then an algorithm can be seen as an implementation of this function that transforms an input into the corresponding output.\nSince there are typically numerous ways to solve a problem, there could be many different algorithms for the same problem. Having multiple solutions is advantageous because a specific solution might be more efficient than others for certain variations of the problem or specific types of inputs.\nFor instance, one sorting algorithm might be best suited for sorting a small collection of integers. Another might excel in sorting a large collection of integers, while a third might be ideal for sorting a collection of variable-length strings.\nBy definition, a sequence of steps can only be called an algorithm if it fulfills the following properties:\n\nIt must be correct.\nIt consists of a series of concrete steps.\nThere is no ambiguity about the step to be performed next.\nIt must comprise a finite number of steps.\nIt must terminate.\n\n\n\n7.1.3 Programs and Their Building Blocks\nBefore discussing programs in detail, let’s briefly review two essential components that make a program run: the CPU (Central Processing Unit) and memory.\nA CPU is the electronic circuitry within a computer that has the ability to execute certain instructions. Its primary function is to fetch, decode, and execute instructions. It has slots to store data, referred to as registers. Communicating with a CPU involves telling it what operation you want to perform and on which data. For instance, you might instruct the CPU to add two numbers stored in registers A and B and store the result in register C.\nCPU instructions are binary codes that specify which operation the CPU should perform. Here’s an example of what they look like:\n10010011001100111110000111011111\nSome bits in the instruction form the opcode, the operation code. The opcode is a unique identifier for an operation, like adding integers. Other bits form the operand(s), the data on which to operate. The operand can be where the data is stored (the name of a register or an address in memory), or where to store the result of the operation (again, the name of a register or an address in memory).\nFor human readability, there are notations to represent these binary instructions. Here is an example of a set of instructions in a human-readable form:\n.global main\nmain:\naddi   sp, sp, -16\nsd     t0, 0(sp)\nsd     t1, 8(sp)\ncall   some_function\nld     t0, 0(sp)\nld     t1, 8(sp)\n# Use t0 and t1 here as if nothing happened.\naddi   sp, sp, 16\nPrograms are structured into sections. They include code sections, which contain a list of instructions, and data sections, which hold binary data such as text, images, or numbers that the program needs to use. Typically, there is a designated “main” section that contains the instructions to be executed first.\nWhen you initiate an executable (with the exception of Mac “applications”), the binary data (“bits”) are read from the hard disk and transferred to the main memory (RAM). The execution of the program begins when the first instruction from the “main” section is transferred to the CPU.\nIn this context, a computer program’s code can be seen as an instance, or concrete representation, of an algorithm. Although the terms “algorithm” and “program” are distinct, they are often used interchangeably for simplification.",
    "crumbs": [
      "Algorithmic Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Algorithmic Analysis</span>"
    ]
  },
  {
    "objectID": "03_algorithmic_analysis.html#comparing-the-performance-of-programs",
    "href": "03_algorithmic_analysis.html#comparing-the-performance-of-programs",
    "title": "7  Algorithmic Analysis",
    "section": "7.2 Comparing the Performance of Programs",
    "text": "7.2 Comparing the Performance of Programs\nWhen you compile or build a program, its code is converted into a series of instructions and data in memory. However, the execution time of the same program can vary across different machines due to differences in the processor’s capabilities.\nAs each machine can potentially have a different processor, comparing the speed of programs can be a complex task, and is only meaningful when the processor is the same or standardized. Even when the processor is standardized, many factors affect performance -\n\nBackground tasks on one machine can interfere with performance measurements.\nEven if you have the exact same processor, differences in manufacturing mean each can run at a different clock frequency.\nSmall differences in ambient temperature affect how high a processor can clock.\nThe mounting pressure of a cooler can affect heat transfer and in turn how high a processor can clock.\nMany cooling solutions involve vapor chambers. The orientation of vapor chambers can affect heat transfer and in turn how high a processor can clock.\nEven cosmic radiation can affect processors and memory.\n\nTherefore, we usually prefer to compare algorithms instead. The methods of comparing algorithms will be discussed in the following sections of this chapter.",
    "crumbs": [
      "Algorithmic Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Algorithmic Analysis</span>"
    ]
  },
  {
    "objectID": "03_algorithmic_analysis.html#analyzing-algorithms",
    "href": "03_algorithmic_analysis.html#analyzing-algorithms",
    "title": "7  Algorithmic Analysis",
    "section": "7.3 Analyzing Algorithms",
    "text": "7.3 Analyzing Algorithms\nOne of the key components of this course is to provide a framework for predicting the performance of algorithms just by inspecting their structure. Let’s dive into the process of analyzing an algorithm’s time complexity.\n\n7.3.1 Predicting Execution Time\nConsider a simple method that adds two numbers:\npublic int add(int lhs, int rhs)\nSuppose calling add(2, 4) takes 1 second. How long would add(40, 50) or add(343245634, 32432423) take? As you might expect, all these operations, despite the difference in magnitude of the numbers involved, take approximately the same time. That’s because the time complexity of an addition operation does not depend on the values of the numbers but on the number of operations involved, which, in this case, is a single addition.\n\n\n7.3.2 Impact of Input Size\nNow, let’s examine a slightly more complex method that sums up a list of numbers:\npublic int sumOfList(List&lt;int&gt; l)\nAssuming that adding two numbers takes one second, how long would summing a list of 10 numbers take? We can infer that the time taken by sumOfList depends on the size of the list we provide. For instance, summing up a list of 10 numbers would take about half the time needed to sum up a list of 20 numbers.\nHere’s an implementation of sumOfList:\nimport java.util.ArrayList;\n\nclass Square {\n  static int sumOfList(ArrayList&lt;Integer&gt; l) {\n    int sum = 0;\n    for (int i : l) {\n      sum += i;\n    }\n    return sum;\n  }\n}\nThe key insight is that the execution time of this method depends on the number of elements in the list - which is the size of the input. The time complexity is directly proportional to the number of times the addition statement is executed, which is equal to the size of the list.\n\n\n7.3.3 Iterations and Input Size\nLet’s take it a step further. If you’re summing a list of N items, each of which is another list of M items, the operation would take N * M addition statements. Here, the time complexity is a function of both N and M.\n\n\n7.3.4 Gauging Relative Execution Time\nThe crux of analyzing an algorithm’s performance lies in understanding how many times statements in the program run as a function of the size of the input. This approach enables us to estimate how long two invocations of the same method will take relative to each other, given the size of the input for each.\nUnderstanding this concept will allow you to better predict the performance of algorithms, which is a crucial skill in efficient programming and system design.",
    "crumbs": [
      "Algorithmic Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Algorithmic Analysis</span>"
    ]
  },
  {
    "objectID": "03_algorithmic_analysis.html#algorithm-complexities",
    "href": "03_algorithmic_analysis.html#algorithm-complexities",
    "title": "7  Algorithmic Analysis",
    "section": "7.4 Algorithm Complexities",
    "text": "7.4 Algorithm Complexities\nIn order to evaluate an algorithm’s efficiency, we analyze its time complexity and space complexity, both of which describe how the algorithm’s performance scales with the size of the input.\n\n7.4.1 Time Complexity\nTime complexity measures how the execution time of an algorithm increases with the size of the input. For instance, counting how many times the number \\(5\\) appears in a list requires checking each number in the list. Hence, the time complexity is directly proportional to the size of the list.\n\n\n7.4.2 Space Complexity\nSpace complexity quantifies the amount of memory an algorithm requires relative to the size of the input. Using the previous example, we would need enough space to store the list and an additional space to store the counter. Thus, the space complexity is proportional to the size of the list, or more precisely, \\(N + 1\\).\n\n\n7.4.3 Conditional Statements and Complexity\nIn cases where conditional statements are present, the number of executed statements depends on which branch the program takes. One branch may contain more statements than the other. To handle such scenarios, we introduce the concept of Big-O, Big-Ω, and Big-Θ notations.\n\n7.4.3.1 Big-O Notation\nThe Big-O notation describes the worst-case time complexity of an algorithm, essentially providing an upper bound on the time taken. This notation considers the scenario where the program consistently takes the path with the most statements. While Big-O is commonly used for time complexity, it can also describe space complexity.\nThe notation comprises two parts: the function itself and the variable representing the input size. Generally, ‘n’ is used to represent the input size, and constants and coefficients are typically ignored. For instance, the following functions are all \\(O(n)\\):\n\n\\(n + 1\\)\n\\(2n\\)\n\\(103n + 124\\)\nOnly the highest degree of ‘n’ is considered when determining Big-O notation. Therefore, functions such as n², n/2, or √n are not considered O(n).\n\n\n\n7.4.3.2 Big-Ω Notation\nThe Big-Ω notation represents the best-case complexity of an algorithm. It follows the same format as Big-O notation but focuses on the scenario where the program consistently takes the path with the fewest statements.\n\n\n7.4.3.3 Big-Θ Notation\nThe Big-Θ notation is used to denote the average-case complexity of an algorithm. It again follows the same structure as Big-O, but it considers both the best and worst-case scenarios to provide an average estimate of the algorithm’s performance.\nRemember, these notations and complexities are pivotal in estimating the performance of an algorithm based on the size or other properties of the input, correlating to the number of steps taken by the algorithm. This understanding is crucial when designing efficient and effective solutions in computer science.",
    "crumbs": [
      "Algorithmic Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Algorithmic Analysis</span>"
    ]
  },
  {
    "objectID": "03_algorithmic_analysis.html#common-big-o-complexities",
    "href": "03_algorithmic_analysis.html#common-big-o-complexities",
    "title": "7  Algorithmic Analysis",
    "section": "7.5 Common Big-O Complexities",
    "text": "7.5 Common Big-O Complexities\nBig-O notation is a way of expressing the worst-case time complexity of an algorithm. It describes how the running time of an algorithm changes as the size of its input grows. The most common Big-O complexities are:\n\n7.5.1 Constant Complexity and Growth\nConstant complexity, often represented as \\(O(1)\\), occurs when the running time of an algorithm or the amount of work needed does not change with the size of the input (\\(N\\)). This means the algorithm takes a fixed amount of time, regardless of how many elements it is processing.\nFor example, accessing an element in an array by its index is an operation of constant complexity. This is because it takes roughly the same amount of time, regardless of the size of the array:\n\n\\(f(n) = 1\\) for all \\(n\\)\n\nIn this case, no matter how large or small our input size is, the amount of work we have to do remains the same. This is the most efficient complexity an algorithm can have.\n\n\n7.5.2 Linear Complexity and Growth\nLinear complexity, often represented as \\(O(n)\\), occurs when the running time of an algorithm or the amount of work needed scales proportionally with the size of the input (\\(N\\)). For every additional element in the input, a fixed amount of work is added.\nFor example, finding an element in an unsorted list is an operation of linear complexity, as the algorithm might need to look at every element once:\n\n\\(f(n) = n\\)\n\nIn this case, if we add one more element to our input size, we add one more unit of work. This is because every additional element requires the same amount of work.\n\n\n7.5.3 Quadratic Complexity and Growth\nQuadratic complexity, often represented as \\(O(n^2)\\), occurs when the running time of an algorithm or the amount of work needed scales with the square of the size of the input (\\(N\\)). For every additional element in the input, the work increases by a factor of \\(n\\).\nFor example, a simple nested loop for comparing pairs of elements in a list has quadratic complexity. This is because each element is compared to every other element:\n\n\\(f(n) = n^2\\)\n\nIn this case, if we add one more element to our input size, we add \\(n\\) units of work, as we have to compare this new element with every other element already in the list.\n\n\n7.5.4 Exponential Functions and Growth\nAn exponential function is one that includes a variable in the exponent. To illustrate, the function \\(2^n\\) is an exponential function. Here, with each unit increase in the input, the output is multiplied (or scaled up) by a factor of 2. For instance:\n\n\\(f(1) = 2^1 = 2\\)\n\\(f(2) = 2^2 = 4\\)\n\\(f(n+1) = f(n) * 2\\)\n\nExponential growth is characterized by a constant factor scaling up the running time of the algorithm, or the amount of work needed, for each unit increase in the size of the input (often denoted as \\(N\\)). This constant factor is the base of the exponent. This means that for an algorithm with a time complexity of \\(2^N\\), adding one more element to the input could potentially double the amount of work required. Examples of algorithms exhibiting exponential growth include certain solutions to problems like the Towers of Hanoi and calculations of the Fibonacci sequence.\n\n\n7.5.5 Logarithmic Functions and Growth\nLogarithmic functions serve as the inverse of exponential functions. For example, \\(log_2(n)\\) is a logarithmic function. Here, the output is decremented by 1 each time the input is divided (or scaled down) by a factor of 2. For instance:\n\n\\(f(16) = log_2(16) = 4\\)\n\\(f(8) = log_2(8) = 3\\)\n\\(f(n/2) = f(n) - 1\\)\n\nLogarithmic growth is characterized by a decrement of 1 in the running time of the algorithm or the amount of work needed, each time the size of the input (denoted as \\(N\\)) is divided by a constant factor. This constant factor is the base of the logarithm. For an algorithm with a time complexity of \\(log_2(n)\\), if we have an input size of 16, doubling the input size will only increase the amount of work by 1 unit. Similarly, halving the input size will decrease the work by 1 unit.\nAlgorithms often display complexities such as \\(N*log_2(N)\\), which represents a combination of linear and logarithmic growth. Examples of such algorithms that include logarithmic growth are binary search and certain sorting algorithms.\n\n\n7.5.6 Examples\nLet’s look at some examples of algorithms and their Big-O complexities.\nstatic int findMin(x, y) {\n  if (x &lt; y) {\n    return x;\n  } else {\n    return y;\n  }\n}\nThis algorithm finds the minimum of two numbers x and y. It does not depend on the input size, since it only performs one comparison and one return statement. Therefore, its worst-case complexity is \\(O(1)\\). Its best case and average case are also \\(O(1)\\) since they are the same as the worst case.\nstatic int linearSearch(numbers[], target)\n  for (int i = 0; i &lt; numbers.length; i++) {\n    if (numbers[i] == target) {\n      return i;\n    }\n  }\n  return -1;\n}\nThis algorithm performs a linear search on an array of numbers to find a target value. It iterates through each element of the array until it finds the target or reaches the end of the array. In the worst case, it has to check every element of the array, which means its worst-case complexity is \\(O(n)\\), where n is the length of the array. In the best case, it finds the target in the first element, which means its best-case complexity is \\(O(1)\\). In the average case, it finds the target somewhere in the middle of the array, which means its average-case complexity is also \\(O(n)\\).\nWe will talk about more examples of other common worst-case complexities throughout this course.",
    "crumbs": [
      "Algorithmic Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Algorithmic Analysis</span>"
    ]
  },
  {
    "objectID": "03_algorithmic_analysis.html#growth-rate",
    "href": "03_algorithmic_analysis.html#growth-rate",
    "title": "7  Algorithmic Analysis",
    "section": "7.6 Growth Rate",
    "text": "7.6 Growth Rate\nThe growth rate for an algorithm is the rate at which the cost of the algorithm grows as the size of its input grows. The cost can be measured in terms of time, space, or other resources. The worst-case complexity notation essentially denotes the growth rate of the time complexity with respect to the size of a worst-case input.\nThe table below summarizes how different Big-O complexities compare in terms of their growth rates.\n\n\n\nComplexity\nGrowth Rate\n\n\n\n\nO(1)\nConstant\n\n\nO(\\(log(n)\\))\nLogarithmic\n\n\nO(n)\nLinear\n\n\nO(n\\(^2\\))\nQuadratic\n\n\nO(\\(2^n\\))\nExponential\n\n\n\nAs we can see, constant and logarithmic complexities have very low growth rates, meaning that they are very efficient and scalable algorithms. Linear complexity has a moderate growth rate, meaning that it can handle reasonably large inputs but may become slow for very large inputs. Quadratic and exponential complexities have very high growth rates, meaning that they are very inefficient and unscalable algorithms that can only handle small inputs.",
    "crumbs": [
      "Algorithmic Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Algorithmic Analysis</span>"
    ]
  },
  {
    "objectID": "03_algorithmic_analysis.html#summary-review",
    "href": "03_algorithmic_analysis.html#summary-review",
    "title": "7  Algorithmic Analysis",
    "section": "7.7 Summary / Review",
    "text": "7.7 Summary / Review\nIn this section, we learned how to analyze the performance of algorithms using Big-O notation. We saw that measuring the actual running time of an algorithm on real hardware is difficult and impractical because it depends on many factors such as the hardware specifications, the programming language, the compiler, the input size, and the input distribution. Therefore, we need a way to simplify and standardize the performance analysis across different hardware and software platforms.\nWe learned that one way to simplify the performance analysis is to count the number of steps or instructions that an algorithm needs to execute before finishing. We saw that these steps are analogous to the instructions generated by a compiler when it translates our code into machine code. We also learned that different steps may have different costs depending on their complexity, but we can ignore these differences for simplicity and focus on the overall number of steps.\nWe learned that Big-O notation is a mathematical tool that allows us to express the worst-case complexity of an algorithm. It describes how the number of steps grows as a function of the input size in the worst possible scenario. It gives us an upper bound on the performance of an algorithm, meaning that it tells us how slow an algorithm can get in the worst case. We also learned that Big-O notation ignores constant factors and lower-order terms because they become insignificant as the input size grows.\nWe learned about some common Big-O complexities and their growth rates, such as constant, logarithmic, linear, quadratic, and exponential. We saw some examples of algorithms and their Big-O complexities, and how to analyze them using simple rules such as adding complexities for sequential steps, multiplying complexities for nested steps, and taking the maximum complexity for conditional steps.\nWe learned that Big-O notation helps us compare different algorithms and choose the most efficient one for a given problem. It also helps us estimate how well an algorithm can scale to larger inputs and how it can affect the performance of our applications.",
    "crumbs": [
      "Algorithmic Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Algorithmic Analysis</span>"
    ]
  },
  {
    "objectID": "04_unit_testing.html",
    "href": "04_unit_testing.html",
    "title": "8  Unit Testing and Test-Driven Development",
    "section": "",
    "text": "8.1 Background\nImagine you are an architect and you’ve just designed a large, complex building - let’s say a skyscraper. This skyscraper is not merely a single entity; it’s composed of thousands of individual components - the plumbing, the electrical wiring, the elevators, the heating system, and the building’s structural elements, among many others. Now, how would you ensure that the skyscraper works as intended? Would you wait until the entire building is constructed and then start testing every possible scenario? Obviously, this approach is time-consuming, and it exposes you to significant risk.\nA similar challenge exists in the world of software development. Take, for example, a web browser. This is a complex piece of software with hundreds of classes interacting in intricate ways. These classes and methods perform various tasks such as rendering HTML and CSS, processing JavaScript, managing cookies, implementing security features, and many others. Ensuring the correct functionality of this software is a daunting task, given the vast range of potential inputs. After all, there are billions of web pages on the internet, each with its unique combination of technologies, designs, and user interactions. How can you guarantee that your browser works flawlessly with all of them? A naive approach would be to load each web page and observe the output, but this process is not only time-consuming but also practically impossible.\nThis conundrum begs the question: How can we validate the correct functionality of a software product efficiently? The direction points towards automation - the ability to conduct tests without manual intervention. But how can we achieve this, especially given the enormous application surface area?\nThis is where unit testing and Test-Driven Development (TDD) come in.",
    "crumbs": [
      "Unit Testing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Unit Testing and Test-Driven Development</span>"
    ]
  },
  {
    "objectID": "04_unit_testing.html#the-power-of-unit-testing",
    "href": "04_unit_testing.html#the-power-of-unit-testing",
    "title": "8  Unit Testing and Test-Driven Development",
    "section": "8.2 The Power of Unit Testing",
    "text": "8.2 The Power of Unit Testing\nWhile the surface area of an entire application is vast, the surface area of individual classes and units of code within the software project is significantly smaller. If we can write tests to verify that each method within each class functions correctly for all possible inputs, we reduce the complexity of the problem.\nAt first glance, it might seem like an overwhelming task. Even a moderately complex software project can have thousands of methods spread across hundreds of classes. Writing tests for all of them could result in thousands of test cases. But this is precisely where automation proves its worth. By automating these tests, we can execute them each time we modify our code, ensuring the functionality remains intact. This method gives us confidence that our changes have not inadvertently introduced bugs into existing functionality.\nThe key principle here is that by ensuring each individual unit of our software behaves correctly, we can be reasonably confident that the application as a whole operates as expected, provided the software architecture is sound. In this manner, unit testing allows us to break down the monumental task of verifying a complex software system’s functionality into manageable, automated tasks.\nIn the following sections, we will delve deeper into the concept of unit testing, its implementation in Java, and the practice of Test-Driven Development, where tests actually guide and shape the development of the software. Buckle up, for we’re about to embark on an exciting journey that will fundamentally change how you approach software development!",
    "crumbs": [
      "Unit Testing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Unit Testing and Test-Driven Development</span>"
    ]
  },
  {
    "objectID": "04_unit_testing.html#a-basic-approach-to-unit-testing",
    "href": "04_unit_testing.html#a-basic-approach-to-unit-testing",
    "title": "8  Unit Testing and Test-Driven Development",
    "section": "8.3 A Basic Approach to Unit Testing",
    "text": "8.3 A Basic Approach to Unit Testing\nIn order to illustrate the process of unit testing in Java, let’s consider a simple utility class named MathUtil. This class defines basic arithmetic operations such as add, subtract, etc.\npublic class MathUtil {\n    public int add(int a, int b) {\n        return a + b;\n    }\n    \n    // More methods for subtract, multiply, etc.\n}\nAs we discussed in the previous section, to ensure our MathUtil functions correctly, we associate it with a MathUtilTest class. This class contains multiple test methods, each designed to verify a different scenario of the operations provided by MathUtil.\npublic class MathUtilTest {\n\n    public boolean testAdd1() {\n        MathUtil m = new MathUtil();\n        int lhs = 5;\n        int rhs = 7;\n\n        if (m.add(lhs, rhs) == lhs + rhs) {\n            return true;\n        } else {\n            return false;\n        }\n    }\n    \n    // More test methods for other cases...\n}\nIn the above example, the testAdd1 method tests the addition of two positive numbers. We could also add methods like testAdd2 to test adding a positive and a negative number, testAdd3 to test adding two negative numbers, and so forth. Each of these methods tests a specific scenario and validates that the result is as expected.",
    "crumbs": [
      "Unit Testing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Unit Testing and Test-Driven Development</span>"
    ]
  },
  {
    "objectID": "04_unit_testing.html#recognizing-the-inefficiencies-and-redundancies",
    "href": "04_unit_testing.html#recognizing-the-inefficiencies-and-redundancies",
    "title": "8  Unit Testing and Test-Driven Development",
    "section": "8.4 Recognizing the Inefficiencies and Redundancies",
    "text": "8.4 Recognizing the Inefficiencies and Redundancies\nWhile the above approach accomplishes our objective of validating the methods in our MathUtil class, you might have already noticed that it’s far from optimal. There are several glaring issues that can make this method tedious and inefficient:\n\n8.4.1 Redundancy\nEvery test method follows a similar pattern - we perform an operation and then verify if the result matches the expected outcome. This redundancy suggests we could abstract out the verification part into a separate method.\nIn the following expanded MathUtilTest class, you can observe that testAdd2 and testAdd3 follow the exact same pattern as testAdd1. They create an instance of MathUtil, perform an operation, and then compare the result with the expected outcome. This repetitive pattern across multiple tests highlights the redundancy and inefficiency of this approach.\npublic class MathUtilTest {\n\n    public boolean testAdd1() {\n        MathUtil m = new MathUtil();\n        int lhs = 5;\n        int rhs = 7;\n\n        if (m.add(lhs, rhs) == lhs + rhs) {\n            return true;\n        } else {\n            return false;\n        }\n    }\n    \n    public boolean testAdd2() {\n        MathUtil m = new MathUtil();\n        int lhs = -5;\n        int rhs = 7;\n\n        if (m.add(lhs, rhs) == lhs + rhs) {\n            return true;\n        } else {\n            return false;\n        }\n    }\n\n    public boolean testAdd3() {\n        MathUtil m = new MathUtil();\n        int lhs = -5;\n        int rhs = -7;\n\n        if (m.add(lhs, rhs) == lhs + rhs) {\n            return true;\n        } else {\n            return false;\n        }\n    }\n    \n    // More test methods for other cases...\n}\n\n\n8.4.2 Lack of Automation\nLet’s see how we need to currently run the tests we’ve written.\npublic class MathUtilTest {\n\n    // testAdd1, testAdd2, testAdd3, etc. test methods...\n\n    public static void main(String[] args) {\n        MathUtilTest test = new MathUtilTest();\n\n        System.out.println(\"testAdd1 result: \" + (test.testAdd1() ? \"PASS\" : \"FAIL\"));\n        System.out.println(\"testAdd2 result: \" + (test.testAdd2() ? \"PASS\" : \"FAIL\"));\n        System.out.println(\"testAdd3 result: \" + (test.testAdd3() ? \"PASS\" : \"FAIL\"));\n\n        // add more prints for other test cases\n    }\n}\nWith this main method, you can now run the MathUtilTest class, and it will execute each of the testAdd methods and print whether each test passed or failed. This method is a basic way to manually execute the tests and check their results.\nCurrently, we need to call each test method manually to run our tests. An automated system that could execute all tests for us would save time and reduce the chances of human error. However, as we will see later, there are better approaches to automation than the one we’ve used here.",
    "crumbs": [
      "Unit Testing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Unit Testing and Test-Driven Development</span>"
    ]
  },
  {
    "objectID": "04_unit_testing.html#a-slightly-more-sophisticated-approach-to-unit-testing",
    "href": "04_unit_testing.html#a-slightly-more-sophisticated-approach-to-unit-testing",
    "title": "8  Unit Testing and Test-Driven Development",
    "section": "8.5 A slightly more sophisticated approach to Unit Testing",
    "text": "8.5 A slightly more sophisticated approach to Unit Testing\nTo alleviate the redundancy, we can create a method that compares the expected and actual results and raises an error if they do not match. This method, which we can call assertEquals, would look something like this:\npublic static void assertEquals(String testCaseName, int expected, int actual) {\n    if (expected != actual) {\n        System.out.println(testCaseName + \" result: FAIL\");\n    } else {\n        System.out.println(testCaseName + \" result: PASS\");\n    }\n}\nThen we can simplify our test methods by using assertEquals:\npublic void testAdd() {\n    MathUtil m = new MathUtil();\n\n    assertEquals(\"testAddTwoPositive\", m.add(5, 7), 13);\n    assertEquals(\"testAddTwoNegative\", m.add(-5, -7), -13);\n    assertEquals(\"testAddNegPos\", m.add(-5, 7), 2);\n}\nNow, our test case looks cleaner and easier to understand. The assertEquals method abstracts away the comparison details, leaving only the test logic in the test case. We can apply this simplification to all our test methods.\nThis approach significantly reduces the redundancy in our test code, making it easier to write and maintain our tests. However, we are still manually running each test method from the main method. What if we could also automate the execution of all test methods?",
    "crumbs": [
      "Unit Testing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Unit Testing and Test-Driven Development</span>"
    ]
  },
  {
    "objectID": "04_unit_testing.html#automating-test-execution",
    "href": "04_unit_testing.html#automating-test-execution",
    "title": "8  Unit Testing and Test-Driven Development",
    "section": "8.6 Automating Test Execution",
    "text": "8.6 Automating Test Execution\nWhat if we could just call a single method that runs all our test methods? Let’s define a simple runTests method that does exactly that.\npublic void runTests() {\n    testAdd();\n    // Call all other test methods here...\n}\nAnd then you can simply call the runTests method to execute all your tests:\npublic static void main(String[] args) {\n    MathUtilTest test = new MathUtilTest();\n    test.runTests();\n}\nThis approach is an improvement over manually running each test. However, it still has some drawbacks. For instance, when you add a new test method, you need to remember to add a call to this method in the runTests method. It would be better if our test framework could automatically detect and run all test methods without requiring any modifications to the runTests method. As we’ll see later, this is precisely what test frameworks like JUnit offer.",
    "crumbs": [
      "Unit Testing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Unit Testing and Test-Driven Development</span>"
    ]
  },
  {
    "objectID": "04_unit_testing.html#summary",
    "href": "04_unit_testing.html#summary",
    "title": "8  Unit Testing and Test-Driven Development",
    "section": "8.7 Summary",
    "text": "8.7 Summary\nSo far, we have seen how unit testing can be a powerful tool in ensuring that individual units of code within a larger software application function as expected. We have also discussed and implemented a basic system for automating unit tests in Java, gradually refining this system to make it more efficient and less redundant.\nIn the following sections, we will discuss JUnit, a popular unit testing framework in Java that takes automation and convenience to the next level. We will also explore the practice of Test-Driven Development, where we let our tests guide the development of our software, helping us to write cleaner, more robust code. Stay tuned!",
    "crumbs": [
      "Unit Testing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Unit Testing and Test-Driven Development</span>"
    ]
  },
  {
    "objectID": "05_writing_junit_tests.html",
    "href": "05_writing_junit_tests.html",
    "title": "9  Writing JUnit Tests and Test-Driven Development",
    "section": "",
    "text": "9.1 Introduction to JUnit\nUnit testing is crucial to ensure the accuracy and performance of your code. But as we’ve seen, managing and writing tests can be a bit cumbersome. This is where testing frameworks, like JUnit, come in. They automate the tedious parts of testing and provide us with a plethora of tools to write effective tests.\nJUnit is a widely used testing framework in the Java world. It automates the process of running tests and provides us with a wide range of assertion methods to validate our code. JUnit helps to simplify our test code, making it easier to read and maintain.\nSo, why is JUnit so popular?\nNow that you understand what JUnit is and why it’s beneficial let’s see how to use it in our MathUtil class.",
    "crumbs": [
      "Unit Testing",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Writing JUnit Tests and Test-Driven Development</span>"
    ]
  },
  {
    "objectID": "05_writing_junit_tests.html#introduction-to-junit",
    "href": "05_writing_junit_tests.html#introduction-to-junit",
    "title": "9  Writing JUnit Tests and Test-Driven Development",
    "section": "",
    "text": "Simplicity: JUnit simplifies the process of writing and running tests. The framework handles the boilerplate code, allowing us to focus solely on writing the test cases.\nAssertion Library: JUnit provides a comprehensive set of assertion methods that help us validate our code against a wide range of conditions.\nAnnotations: JUnit uses annotations to define test methods and setup methods, making our test code easier to read and understand.\nAutomatic Test Discovery: JUnit automatically finds and runs all test methods, so we don’t have to manually list them in our code.\nIDE Integration: Most modern IDEs provide first-class support for JUnit, including features such as generating test cases and displaying test results in a friendly format.",
    "crumbs": [
      "Unit Testing",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Writing JUnit Tests and Test-Driven Development</span>"
    ]
  },
  {
    "objectID": "05_writing_junit_tests.html#useful-junit-assertions",
    "href": "05_writing_junit_tests.html#useful-junit-assertions",
    "title": "9  Writing JUnit Tests and Test-Driven Development",
    "section": "9.2 Useful JUnit Assertions",
    "text": "9.2 Useful JUnit Assertions\nJUnit provides a set of methods called assertions that are used to test the expected output of your code. These assertions help verify that your code behaves as expected under different conditions.\nLet’s take a look at some commonly used assertions:\n\n9.2.1 assertEquals\nThis assertion checks if two values are equal:\nassertEquals(expected, actual);\nIf actual is not equal to expected, the assertion fails, and the test method will terminate immediately.\nLet’s rewrite our addTest1 method using JUnit’s assertEquals:\n@Test\npublic void addTest1() {\n    MathUtil m = new MathUtil();\n    int lhs = 5;\n    int rhs = 7;\n\n    assertEquals(lhs + rhs, m.add(lhs, rhs));\n}\n\n\n9.2.2 assertTrue and assertFalse\nThese assertions verify if a condition is true or false, respectively:\nassertTrue(condition);\nassertFalse(condition);\nIf the condition does not meet the expectation (i.e., true for assertTrue and false for assertFalse), the assertion fails, and the test method will terminate immediately.\n\n\n9.2.3 assertNotNull and assertNull\nThese assertions check if an object is null or not:\nassertNotNull(object);\nassertNull(object);\nIf the object does not meet the expectation (i.e., not null for assertNotNull and null for assertNull), the assertion fails, and the test method will terminate immediately.\nThese are just a few examples. JUnit provides a comprehensive set of assertions to cover almost any condition you might want to verify.",
    "crumbs": [
      "Unit Testing",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Writing JUnit Tests and Test-Driven Development</span>"
    ]
  },
  {
    "objectID": "05_writing_junit_tests.html#setting-up-junit",
    "href": "05_writing_junit_tests.html#setting-up-junit",
    "title": "9  Writing JUnit Tests and Test-Driven Development",
    "section": "9.3 Setting Up JUnit",
    "text": "9.3 Setting Up JUnit\nBefore you can use JUnit, you need to make sure the library is on your classpath. This process can vary depending on the IDE and build system you’re using.\nFor our labs, we will ensure the JUnit library is on our classpath by pre-configuring the project and IDE for you. However, if you’re working on your own project, you’ll need to add the JUnit library to your project’s classpath.\nWhen working on your own projects, you might be interested in using a build system like Maven or Gradle to manage your dependencies. These build systems make it easy to add and manage dependencies in your project. For example, if you’re using Maven, you can add the following dependency to your pom.xml file:\n&lt;dependency&gt;\n    &lt;groupId&gt;org.junit.jupiter&lt;/groupId&gt;\n    &lt;artifactId&gt;junit-jupiter-api&lt;/artifactId&gt;\n   \n\n &lt;version&gt;5.7.0&lt;/version&gt;\n    &lt;scope&gt;test&lt;/scope&gt;\n&lt;/dependency&gt;\nThis will automatically download the JUnit library and add it to your project’s classpath.\nRegardless of how you added the JUnit library to your project, next, we need to import the necessary classes and annotations from JUnit. At the top of our MathUtilTest class, we add the following import statements:\nimport static org.junit.jupiter.api.Assertions.*;\nimport org.junit.jupiter.api.Test;\nimport org.junit.jupiter.api.BeforeEach;\nThe first import statement statically imports all assertion methods from Assertions, allowing us to use them directly in our code. The second import statement imports the Test annotation, which we use to denote our test methods. The third import statement imports the BeforeEach annotation, which we’ll discuss in a moment.",
    "crumbs": [
      "Unit Testing",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Writing JUnit Tests and Test-Driven Development</span>"
    ]
  },
  {
    "objectID": "05_writing_junit_tests.html#utilizing-test-and-beforeeach-annotations",
    "href": "05_writing_junit_tests.html#utilizing-test-and-beforeeach-annotations",
    "title": "9  Writing JUnit Tests and Test-Driven Development",
    "section": "9.4 Utilizing @Test and @BeforeEach Annotations",
    "text": "9.4 Utilizing @Test and @BeforeEach Annotations\nIn JUnit, we use the @Test annotation to indicate that a method is a test method. This allows JUnit to automatically discover and run this method as a test.\n@Test\npublic void addTest1() {\n    // test code...\n}\nHowever, what if we have some setup code that we want to run before each test? This is where the @BeforeEach annotation comes in. Any method annotated with @BeforeEach will be run before each @Test method.\nLet’s say we want to create a new MathUtil instance before each test:\nMathUtil m;\n\n@BeforeEach\npublic void setup() {\n    m = new MathUtil();\n}\nNow, before each test method is run, JUnit will first execute the setup method, ensuring that we have a fresh MathUtil instance for each test.",
    "crumbs": [
      "Unit Testing",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Writing JUnit Tests and Test-Driven Development</span>"
    ]
  },
  {
    "objectID": "05_writing_junit_tests.html#interpreting-junit-test-runner-output",
    "href": "05_writing_junit_tests.html#interpreting-junit-test-runner-output",
    "title": "9  Writing JUnit Tests and Test-Driven Development",
    "section": "9.5 Interpreting JUnit Test Runner Output",
    "text": "9.5 Interpreting JUnit Test Runner Output\nUnderstanding the output of the JUnit test runner is crucial for interpreting the results of your tests. This helps you diagnose issues in your code and identify exactly what went wrong. Let’s analyze the output of the junit-platform-console-standalone test runner to get a feel for how this works.\n╷\n├─ JUnit Jupiter ✔\n├─ JUnit Vintage ✔\n│  └─ BinarySearchTreeHiddenTest ✔\n│     ├─ testInsertAndSearch ✔\n│     ├─ testDeleteSingleNode ✔\n│     ├─ testTreeTraversals ✘ expected:&lt;[2, 3, 4, 5, [6, 7], 8]&gt; but was:&lt;[2, 3, 4, 5, [7, 6], 8]&gt;\n│     ├─ testContainsElementNotInTree ✔\n│     ├─ testContainsEmptyTree ✔\n│     ├─ testDeleteEmptyTree ✔\n│     ├─ testContainsElementInTree ✔\n│     ├─ testSearchElementInTree ✔\n│     ├─ testInsertMultipleElements ✔\n│     ├─ testDeleteDuplicateElements ✔\n│     ├─ testDeleteElementNotInTree ✔\n│     ├─ testInsertNegativeNumbers ✔\n│     ├─ testInsertAndSize ✔\n│     ├─ testSearchEmptyTree ✔\n│     ├─ testDeleteNodeWithMultipleElements ✔\n│     ├─ testInsertDuplicatesAndRemove ✔\n│     ├─ testInsertSingleElement ✔\n│     └─ testSearchElementNotInTree ✔\n└─ JUnit Platform Suite ✔\n\nFailures (1):\n  JUnit Vintage:BinarySearchTreeHiddenTest:testTreeTraversals\n    =&gt; org.junit.ComparisonFailure: expected:&lt;[2, 3, 4, 5, [6, 7], 8]&gt; but was:&lt;[2, 3, 4, 5, [7, 6], 8]&gt;\n       DataStructures.BinarySearchTreeHiddenTest.testTreeTraversals(BinarySearchTreeHiddenTest.java:220)\n       [...]\nThe JUnit console output provides a tree structure representing the test execution. The topmost nodes represent the test engines used, in this case, JUnit Jupiter and JUnit Vintage. Underneath each engine are the individual test classes, such as BinarySearchTreeHiddenTest.\nWithin each test class node, there are child nodes representing each test method, such as testInsertAndSearch or testDeleteSingleNode. These methods are marked with a ✔ symbol if they passed, and with a ✘ symbol if they failed. In this case, we see that testTreeTraversals has failed.\nAccompanying the failure symbol is a brief description of the failure, which is the assertion message from the test method. In this example, the test expected the array [2, 3, 4, 5, 6, 7, 8], but received the array [2, 3, 4, 5, 7, 6, 8]. This discrepancy caused the test to fail.\nAfter the tree structure, there is a section titled Failures which provides more detailed information about each failure. For each failure, it lists:\n\nThe test class and method that failed.\nThe type of assertion failure that occurred, which is org.junit.ComparisonFailure in this case.\nThe detailed assertion failure message, which is the same as what’s shown in the tree structure.\nThe location in the code where the failure occurred, which can be very useful (and is often the first thing you should look at when debugging a test failure). In this case, the failure occurred on line 220 of BinarySearchTreeHiddenTest.java (see the DataStructures.BinarySearchTreeHiddenTest.testTreeTraversals(BinarySearchTreeHiddenTest.java:220) line).",
    "crumbs": [
      "Unit Testing",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Writing JUnit Tests and Test-Driven Development</span>"
    ]
  },
  {
    "objectID": "05_writing_junit_tests.html#conclusion",
    "href": "05_writing_junit_tests.html#conclusion",
    "title": "9  Writing JUnit Tests and Test-Driven Development",
    "section": "9.6 Conclusion",
    "text": "9.6 Conclusion\nIn conclusion, JUnit simplifies the process of writing and managing tests. It provides a comprehensive set of assertion methods to verify our code and uses annotations to define and organize our tests, making them easier to read and understand. By taking advantage of these features, we can write more effective and maintainable tests. In the next section, we’ll dive deeper into Test-Driven Development, a methodology that leverages the power of testing to guide and improve the development process.",
    "crumbs": [
      "Unit Testing",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Writing JUnit Tests and Test-Driven Development</span>"
    ]
  },
  {
    "objectID": "05_writing_junit_tests.html#test-driven-development",
    "href": "05_writing_junit_tests.html#test-driven-development",
    "title": "9  Writing JUnit Tests and Test-Driven Development",
    "section": "9.7 Test-Driven Development",
    "text": "9.7 Test-Driven Development\nTest-Driven Development (TDD) is a software development methodology that is centered around the idea of writing tests before writing the actual code. It is a highly disciplined process that follows a strict order of operations: red, green, refactor. This method has profound implications on the design, quality, and reliability of the software.\nLet’s dive into what these steps entail.\n\nRed: Write a test that covers a specific functionality you want to implement. This test should fail initially because you haven’t written the actual code yet. This stage helps you think about the functionality in detail, ponder on the inputs and expected outputs, and outline the structure of your code.\nGreen: Write the minimal amount of code needed to pass the test. At this stage, don’t worry about the elegance of your code. Your primary focus is on functionality. Run your test, and it should pass this time.\nRefactor: Refactor the code you just wrote in the green stage to eliminate duplication, improve readability, and ensure the code adheres to the best practices. After refactoring, all tests should still pass. If a test fails, it means the refactoring broke the functionality, and you need to revise your changes.\n\nThis cycle repeats for every small chunk of functionality you add to your software. With this approach, you are incrementally building your software with the assurance that at each step, the implemented functionality is working as expected.\n\n9.7.1 The Motivation Behind Test-Driven Development\nYou might be wondering, why would you want to put in the extra effort to write tests before writing the actual code? Here are a few motivating factors:\n\nConfidence: With TDD, you can be confident that your code works because you have tests that prove it. This confidence is especially important when you need to modify your code later. Changes can break existing functionality, but with a robust set of tests, you can quickly catch and fix these regressions.\nBetter Design: Writing tests first forces you to think about your code from a user’s perspective. This shift in viewpoint often results in better code organization and modularity because you design your code to be easy to test, which typically means it is also easy to use and modify.\nDocumentation: Tests act as a form of documentation that shows how the code is supposed to work. New team members can look at the tests to understand what each function is supposed to do and what edge cases it handles.\nDevelopment Speed: While TDD might seem to slow you down at the beginning, it typically results in faster development in the long run. With TDD, you spend less time debugging and fixing bugs because you catch them early in the development process, before they become entangled with other parts of the code.\n\nIn conclusion, TDD is a powerful methodology that can significantly improve the quality of your code and your efficiency as a developer. While it might seem difficult at first, with practice, it becomes a natural part of the development process.",
    "crumbs": [
      "Unit Testing",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Writing JUnit Tests and Test-Driven Development</span>"
    ]
  },
  {
    "objectID": "06_mutation_testing.html",
    "href": "06_mutation_testing.html",
    "title": "10  Introduction to Mutation Testing",
    "section": "",
    "text": "10.1 Unit Testing, a Recap\nWriting correct code is challenging. Owning your code, maintaining it, and ensuring it always performs as expected can be even more difficult. After all, we are human, and mistakes are a part of our nature. Therefore, we can’t always rely solely on our ability to catch these mistakes. Instead, automated processes that can help us identify and correct these errors are truly invaluable.\nUnit testing is one such automated process that tests your code. These tests target individual units of your code and are designed to verify their behavior. In programming, a method can be thought of as the smallest unit of code, and Classes, Packages, and Modules are larger units of code. Every unit test is designed to validate a single behavior of a single unit of code.\nUnit testing is so crucial that they are a mandatory requirement for many software development projects. For instance, if you are contributing a Java source class to most projects, you are also required to provide unit tests for that class. In this course, where we deal with Data Structures and Algorithms, learning how to implement them in Java, etc., submitting your implementations without Unit tests would be like submitting a paper without a bibliography. It might make us question, “Sure, you wrote this and it looks convincing, but where’s your evidence? Why should I believe you?”\nAnd that’s why you will be required to write unit tests for your Data Structures and Algorithms implementations. But then, how do you know that your unit tests are correct?\nConsider this example:\nThe test passes, but does it mean it’s correct? Let’s examine the test.",
    "crumbs": [
      "Unit Testing",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Introduction to Mutation Testing</span>"
    ]
  },
  {
    "objectID": "06_mutation_testing.html#unit-testing-a-recap",
    "href": "06_mutation_testing.html#unit-testing-a-recap",
    "title": "10  Introduction to Mutation Testing",
    "section": "",
    "text": "@Test\npublic void testAdd() {\n   assertTrue(1 == 1);\n}",
    "crumbs": [
      "Unit Testing",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Introduction to Mutation Testing</span>"
    ]
  },
  {
    "objectID": "06_mutation_testing.html#writing-correct-unit-tests",
    "href": "06_mutation_testing.html#writing-correct-unit-tests",
    "title": "10  Introduction to Mutation Testing",
    "section": "10.2 Writing Correct Unit Tests",
    "text": "10.2 Writing Correct Unit Tests\nTaking a closer look at the unit test above, two problems become evident:\n\nThe add method isn’t called in the test.\nThe test can’t fail.\n\nTo address the first issue, we can leverage tools that detect when a unit test doesn’t cover a particular line of code. Here “cover” means “execute”.\nSo what about a test like this?\n@Test\npublic void testAdd() {\n   assertTrue(add(1, 1) == add(1, 1));\n}\nThe add method is called, and test coverage is at 100%. However, this test can’t fail, meaning it is not correct.",
    "crumbs": [
      "Unit Testing",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Introduction to Mutation Testing</span>"
    ]
  },
  {
    "objectID": "06_mutation_testing.html#mutation-testing-a-solution-to-test-the-correctness-of-your-unit-tests",
    "href": "06_mutation_testing.html#mutation-testing-a-solution-to-test-the-correctness-of-your-unit-tests",
    "title": "10  Introduction to Mutation Testing",
    "section": "10.3 Mutation Testing: A Solution to Test the Correctness of Your Unit Tests",
    "text": "10.3 Mutation Testing: A Solution to Test the Correctness of Your Unit Tests\nMutation testing provides a way to test the correctness of your unit tests. It can help you find tests that can’t fail, tests that need more test cases, and even logic errors in your code.\nThe basic idea of mutation testing is as follows: if you claim your source code is correct, and that the tests prove it, mutation testing will challenge that claim. Mutation testing introduces small changes, called mutations, to your source code. If your tests still pass after these changes, it suggests that your test or source code may not be correct.\nThese mutations are created by algorithms called mutators or mutation operators. Each time a mutator runs, it receives a fresh copy of your source code and makes only one change. The result of a mutator is a mutant - a mutated version of your source code.",
    "crumbs": [
      "Unit Testing",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Introduction to Mutation Testing</span>"
    ]
  },
  {
    "objectID": "06_mutation_testing.html#mutation-testing-in-practice",
    "href": "06_mutation_testing.html#mutation-testing-in-practice",
    "title": "10  Introduction to Mutation Testing",
    "section": "10.4 Mutation Testing in Practice",
    "text": "10.4 Mutation Testing in Practice\nHere’s how mutation testing works in practice:\n\nApply a set of mutators to your source code to produce a collection of mutants.\nFor each mutant, run your unit tests.\n\nIf the unit tests pass, the mutant has survived.\nIf the unit tests fail, the mutant was killed\n\n\n.\nThe aim is to kill as many mutants as possible. If too many mutants survive, it indicates that your unit tests are not sufficient to prove that your source code is correct. If you manage to kill all mutants, you can use your unit tests to argue that your source code is indeed correct.\nIn our course, only unit tests that leave no surviving mutants will be accepted as a valid submission.\n\n10.4.1 What are these mutators?\nMutators introduce specific types of changes to your code. For instance, a primitive returns mutator (PRIMITIVE_RETURNS) replaces int, short, long, char, float, and double return values with 0. So, for example, this method:\npublic int add(int lhs, int rhs) {\n    return lhs + rhs;\n}\nbecomes:\npublic int add(int lhs, int rhs) {\n    return 0;\n}\nThis mutant would survive in two cases: if the add method is never tested or if the only test cases for add are ones where the result is 0. To fix this, you can add more test cases that test add with non-zero results.\nAnother example of a mutator is the Remove Conditionals Mutator (REMOVE_CONDITIONALS), which removes all conditional statements such that the guarded statements always execute.\nFor example, this code:\nif (a == b) {\n  // do something\n}\nbecomes:\nif (true) {\n  // do something\n}\nThis mutant would survive if the source method is never tested, if all test cases for the source method only ever test the true case, or if both branches have the exact same code or equivalent code, to begin with. To fix this, you can add more test cases that test the false case, and ensure that the true and false branches are not equivalent.\nThrough mutation testing, you will get valuable feedback to improve your code and tests, making them more robust and reliable.\nRemember, a well-tested code base is not just about coverage—it’s about the quality of tests, their ability to catch mistakes, and their resilience against possible errors. Mutation testing is an invaluable tool in achieving this.\nCertainly, let’s decipher the feedback from the Coding Rooms.",
    "crumbs": [
      "Unit Testing",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Introduction to Mutation Testing</span>"
    ]
  },
  {
    "objectID": "06_mutation_testing.html#understanding-codingrooms-feedback",
    "href": "06_mutation_testing.html#understanding-codingrooms-feedback",
    "title": "10  Introduction to Mutation Testing",
    "section": "10.5 Understanding CodingRooms Feedback",
    "text": "10.5 Understanding CodingRooms Feedback\nIn the feedback provided by the autograder in Coding Rooms, you can find two main parts. The first part lists the mutation tests that have been run, and the second part provides a grading overview.\n\n10.5.1 Understanding Mutation Test Feedback\nEach mutation test feedback starts with “Running Mutation tests”, followed by details about each mutation test performed.\nRunning Mutation tests -\nRan mutation tests for Calculator.CalculatorTest -\n-[ RECORD 0 ]---------+------------------------------------\nMutation type         | RemoveConditionalMutator_EQUAL_ELSE\nSource method mutated | divide\nLine no. of mutation  | 56\nTest examined         | None\nResult                | SURVIVED\nIn the example above, the autograder has run a mutation test on the “divide” method of your “Calculator” class. Here is what each line in the record means:\n\nMutation type: The type of mutation that was made to your code. In this case, a RemoveConditionalMutator_EQUAL_ELSE mutation was made. This mutation type removes a conditional (==) operator and always takes the else branch.\nSource method mutated: The method in your code that was mutated for the test. In this case, it’s the “divide” method.\nLine no. of mutation: The line number in your code where the mutation was applied.\nTest examined: This line indicates which test case was run against the mutant. “None” means no specific test case was chosen, and all available tests were run.\nResult: The outcome of the mutation test. If your tests fail against the mutated code (which is a good thing!), this line will read “KILLED”. If your tests pass (which implies your tests didn’t catch the error introduced by the mutation), this line will read “SURVIVED”. In this case, the mutant has survived, indicating your tests didn’t catch the error.\n\nIf any mutants survive, the autograder lists those under the line, “Problematic mutation test failures printed about.”\n\n\n10.5.2 Understanding the Grading Overview\nThe grading overview gives you a quick summary of how your code has performed in the tests. It breaks down the grading by requirements.\n┌──────────────────────────────────────────────────────┬\n│                   Grading Overview                   │\n├──────────────────────────────────────────────────────┼\n│ Requirement │    Grade    │          Reason          │\n├─────────────┼─────────────┼──────────────────────────┤\n│      1      │ 10.00/10.00 │   - 4/4 tests passing.   │\n├─────────────┼─────────────┼──────────────────────────┤\n│      2      │ 32.00/40.00 │ -8 Penalty due to surviv │\n│             │             │       ing muations       │\n├─────────────┼─────────────┼──────────────────────────┤\n│                  Total: 42.00/50.00                  │\n└──────────────────────────────────────────────────────┴\nIn the example above, we have:\n\nRequirement 1: This section corresponds to the first requirement of the assignment. The student received a full score (10.00/10.00) because all 4 test cases associated with this requirement passed.\nRequirement 2: This section corresponds to the second requirement of the assignment. The student lost 8 points because of surviving mutations, resulting in a score of 32.00/40.00 for this requirement.\n\nThe total grade of the assignment is\n42.00/50.00, indicating the need for further improvement.\nRemember, each surviving mutation indicates a flaw in your test cases—they didn’t catch an erroneous mutation. To improve your grade, aim to update your test cases to ensure that they effectively detect the mutations.",
    "crumbs": [
      "Unit Testing",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Introduction to Mutation Testing</span>"
    ]
  },
  {
    "objectID": "07_list_of_mutators.html",
    "href": "07_list_of_mutators.html",
    "title": "11  List of Mutators",
    "section": "",
    "text": "11.1 Conditionals Boundary Mutator (CONDITIONALS_BOUNDARY)\nThe conditionals boundary mutator replaces the relational operators &lt;, &lt;=, &gt;, &gt;=\nwith their boundary counterpart as per the table below.\n{:.table }\nFor example\nwill be mutated to",
    "crumbs": [
      "Unit Testing",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>List of Mutators</span>"
    ]
  },
  {
    "objectID": "07_list_of_mutators.html#conditionals-boundary-mutator-conditionals_boundary",
    "href": "07_list_of_mutators.html#conditionals-boundary-mutator-conditionals_boundary",
    "title": "11  List of Mutators",
    "section": "",
    "text": "Original conditional\nMutated conditional\n\n\n\n\n&lt;\n&lt;=\n\n\n&lt;=\n&lt;\n\n\n&gt;\n&gt;=\n\n\n&gt;=\n&gt;\n\n\n\n\n\nif (a &lt; b) {\n  // do something\n}\n\nif (a &lt;= b) {\n  // do something\n}",
    "crumbs": [
      "Unit Testing",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>List of Mutators</span>"
    ]
  },
  {
    "objectID": "07_list_of_mutators.html#increments-mutator-increments",
    "href": "07_list_of_mutators.html#increments-mutator-increments",
    "title": "11  List of Mutators",
    "section": "11.2 Increments Mutator (INCREMENTS)",
    "text": "11.2 Increments Mutator (INCREMENTS)\nThe increments mutator will mutate increments, decrements and assignment increments and decrements of local variables (stack variables). It will replace increments with decrements and vice versa.\nFor example\npublic int method(int i) {\n  i++;\n  return i;\n}\nwill be mutated to\npublic int method(int i) {\n  i--;\n  return i;\n}\nPlease note that the increments mutator will be applied to increments of local variables only. Increments and decrements of member variables will be covered by the Math Mutator.",
    "crumbs": [
      "Unit Testing",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>List of Mutators</span>"
    ]
  },
  {
    "objectID": "07_list_of_mutators.html#invert-negatives-mutator-invert_negs",
    "href": "07_list_of_mutators.html#invert-negatives-mutator-invert_negs",
    "title": "11  List of Mutators",
    "section": "11.3 Invert Negatives Mutator (INVERT_NEGS)",
    "text": "11.3 Invert Negatives Mutator (INVERT_NEGS)\nThe invert negatives mutator inverts negation of integer and floating point numbers. For example\npublic float negate(final float i) {\n  return -i;\n}\nwill be mutated to\npublic float negate(final float i) {\n  return i;\n}",
    "crumbs": [
      "Unit Testing",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>List of Mutators</span>"
    ]
  },
  {
    "objectID": "07_list_of_mutators.html#math-mutator-math",
    "href": "07_list_of_mutators.html#math-mutator-math",
    "title": "11  List of Mutators",
    "section": "11.4 Math Mutator (MATH)",
    "text": "11.4 Math Mutator (MATH)\nThe math mutator replaces binary arithmetic operations for either integer or floating-point arithmetic with another operation. The replacements will be selected according to the table below.\n\n\n\nOriginal conditional\nMutated conditional\n\n\n\n\n+\n-\n\n\n-\n+\n\n\n*\n/\n\n\n/\n*\n\n\n%\n*\n\n\n&\n|\n\n\n|\n&\n\n\n^\n&\n\n\n&lt;&lt;\n&gt;&gt;\n\n\n&gt;&gt;\n&lt;&lt;\n\n\n&gt;&gt;&gt;\n&lt;&lt;\n\n\n\n{:.table}\nFor example\nint a = b + c;\nwill be mutated to\nint a = b - c;\nKeep in mind that the + operator on Strings as in\nString a = \"foo\" + \"bar\";\nis not a mathematical operator but a string concatenation and will be replaced by the compiler with something like\nString a = new StringBuilder(\"foo\").append(\"bar\").toString();\nPlease note that the compiler will also use binary arithmetic operations for increments, decrements and assignment increments and decrements of non-local variables (member variables) although a special iinc opcode for increments exists. This special opcode is restricted to local variables (also called stack variables) and cannot be used for member variables. That means the math mutator will also mutate\npublic class A {\n  private int i;\n\n  public void foo() {\n    this.i++;\n  }\n}\nto\npublic class A {\n  private int i;\n\n  public void foo() {\n    this.i = this.i - 1;\n  }\n}\nSee the Increments Mutator for details.",
    "crumbs": [
      "Unit Testing",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>List of Mutators</span>"
    ]
  },
  {
    "objectID": "07_list_of_mutators.html#negate-conditionals-mutator-negate_conditionals",
    "href": "07_list_of_mutators.html#negate-conditionals-mutator-negate_conditionals",
    "title": "11  List of Mutators",
    "section": "11.5 Negate Conditionals Mutator (NEGATE_CONDITIONALS)",
    "text": "11.5 Negate Conditionals Mutator (NEGATE_CONDITIONALS)\nThe negate conditionals mutator will mutate all conditionals found according to the replacement table below.\n\n\n\nOriginal conditional\nMutated conditional\n\n\n\n\n==\n!=\n\n\n!=\n==\n\n\n&lt;=\n&gt;\n\n\n&gt;=\n&lt;\n\n\n&lt;\n&gt;=\n\n\n&gt;\n&lt;=\n\n\n\n{:.table }\nFor example\nif (a == b) {\n  // do something\n}\nwill be mutated to\nif (a != b) {\n  // do something\n}\nThis mutator overlaps to a degree with the conditionals boundary mutator, but is less stable i.e these mutations are generally easier for a test suite to detect.",
    "crumbs": [
      "Unit Testing",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>List of Mutators</span>"
    ]
  },
  {
    "objectID": "07_list_of_mutators.html#return-values-mutator-return_vals",
    "href": "07_list_of_mutators.html#return-values-mutator-return_vals",
    "title": "11  List of Mutators",
    "section": "11.6 Return Values Mutator (RETURN_VALS)",
    "text": "11.6 Return Values Mutator (RETURN_VALS)\nThis mutator has been superseded by the new returns mutator set. See Empty returns, False returns, True returns, Null returns and Primitive returns.\nThe return values mutator mutates the return values of method calls. Depending on the return type of the method another mutation is used.4\n\n\n\n\n\n\n\nReturn Type\nMutation\n\n\n\n\nboolean\nreplace the unmutated return value true with false and replace the unmutated return value false with true\n\n\nint byte short\nif the unmutated return value is 0 return 1, otherwise mutate to return value 0\n\n\nlong\nreplace the unmutated return value x with the result of x+1\n\n\nfloat double\nreplace the unmutated return value x with the result of -(x+1.0) if x is not NAN and replace NAN with 0\n\n\nObject\nreplace non-null return values with null and throw a java.lang.RuntimeException if the unmutated method would return null\n\n\n\n{:.table}\nFor example\npublic Object foo() {\n  return new Object();\n}\nwill be mutated to\npublic Object foo() {\n  new Object();\n  return null;\n}\nPlease note that constructor calls are not considered void method calls. See the Constructor Call Mutator for mutations of constructors or the Non Void Method Call Mutator for mutations of non void methods.",
    "crumbs": [
      "Unit Testing",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>List of Mutators</span>"
    ]
  },
  {
    "objectID": "07_list_of_mutators.html#void-method-call-mutator-void_method_calls",
    "href": "07_list_of_mutators.html#void-method-call-mutator-void_method_calls",
    "title": "11  List of Mutators",
    "section": "11.7 Void Method Call Mutator (VOID_METHOD_CALLS)",
    "text": "11.7 Void Method Call Mutator (VOID_METHOD_CALLS)\nThe void method call mutator removes method calls to void methods. For example\npublic void someVoidMethod(int i) {\n  // does something\n}\n\npublic int foo() {\n  int i = 5;\n  someVoidMethod(i);\n  return i;\n}\nwill be mutated to\npublic void someVoidMethod(int i) {\n  // does something\n}\n\npublic int foo() {\n  int i = 5;\n  return i;\n}",
    "crumbs": [
      "Unit Testing",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>List of Mutators</span>"
    ]
  },
  {
    "objectID": "07_list_of_mutators.html#empty-returns-mutator-empty_returns",
    "href": "07_list_of_mutators.html#empty-returns-mutator-empty_returns",
    "title": "11  List of Mutators",
    "section": "11.8 Empty returns Mutator (EMPTY_RETURNS)",
    "text": "11.8 Empty returns Mutator (EMPTY_RETURNS)\nReplaces return values with an ‘empty’ value for that type as follows\n\njava.lang.String -&gt; “”\njava.util.Optional -&gt; Optional.empty()\njava.util.List -&gt; Collections.emptyList()\njava.util.Collection -&gt; Collections.emptyList()\njava.util.Set -&gt; Collections.emptySet()\njava.lang.Integer -&gt; 0\njava.lang.Short -&gt; 0\njava.lang.Long -&gt; 0\njava.lang.Character -&gt; 0\njava.lang.Float -&gt; 0\njava.lang.Double -&gt; 0\n\nPitest will filter out equivalent mutations to methods that are already hard coded to return the empty value.",
    "crumbs": [
      "Unit Testing",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>List of Mutators</span>"
    ]
  },
  {
    "objectID": "07_list_of_mutators.html#false-returns-mutator-false_returns",
    "href": "07_list_of_mutators.html#false-returns-mutator-false_returns",
    "title": "11  List of Mutators",
    "section": "11.9 False returns Mutator (FALSE_RETURNS)",
    "text": "11.9 False returns Mutator (FALSE_RETURNS)\nReplaces primitive and boxed boolean return values with false.\nPitest will filter out equivalent mutations to methods that are already hard coded to return false.",
    "crumbs": [
      "Unit Testing",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>List of Mutators</span>"
    ]
  },
  {
    "objectID": "07_list_of_mutators.html#true-returns-mutator-true_returns",
    "href": "07_list_of_mutators.html#true-returns-mutator-true_returns",
    "title": "11  List of Mutators",
    "section": "11.10 True returns Mutator (TRUE_RETURNS)",
    "text": "11.10 True returns Mutator (TRUE_RETURNS)\nReplaces primitive and boxed boolean return values with true.\nPitest will filter out equivalent mutations to methods that are already hard coded to return true.",
    "crumbs": [
      "Unit Testing",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>List of Mutators</span>"
    ]
  },
  {
    "objectID": "07_list_of_mutators.html#null-returns-mutator-null_returns",
    "href": "07_list_of_mutators.html#null-returns-mutator-null_returns",
    "title": "11  List of Mutators",
    "section": "11.11 Null returns Mutator (NULL_RETURNS)",
    "text": "11.11 Null returns Mutator (NULL_RETURNS)\nReplaces return values with null. Methods that can be mutated by the EMPTY_RETURNS mutator or that are directly annotated with NotNull will not be mutated.\nPitest will filter out equivalent mutations to methods that are already hard coded to return null.",
    "crumbs": [
      "Unit Testing",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>List of Mutators</span>"
    ]
  },
  {
    "objectID": "07_list_of_mutators.html#primitive-returns-mutator-primitive_returns",
    "href": "07_list_of_mutators.html#primitive-returns-mutator-primitive_returns",
    "title": "11  List of Mutators",
    "section": "11.12 Primitive returns Mutator (PRIMITIVE_RETURNS)",
    "text": "11.12 Primitive returns Mutator (PRIMITIVE_RETURNS)\nReplaces int, short, long, char, float and double return values with 0.\nPitest will filter out equivalent mutations to methods that are already hard coded to return 0.",
    "crumbs": [
      "Unit Testing",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>List of Mutators</span>"
    ]
  },
  {
    "objectID": "07_list_of_mutators.html#constructor-call-mutator-constructor_calls",
    "href": "07_list_of_mutators.html#constructor-call-mutator-constructor_calls",
    "title": "11  List of Mutators",
    "section": "11.13 Constructor Call Mutator (CONSTRUCTOR_CALLS)",
    "text": "11.13 Constructor Call Mutator (CONSTRUCTOR_CALLS)\nOptional mutator that replaces constructor calls with null values. For example\npublic Object foo() {\n  Object o = new Object();\n  return o;\n}\nwill be mutated to\npublic Object foo() {\n  Object o = null;\n  return o;\n}\nPlease note that this mutation is fairly unstable and likely to cause NullPointerExceptions even with weak test suites.\nThis mutator does not affect non constructor method calls. See Void Method Call Mutator for mutations of void methods and Non Void Method Call Mutator for mutations of non void methods.",
    "crumbs": [
      "Unit Testing",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>List of Mutators</span>"
    ]
  },
  {
    "objectID": "08_sorting_intro.html",
    "href": "08_sorting_intro.html",
    "title": "12  Introduction to Sorting",
    "section": "",
    "text": "12.1 Understanding the Sorting Problem\nSorting is an essential problem in computer science and software engineering. Simply put, sorting is the task of rearranging a set of items, such as an array, in a specific order. For instance, an array of integers might be sorted in ascending or descending order based on their numeric value.\nOne might wonder why sorting is so ubiquitous in the field of software engineering. The answer lies in the multitude of applications that it has. Sorting allows for more efficient searching, data compression, and organization. It is a crucial step in numerous algorithms and computational tasks. The efficiency of a sorting algorithm can significantly affect the performance of these applications.",
    "crumbs": [
      "Iterative Sorting and Searching",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to Sorting</span>"
    ]
  },
  {
    "objectID": "08_sorting_intro.html#developing-a-sorting-algorithm-from-scratch",
    "href": "08_sorting_intro.html#developing-a-sorting-algorithm-from-scratch",
    "title": "12  Introduction to Sorting",
    "section": "12.2 Developing a Sorting Algorithm from Scratch",
    "text": "12.2 Developing a Sorting Algorithm from Scratch\nOne might find the task of developing a sorting algorithm from scratch quite daunting. Sorting algorithms, like many computational problems, have an abstract and often complex nature that can make it difficult to understand, let alone create. If shown the solution, one might understand how it works and even memorize it, but the thought process behind creating such a solution can be elusive.\nA common approach to overcoming such challenges is to simplify the problem. By breaking down a complex problem into simpler subproblems, we can gain insights into the problem’s structure and ultimately develop a solution. This approach will be demonstrated as we develop a sorting algorithm.",
    "crumbs": [
      "Iterative Sorting and Searching",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to Sorting</span>"
    ]
  },
  {
    "objectID": "08_sorting_intro.html#starting-simple-sorting-an-array-of-size-1",
    "href": "08_sorting_intro.html#starting-simple-sorting-an-array-of-size-1",
    "title": "12  Introduction to Sorting",
    "section": "12.3 Starting Simple: Sorting an Array of Size 1",
    "text": "12.3 Starting Simple: Sorting an Array of Size 1\nThe simplest possible sorting problem involves an array of size one. But, of course, this array is already sorted as it only contains a single element. Thus, we need to do nothing to sort this array. Here’s how this can be implemented in Java:\npublic static void sort1(int[] arr) {\n  // do nothing\n}",
    "crumbs": [
      "Iterative Sorting and Searching",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to Sorting</span>"
    ]
  },
  {
    "objectID": "08_sorting_intro.html#sorting-an-array-of-size-2",
    "href": "08_sorting_intro.html#sorting-an-array-of-size-2",
    "title": "12  Introduction to Sorting",
    "section": "12.4 Sorting an Array of Size 2",
    "text": "12.4 Sorting an Array of Size 2\nWhat about an array of size two? In this case, we simply need to compare the two elements and swap them if they are out of order:\npublic static void sort2(int[] arr) {\n    if (arr[0] &gt; arr[1]) {\n      int temp = arr[0];\n      arr[0] = arr[1];\n      arr[1] = temp;\n    }\n}",
    "crumbs": [
      "Iterative Sorting and Searching",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to Sorting</span>"
    ]
  },
  {
    "objectID": "08_sorting_intro.html#expanding-the-problem-sorting-an-array-of-size-3",
    "href": "08_sorting_intro.html#expanding-the-problem-sorting-an-array-of-size-3",
    "title": "12  Introduction to Sorting",
    "section": "12.5 Expanding the Problem: Sorting an Array of Size 3",
    "text": "12.5 Expanding the Problem: Sorting an Array of Size 3\nWhen sorting an array of size three, we perform similar steps as we did for the array of size two. First, we compare and potentially swap the first two numbers. Then, we do the same for the second and third numbers. Finally, we compare the first two numbers once again:\npublic static void sort3(int[] arr) {\n    if (arr[0] &gt; arr[1]) {\n      int temp = arr[0];\n      arr[0] = arr[1];\n      arr[1] = temp;\n    }\n    if (arr[1] &gt; arr[2]) {\n      int temp = arr[1];\n      arr[1] = arr[2];\n      arr[2] = temp;\n    }\n    if (arr[0] &gt; arr[1]) {\n      int temp = arr[0];\n      arr[0] = arr[1];\n      arr[1] = temp;\n    }\n}\nBy following this process, we ensure that the array is sorted, regardless of the initial order of the elements.",
    "crumbs": [
      "Iterative Sorting and Searching",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to Sorting</span>"
    ]
  },
  {
    "objectID": "08_sorting_intro.html#scaling-up-sorting-an-array-of-size-4",
    "href": "08_sorting_intro.html#scaling-up-sorting-an-array-of-size-4",
    "title": "12  Introduction to Sorting",
    "section": "12.6 Scaling Up: Sorting an Array of Size 4",
    "text": "12.6 Scaling Up: Sorting an Array of Size 4\nWe can extend the same approach to sort an array of size four. However, notice that once we have the largest number at the last position, the problem\nreduces to sorting an array of size three. This realization will be crucial as we move forward.\npublic static void sort4(int[] arr) {\n    for(int i = 0; i &lt; 3; i++) {\n      if (arr[i] &gt; arr[i+1]) {\n        int temp = arr[i];\n        arr[i] = arr[i+1];\n        arr[i+1] = temp;\n      }\n    }\n    sort3(arr);\n}",
    "crumbs": [
      "Iterative Sorting and Searching",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to Sorting</span>"
    ]
  },
  {
    "objectID": "08_sorting_intro.html#recognizing-the-pattern",
    "href": "08_sorting_intro.html#recognizing-the-pattern",
    "title": "12  Introduction to Sorting",
    "section": "12.7 Recognizing the Pattern",
    "text": "12.7 Recognizing the Pattern\nAs we incrementally solve larger versions of our sorting problem, we start to notice a pattern. For every array size, we first ensure the largest number ends up at the end of the array. Then, we apply the same sorting logic to the remaining elements in the array.\nFor instance, when sorting an array of size 5, we first find and move the largest number to the end of the array, then apply the sorting process for an array of size 4 to the remaining numbers:\npublic static void sort5(int[] arr) {\n  for(int i = 0; i &lt; 4; i++) {\n    if (arr[i] &gt; arr[i+1]) {\n      int temp = arr[i];\n      arr[i] = arr[i+1];\n      arr[i+1] = temp;\n    }\n  }\n  sort4(arr);\n}",
    "crumbs": [
      "Iterative Sorting and Searching",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to Sorting</span>"
    ]
  },
  {
    "objectID": "08_sorting_intro.html#generalizing-the-algorithm",
    "href": "08_sorting_intro.html#generalizing-the-algorithm",
    "title": "12  Introduction to Sorting",
    "section": "12.8 Generalizing the Algorithm",
    "text": "12.8 Generalizing the Algorithm\nThis pattern suggests a way to generalize our algorithm to sort an array of any size. For an array of size n, we first ensure the largest number ends up at the end, then recursively apply the same process to the remaining n-1 elements. By repeatedly applying this process, we ensure that the entire array ends up sorted.\npublic static void sort(int[] arr) {\n  for(int i = arr.length - 1; i &gt; 0; i--) {\n    for(int j = 0; j &lt; i; j++) {\n      if (arr[j] &gt; arr[j+1]) {\n        int temp = arr[j];\n        arr[j] = arr[j+1];\n        arr[j+1] = temp;\n      }\n    }\n  }\n}\nThis algorithm, known as Bubble Sort, represents a simple yet effective solution to the sorting problem. It demonstrates how complex problems can often be broken down into simpler subproblems, providing a valuable lesson for problem-solving in computer science and beyond.",
    "crumbs": [
      "Iterative Sorting and Searching",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to Sorting</span>"
    ]
  },
  {
    "objectID": "09_comparable_comparator.html",
    "href": "09_comparable_comparator.html",
    "title": "13  Comparable and Comparator",
    "section": "",
    "text": "13.1 Sorting Complex Objects\nIn our previous lessons, we have primarily dealt with arrays of basic data types, such as integers, which are inherently easy to compare and sort. However, software development often involves sorting collections of complex objects, such as Employee or Student objects. These user-defined types lack a natural order for comparison, necessitating a custom solution.",
    "crumbs": [
      "Iterative Sorting and Searching",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Comparable and Comparator</span>"
    ]
  },
  {
    "objectID": "09_comparable_comparator.html#defining-a-comparison-method",
    "href": "09_comparable_comparator.html#defining-a-comparison-method",
    "title": "13  Comparable and Comparator",
    "section": "13.2 Defining a Comparison Method",
    "text": "13.2 Defining a Comparison Method\nA rudimentary solution could involve appending a comparison method to our Employee or Student classes, like compareSalary or compareGPA. However, this design lacks versatility - each new attribute requires a fresh comparison method.\nA superior solution employs the Comparable interface, comprising a single method, compareTo. This method, accepting an object of the same type, returns an integer indicating if the current object is lesser than, equal to, or greater than the input object.\nThe Comparable interface in Java is defined as follows:\npublic interface Comparable&lt;T&gt; {\n    int compareTo(T o);\n}",
    "crumbs": [
      "Iterative Sorting and Searching",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Comparable and Comparator</span>"
    ]
  },
  {
    "objectID": "09_comparable_comparator.html#implementing-comparable",
    "href": "09_comparable_comparator.html#implementing-comparable",
    "title": "13  Comparable and Comparator",
    "section": "13.3 Implementing Comparable",
    "text": "13.3 Implementing Comparable\nImplementing the Comparable interface in any class endows it with sortable attributes. For example, sorting Student objects by GPA could be achieved by implementing Comparable&lt;Student&gt; in the Student class, defining compareTo to compare GPAs:\npublic class Student implements Comparable&lt;Student&gt; {\n    private double gpa;\n    // other fields and methods...\n\n    @Override\n    public int compareTo(Student other) {\n        return Double.compare(this.gpa, other.gpa);\n    }\n}\nThe array of Student objects can be sorted with our new comparison logic, arr[j].compareTo(arr[j+1]) &gt; 0:\npublic static &lt;T extends Comparable&lt;T&gt;&gt; void sort(T[] arr) {\n    for(int i = arr.length - 1; i &gt; 0; i--) {\n        for(int j = 0; j &lt; i; j++) {\n            if (arr[j].compareTo(arr[j+1]) &gt; 0) {\n                T temp = arr[j];\n                arr[j] = arr[j+1];\n                arr[j+1] = temp;\n            }\n        }\n    }\n}",
    "crumbs": [
      "Iterative Sorting and Searching",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Comparable and Comparator</span>"
    ]
  },
  {
    "objectID": "09_comparable_comparator.html#comparable-in-the-standard-library",
    "href": "09_comparable_comparator.html#comparable-in-the-standard-library",
    "title": "13  Comparable and Comparator",
    "section": "13.4 Comparable in the Standard Library",
    "text": "13.4 Comparable in the Standard Library\nThe Comparable interface facilitates the creation of versatile sorting functions, capable of sorting arrays of any type implementing Comparable. This applies to user-defined types, as well as many built-in types in the Java standard library.\nImplementing the Comparable interface communicates that a class is able to be compared with other instances of its type. This standard behavior empowers methods to interact with objects more abstractly and flexibly.\nThe Java standard library provides a Comparable interface similar to the one defined earlier. Many built-in classes such as Integer, Double, and String already implement this interface, enabling their comparison and sorting with no additional code. Our Student class can be modified to use java.lang.Comparable in place of our custom interface, maintaining the same compareTo method for use in our generic sort function.",
    "crumbs": [
      "Iterative Sorting and Searching",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Comparable and Comparator</span>"
    ]
  },
  {
    "objectID": "09_comparable_comparator.html#sorting-flexibility-with-comparator",
    "href": "09_comparable_comparator.html#sorting-flexibility-with-comparator",
    "title": "13  Comparable and Comparator",
    "section": "13.5 Sorting Flexibility with Comparator",
    "text": "13.5 Sorting Flexibility with Comparator\nThe Comparable interface is limited to a single compareTo method per class. When multiple sorting methods are required, Java’s Comparator interface comes into play. Comparator represents different orderings for a specific type, allowing multiple comparators per class.\nFor instance, a Comparator for Student objects that orders by GPA could look like this:\nimport java\n\n.util.Comparator;\n\npublic class StudentGPAComparator implements Comparator&lt;Student&gt; {\n    @Override\n    public int compare(Student s1, Student s2) {\n        return Double.compare(s1.getGPA(), s2.getGPA());\n    }\n}\nA modified version of our generic sort function, now accepting a Comparator, can compare elements of the array:\npublic static &lt;T&gt; void sort(T[] arr, Comparator&lt;? super T&gt; comparator) {\n    for(int i = arr.length - 1; i &gt; 0; i--) {\n        for(int j = 0; j &lt; i; j++) {\n            if (comparator.compare(arr[j], arr[j+1]) &gt; 0) {\n                T temp = arr[j];\n                arr[j] = arr[j+1];\n                arr[j+1] = temp;\n            }\n        }\n    }\n}",
    "crumbs": [
      "Iterative Sorting and Searching",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Comparable and Comparator</span>"
    ]
  },
  {
    "objectID": "09_comparable_comparator.html#understanding-natural-and-total-orderings",
    "href": "09_comparable_comparator.html#understanding-natural-and-total-orderings",
    "title": "13  Comparable and Comparator",
    "section": "13.6 Understanding Natural and Total Orderings",
    "text": "13.6 Understanding Natural and Total Orderings\nImplementing Comparable endows a class with a natural ordering, a default ordering used in operations like sorting. For example, String objects follow lexicographic order and Integer objects adhere to numerical order. When Comparable&lt;Student&gt; is implemented to compare GPAs, we establish GPA as the natural ordering for Student objects.\nJava’s Comparator interface allows the definition of additional orderings, known as total orderings. While the natural ordering is default, total orderings, which also follow rules of completeness, transitivity, and antisymmetry, provide flexibility to define any suitable ordering.\nThe Comparable and Comparator interfaces epitomize the power of abstraction in computer science. By concentrating on the core concept of order and essential operations involving order, we can create general, flexible, and reusable code to work with a broad spectrum of data types and orderings.\nIn the next chapter, we will delve into how these concepts are employed in data structures like trees and heaps to manage data for efficient searching and sorting. For now, contemplate the elegance and versatility of the Comparable and Comparator interfaces, and the compelling concept of order they embody.",
    "crumbs": [
      "Iterative Sorting and Searching",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Comparable and Comparator</span>"
    ]
  },
  {
    "objectID": "10_bubble_selection_insertion_sort.html",
    "href": "10_bubble_selection_insertion_sort.html",
    "title": "14  Bubble, Selection, and Insertion Sort",
    "section": "",
    "text": "14.1 Bubble Sort\nBubble sort is a simple sorting algorithm that works by repeatedly comparing and swapping adjacent elements in an array until the array is sorted. Bubble sort is easy to implement and understand, but it is not very efficient for large arrays. The name bubble sort comes from the idea that the larger elements “bubble up” to the end of the array after each iteration.",
    "crumbs": [
      "Iterative Sorting and Searching",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Bubble, Selection, and Insertion Sort</span>"
    ]
  },
  {
    "objectID": "10_bubble_selection_insertion_sort.html#bubble-sort",
    "href": "10_bubble_selection_insertion_sort.html#bubble-sort",
    "title": "14  Bubble, Selection, and Insertion Sort",
    "section": "",
    "text": "14.1.1 Pseudocode\nThe pseudocode for bubble sort is as follows:\nprocedure bubbleSort(A : list of sortable items)\n    n = length(A)\n    repeat\n        swapped = false\n        for i = 1 to n-1 inclusive do\n            if A[i-1] &gt; A[i] then\n                swap(A[i-1], A[i])\n                swapped = true\n            end if\n        end for\n        n = n - 1\n    until not swapped\nend procedure\nThe procedure takes an array A of sortable items and sorts it in ascending order. The procedure uses a variable n to keep track of the unsorted part of the array, and a variable swapped to indicate whether any swaps occurred in the current iteration. The procedure repeats until no swaps are made, which means the array is sorted.\n\n\n14.1.2 An Example\n\nLet us see an example of bubble sort on the following array:\n[5, 1, 4, 2, 8]\nWe start with n = 5 and swapped = false. We compare the first and second elements, 5 and 1, and swap them since 5 &gt; 1. We set swapped to true to indicate that a swap occurred. We compare the second and third elements, 5 and 4, and swap them as well. We continue to compare and swap the adjacent elements until we reach the end of the array. The array after the first iteration is:\n[1, 4, 2, 5, 8]\nWe decrement n by 1, since the last element 8 is now in its correct position. We repeat the process with n = 4 and swapped = false. We compare and swap the first and second elements, 1 and 4, since 1 &lt; 4. We compare and swap the second and third elements, 4 and 2, since 4 &gt; 2. We compare the third and fourth elements, 5 and 8, but do not swap them since 5 &lt; 8. The array after the second iteration is:\n[1, 2, 4, 5, 8]\nWe decrement n by 1, since the last element 5 is now in its correct position. We repeat the process with n = 3 and swapped = false. We compare and swap the first and second elements, 1 and 2, since 1 &lt; 2. We compare and swap the second and third elements, 2 and 4, since 2 &lt; 4. The array after the third iteration is:\n[1, 2, 4, 5, 8]\nWe decrement n by 1, since the last element 4 is now in its correct position. We repeat the process with n = 2 and swapped = false. We compare and swap the first and second elements, 1 and 2, since 1 &lt; 2. The array after the fourth iteration is:\n[1, 2, 4, 5, 8]\nWe decrement n by 1, since the last element 2 is now in its correct position. We repeat the process with n = 1 and swapped = false. We do not compare any elements, since there is only one element left. The array after the fifth iteration is:\n[1, 2, 4, 5, 8]\nWe exit the loop, since swapped is false, which means the array is sorted.\n\n\n\n14.1.3 Lab Instructions\n\nThe template uses Java generics to create a generic class Main that can sort arrays of any type that implements the Comparable interface. Generics are a way of implementing generic programming in Java, which allows you to write code that can work with different types of objects without casting or risking ClassCastException.\nThe constructor of the Main class takes an array of type T as a parameter and assigns it to the data field. The data field is also of type T, which means it can store any type of object that implements Comparable.\nThe BubbleSort method returns an array of type T that is sorted in ascending order using the bubble sort algorithm. The method uses a local variable sorted to store a copy of the data array and then modifies it using the pseudocode provided in the assignment instructions.\nTo compare and swap the elements of the sorted array, you need to use the compareTo method of the Comparable interface. The compareTo method returns a negative integer, zero, or a positive integer if the current object is less than, equal to, or greater than the specified object. For example, if you want to compare the elements at index i-1 and i, you can write:\nif (sorted[i-1].compareTo(sorted[i]) &gt; 0) {\n    // swap the elements\n}\nTo swap the elements of the sorted array, you can use a temporary variable of type T to store one of the elements, and then assign the other element to its place. For example, if you want to swap the elements at index i-1 and i, you can write:\nT temp = sorted[i-1];\nsorted[i-1] = sorted[i];\nsorted[i] = temp;\nAfter performing the bubble sort algorithm, the method returns the sorted array.\nTo test your code, you can create an object of the Main class with different types of arrays, such as Integer, String, or Double, and call the BubbleSort method on them. You can print the original and sorted arrays to check the output. For example, you can write:\nInteger[] intArray = {5, 1, 4, 2, 8};\nMain&lt;Integer&gt; intMain = new Main&lt;&gt;(intArray);\nInteger[] intSorted = intMain.BubbleSort();\nSystem.out.println(\"Original array: \" + Arrays.toString(intArray));\nSystem.out.println(\"Sorted array: \" + Arrays.toString(intSorted));\nThe output should be:\nOriginal array: [5, 1, 4, 2, 8]\nSorted array: [1, 2, 4, 5, 8]",
    "crumbs": [
      "Iterative Sorting and Searching",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Bubble, Selection, and Insertion Sort</span>"
    ]
  },
  {
    "objectID": "10_bubble_selection_insertion_sort.html#selection-sort",
    "href": "10_bubble_selection_insertion_sort.html#selection-sort",
    "title": "14  Bubble, Selection, and Insertion Sort",
    "section": "14.2 Selection Sort",
    "text": "14.2 Selection Sort\n\n14.2.1 Introduction\nSelection sort is a sorting algorithm that repeatedly finds the smallest element in the unsorted part of the array and swaps it with the first element of the unsorted part. This way, the array is divided into two subarrays: one that contains the sorted elements and one that contains the unsorted elements. The process is repeated until the entire array is sorted. Selection sort is a simple and efficient algorithm that works well for small arrays, but it has a high time complexity of O(n^2) for large arrays.\n\n\n14.2.2 Pseudocode\nThe pseudocode for selection sort is as follows:\nProcedure selection_sort(array, N)\n  array - array of items to be sorted\n  N - size of array\nBegin\n  For i = 1 to N-1\n    Begin\n      Set min = i\n      For j = i+1 to N\n        Begin\n          If array[j] &lt; array[min] Then\n            min = j\n          End If\n        End For\n      // Swap the minimum element with the current element\n      If min != i Then\n        Swap array[min] and array[i]\n      End If\n    End For\nEnd Procedure\n\n\n14.2.3 An Example\n\nLet us see an example of how selection sort works on the following array:\n[64, 25, 12, 22, 11]\nIn the first iteration, the algorithm finds the smallest element (11) in the unsorted part of the array and swaps it with the first element of the unsorted part (64). The array becomes:\n[11, 25, 12, 22, 64]\nIn the second iteration, the algorithm finds the smallest element (12) in the remaining unsorted part of the array and swaps it with the second element of the unsorted part (25). The array becomes:\n[11, 12, 25, 22, 64]\nIn the third iteration, the algorithm finds the smallest element (22) in the remaining unsorted part of the array and swaps it with the third element of the unsorted part (25). The array becomes:\n[11, 12, 22, 25, 64]\nIn the fourth iteration, the algorithm finds the smallest element (25) in the remaining unsorted part of the array and swaps it with the fourth element of the unsorted part (25). The array becomes:\n[11, 12, 22, 25, 64]\nIn the fifth iteration, the algorithm finds the smallest element (64) in the remaining unsorted part of the array and swaps it with the fifth element of the unsorted part (64). The array becomes:\n[11, 12, 22, 25, 64]\nThe array is now sorted and the algorithm terminates.\n\n\n\n14.2.4 Instructions\n\nThe template is a generic class that can sort any type of data that implements the Comparable interface. This means that the data type T must have a method compareTo(T other) that returns a negative, zero, or positive integer depending on whether the current object is less than, equal to, or greater than the other object.\nThe constructor of the class takes an array of type T as a parameter and assigns it to the data field. The data field is the array that needs to be sorted.\nThe SelectionSort() method is where the students need\n\nto write the code for the selection sort algorithm. The method should return a sorted array of type T.\n\nThe students can use the pseudocode provided in the previous section as a guide for writing the code. They need to use a for loop to iterate over the unsorted part of the array, find the minimum element, and swap it with the first element of the unsorted part. They can use the compareTo() method to compare the elements of the array. They can use a temporary variable to store the value of the element to be swapped.\nThe students can test their code by creating an object of the Main class with different types of data, such as integers, strings, or custom objects, and calling the SelectionSort() method on it. They can print the original and sorted arrays to check the output. They can also use different sizes of arrays to see how the algorithm performs.",
    "crumbs": [
      "Iterative Sorting and Searching",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Bubble, Selection, and Insertion Sort</span>"
    ]
  },
  {
    "objectID": "10_bubble_selection_insertion_sort.html#insertion-sort",
    "href": "10_bubble_selection_insertion_sort.html#insertion-sort",
    "title": "14  Bubble, Selection, and Insertion Sort",
    "section": "14.3 Insertion Sort",
    "text": "14.3 Insertion Sort\n\n14.3.1 Introduction\nInsertion sort is a simple and adaptive sorting technique that works by inserting each element into its correct position in a sorted subarray. The sorted subarray starts with the first element and grows by one element in each iteration. The element to be inserted is compared with the previous elements in the sorted subarray and swapped if they are larger. This process continues until the element finds its correct position or reaches the beginning of the array.\n\n\n14.3.2 Pseudocode\nThe pseudocode for insertion sort is as follows:\nprocedure insertionSort (A: list of sortable items)\n    n = length (A)\n    for i = 1 to n - 1 do\n        j = i\n        while j &gt; 0 and A [j-1] &gt; A [j] do\n            swap (A [j], A [j-1])\n            j = j - 1\n        end while\n    end for\nend procedure\n\n\n14.3.3 An Example\nConsider the array: [3, 5, 7, 11, 13, 2, 9, 6]\nBelow is a detailed walkthrough of each iteration:\n\nIteration 1: [3, 5, 7, 11, 13, 2, 9, 6]\nThe sorted subarray is [3] and the element to be inserted is 5. Since 5 is larger than 3, no swap is needed.\nIteration 2: [3, 5, 7, 11, 13, 2, 9, 6]\nThe sorted subarray is [3, 5] and the element to be inserted is 7. Since 7 is larger than 5, no swap is needed.\nIteration 3: [3, 5, 7, 11, 13, 2, 9, 6]\nThe sorted subarray is [3, 5, 7] and the element to be inserted is 11. Since 11 is larger than 7, no swap is needed.\nIteration 4: [3, 5, 7, 11, 13, 2, 9, 6]\nThe sorted subarray is [3, 5, 7, 11] and the element to be inserted is 13. Since 13 is larger than 11, no swap is needed.\nIteration 5: [3, 5, 7, 11, 13, 2, 9, 6]\nThe sorted subarray is [3, 5, 7, 11, 13] and the element to be inserted is 2. Since 2 is smaller than 13, 13 and 2 are swapped.\nAfter swapping: [3, 5, 7, 11, 2, 13, 9, 6]\nIteration 6: [3, 5, 7, 11, 2, 13, 9, 6]\nThe sorted subarray is [3, 5, 7, 11, 2, 13] and the element to be inserted is 2. Since 2 is smaller than 11, 11 and 2 are swapped.\nAfter swapping: [3, 5, 7, 2, 11, 13, 9, 6]\nIteration 7: `[3, 5, 7,\n\n2, 11, 13, 9, 6]`\nThe sorted subarray is [3, 5, 7, 2, 11, 13] and the element to be inserted is 2. Since 2 is smaller than 7, 7 and 2 are swapped.\nAfter swapping: [3, 5, 2, 7, 11, 13, 9, 6]\n\nIteration 8: [3, 5, 2, 7, 11, 13, 9, 6]\nThe sorted subarray is [3, 5, 2, 7, 11, 13] and the element to be inserted is 2. Since 2 is smaller than 5, 5 and 2 are swapped.\nAfter swapping: [3, 2, 5, 7, 11, 13, 9, 6]\nIteration 9: [3, 2, 5, 7, 11, 13, 9, 6]\nThe sorted subarray is [3, 2, 5, 7, 11, 13] and the element to be inserted is 2. Since 2 is smaller than 3, 3 and 2 are swapped.\nAfter swapping: [2, 3, 5, 7, 11, 13, 9, 6]\nIteration 10: [2, 3, 5, 7, 11, 13, 9, 6]\nThe sorted subarray is [2, 3, 5, 7, 11, 13] and the element to be inserted is 9. Since 9 is smaller than 13, 13 and 9 are swapped.\nAfter swapping: [2, 3, 5, 7, 11, 9, 13, 6]\nIteration 11: [2, 3, 5, 7, 11, 9, 13, 6]\nThe sorted subarray is [2, 3, 5, 7, 11, 9, 13] and the element to be inserted is 9. Since 9 is smaller than 11, 11 and 9 are swapped.\nAfter swapping: [2, 3, 5, 7, 9, 11, 13, 6]\nIteration 12: [2, 3, 5, 7, 9, 11, 13, 6]\nThe sorted subarray is [2, 3, 5, 7, 9, 11, 13] and the element to be inserted is 9. Since 9 is larger than 7, no swap is needed.\nIteration 13: [2, 3, 5, 7, 9, 11, 13, 6]\nThe sorted subarray is [2, 3, 5, 7, 9, 11, 13] and the element to be inserted is 6. Since 6 is smaller than 13, 13 and 6 are swapped.\nAfter swapping: [2, 3, 5, 7, 9, 11, 6, 13]\n**Iteration\n\n14**: [2, 3, 5, 7, 9, 11, 6, 13]\nThe sorted subarray is [2, 3, 5, 7, 9, 11, 6, 13] and the element to be inserted is 6. Since 6 is smaller than 11, 11 and 6 are swapped.\n*After swapping*: `[2, 3, 5, 7, 9, 6, 11, 13]` \n\nIteration 15: [2, 3, 5, 7, 9, 6, 11, 13]\nThe sorted subarray is [2, 3, 5, 7, 9, 6, 11, 13] and the element to be inserted is 6. Since 6 is smaller than 9, 9 and 6 are swapped.\nAfter swapping: [2, 3, 5, 7, 6, 9, 11, 13]\nIteration 16: [2, 3, 5, 7, 6, 9, 11, 13]\nThe sorted subarray is [2, 3, 5, 7, 6, 9, 11, 13] and the element to be inserted is 6. Since 6 is smaller than 7, 7 and 6 are swapped.\nAfter swapping: [2, 3, 5, 6, 7, 9, 11, 13]\nIteration 17: [2, 3, 5, 6, 7, 9, 11, 13]\nThe sorted subarray is [2, 3, 5, 6, 7, 9, 11, 13] and the element to be inserted is 6. Since 6 is larger than 5, no swap is needed.\n\nFinal array: [2, 3, 5, 6, 7, 9, 11, 13]\nThe sorted subarray is [2, 3, 5, 6, 7, 9, 11, 13] and the array is fully sorted.\n\n\n14.3.4 Instructions\n\nStart by understanding how insertion sort works and what it does to sort an array.\nStudy the given template for the Main class and understand how it uses Java generics to sort an array of any type that implements the Comparable interface.\nImplement the InsertionSort method by following the pseudocode provided above. Students should create a copy of the input array and modify the copy in-place to produce the sorted array.\nTest their implementation using a variety of input arrays and verify that the output is correct.\nEncourage students to optimize their implementation by considering edge cases and identifying areas for improvement.\nRemind students to comment their code and explain their thought process as they work through the assignment. This will help them develop their coding skills and become better problem solvers.",
    "crumbs": [
      "Iterative Sorting and Searching",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Bubble, Selection, and Insertion Sort</span>"
    ]
  },
  {
    "objectID": "11_linear_search.html",
    "href": "11_linear_search.html",
    "title": "15  Linear Search",
    "section": "",
    "text": "15.1 Pseudocode\nLinear search is one of the simplest searching algorithms. The linear search algorithm scans one item at a time, without jumping to any item. It starts at the beginning of a list (or array) and sequentially checks each element until it finds a match.\nHere’s a general breakdown of how the linear search algorithm works:\nLinear search can be applied to any type of data. However, since it searches element-by-element, it can be inefficient for large lists or arrays.\nThe pseudocode for linear search is straightforward:\nThis pseudocode represents a function called LinearSearch that takes two parameters: a list (or array) A and a key to search for in A. The function returns the index of key if found, otherwise it returns -1.",
    "crumbs": [
      "Iterative Sorting and Searching",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Linear Search</span>"
    ]
  },
  {
    "objectID": "11_linear_search.html#pseudocode",
    "href": "11_linear_search.html#pseudocode",
    "title": "15  Linear Search",
    "section": "",
    "text": "procedure LinearSearch(A, key)\n    for i from 1 to length(A)\n        if A[i] equals key\n            return i\n    return -1",
    "crumbs": [
      "Iterative Sorting and Searching",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Linear Search</span>"
    ]
  },
  {
    "objectID": "11_linear_search.html#an-example",
    "href": "11_linear_search.html#an-example",
    "title": "15  Linear Search",
    "section": "15.2 An Example",
    "text": "15.2 An Example\nLet’s consider the following example: We have an array [10, 15, 20, 25, 30, 35] and we want to find the number 20.\n\nIteration 1: The first element is 10. 10 is not equal to 20, so we move on to the next element.\nIteration 2: The second element is 15. 15 is not equal to 20, so we move on to the next element.\nIteration 3: The third element is 20. 20 is equal to 20, so we stop searching. The index of the number 20 is 2 (considering the first index as 0).\n\nTherefore, the linear search algorithm would return 2 as the index of the number 20 in the given array.",
    "crumbs": [
      "Iterative Sorting and Searching",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Linear Search</span>"
    ]
  },
  {
    "objectID": "11_linear_search.html#instructions",
    "href": "11_linear_search.html#instructions",
    "title": "15  Linear Search",
    "section": "15.3 Instructions",
    "text": "15.3 Instructions\nHere are some instructions to help you start working on the assignment:\n\nThe template uses Java generics to create a generic class Main that can search for a key element in an array of any type that implements the Comparable interface. Generics are a way of implementing generic programming in Java, which allows you to write code that can work with different types of objects without casting or risking ClassCastException.\nThe constructor of the Main class takes an array of type T as a parameter and assigns it to the array field. The array field is also of type T, which means it can store any type of object that implements Comparable.\nThe LinearSearch method returns an int that is the index of the key element in the array, or -1 if the key element is not found. The method uses a for loop to iterate over the array and compare each element with the key using the compareTo method of the Comparable interface. The compareTo method returns a negative integer, zero, or a positive integer if the current object is less than, equal to, or greater than the specified object. For example, if you want to compare the element at index i with the key, you can write:\nif (array[i].compareTo(key) == 0) {\n    // return the index\n}\nThe LinearSearch method has a time complexity of O(n), where\n\nn is the size of the array, because it may have to scan the entire array to find the key element. 4. To test your code, you can create an object of the Main class with different types of arrays, such as Integer, String, or Double, and call the LinearSearch method on them. You can print the key element and the index returned by the methods to check the output. For example, you can write:\n```java\nInteger[] intArray = {1, 2, 4, 5, 8};\nMain&lt;Integer&gt; intMain = new Main&lt;&gt;(intArray);\nInteger key = 4;\nint linearIndex = intMain.LinearSearch(key);\nSystem.out.println(\"Key element: \" + key);\nSystem.out.println(\"Linear search index: \" + linearIndex);\n```\n\nThe output of the above code should be:\n\n```\nKey element: 4\nLinear search index: 2\n```",
    "crumbs": [
      "Iterative Sorting and Searching",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Linear Search</span>"
    ]
  },
  {
    "objectID": "12_collections_of_data_intro.html",
    "href": "12_collections_of_data_intro.html",
    "title": "16  Collections of Data",
    "section": "",
    "text": "16.1 (Active) Memory in Computers\nTo fully grasp the concept of data collections in Java, we must first lay a foundational understanding of memory in computing systems. It is helpful to visualize the computer’s memory as a vast grid, with each cell in the grid being capable of storing a certain number of bits. This simple grid becomes the underpinning infrastructure for data storage, holding variables that serve a multitude of functions in our programming ventures.\nG\n\n\n\na\n\n1\n\n0\n\n1\n\n1\n\n0\n\n1\n\n0\n\n1\n\n0\n\n1\n\n0\n\n1\n\n0\n\n1\n\n0\n\n1\n\n1\n\n0\n\n1\n\n0\n\n1\n\n0\n\n1\n\n0\n\n1\n\n0\n\n1\n\n1\n\n0\n\n1\n\n0\n\n1\n\n0\n\n1\n\n0\n\n1\n\n0\n\n1\n\n0\n\n1\n\n1\n\n0\n\n1\n\n0\n\n1\n\n0\n\n1\n\n0\n\n\n\n\n\n\nFigure 16.1: A diagram showing a grid of memory cells, each capable of storing a single bit.",
    "crumbs": [
      "Sequential Collections of Data",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Collections of Data</span>"
    ]
  },
  {
    "objectID": "12_collections_of_data_intro.html#the-concept-of-data-collections",
    "href": "12_collections_of_data_intro.html#the-concept-of-data-collections",
    "title": "16  Collections of Data",
    "section": "16.2 The Concept of Data Collections",
    "text": "16.2 The Concept of Data Collections\nIn programming, the need often arises to handle groups of similar variables. These groups are what we refer to as data collections. A data collection is essentially an assembly of bits in the memory, representing a group of variables. For instance, you could have a collection of four 2-bit variables, which together would take up eight bits of memory. But the behavior of these collections can vary, particularly when considering size constraints.",
    "crumbs": [
      "Sequential Collections of Data",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Collections of Data</span>"
    ]
  },
  {
    "objectID": "12_collections_of_data_intro.html#types-of-collections-fixed-and-dynamic",
    "href": "12_collections_of_data_intro.html#types-of-collections-fixed-and-dynamic",
    "title": "16  Collections of Data",
    "section": "16.3 Types of Collections: Fixed and Dynamic",
    "text": "16.3 Types of Collections: Fixed and Dynamic\nWhen analyzing the size of data collections, we encounter two main types: fixed (or static) and dynamic collections.\nFixed or Static Collections: As the name suggests, in a static collection, the number of variables is constant. This implies that the collection’s size is immutable, and the total memory occupancy remains consistent. An array of 10 integers is a perfect example of a static collection - regardless of the values stored in it, the array always contains ten integers.\n\n\n\n\n\n\n\n\nG\n\n\n\na\n\n\nData\n\n\nData\n\n\nData\n\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n\n\nFigure 16.2: A diagram showing a static collection of data where the five 4-bit variables are placed sequentially in memory.\n\n\n\n\n\nDynamic Collections: Dynamic collections differ from their static counterparts in that their sizes are subject to change over time. Consider, for example, a collection storing the names of your favorite musicians. As your taste in music evolves, so does this list, reflecting an expansion or contraction based on your preferences. Thus, it is a dynamic collection.",
    "crumbs": [
      "Sequential Collections of Data",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Collections of Data</span>"
    ]
  },
  {
    "objectID": "12_collections_of_data_intro.html#memory-layout-of-collections",
    "href": "12_collections_of_data_intro.html#memory-layout-of-collections",
    "title": "16  Collections of Data",
    "section": "16.4 Memory Layout of Collections",
    "text": "16.4 Memory Layout of Collections\nLet’s take a deeper look into how these collections are stored in memory. Note that the memory allocation of these collections can be influenced by factors such as the variables’ data type and size.\nA common approach to store a collection of two 4-bit numbers is sequentially, positioning them back-to-back. However, this is not the only strategy. Depending on the system’s memory management and data alignment methodologies, ‘padding’ could be introduced, which involves inserting extra bits between variables to align them properly in memory.\nRegardless of these variations, a crucial consideration when working with collections is devising a mechanism to locate the next variable in the sequence. This task is straightforward for static collections due to the known size of each variable. But how about dynamic collections?",
    "crumbs": [
      "Sequential Collections of Data",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Collections of Data</span>"
    ]
  },
  {
    "objectID": "12_collections_of_data_intro.html#tackling-dynamic-collections",
    "href": "12_collections_of_data_intro.html#tackling-dynamic-collections",
    "title": "16  Collections of Data",
    "section": "16.5 Tackling Dynamic Collections",
    "text": "16.5 Tackling Dynamic Collections\nDynamic collections present an interesting challenge due to their mutable size. One way to navigate this issue is by earmarking a portion of the memory to store the length of the collection. For instance, the first four bits could be utilized to denote the size of the collection.\nBy adopting this method, we can efficiently locate subsequent data elements in our collection and discern the collection’s end point. Bear in mind that the data in collections doesn’t have to be stored adjacently. We can opt to pad each element with a fixed number of bits. As long as the padding size remains consistent, we can still locate the next piece of data.\n\n\n\n\n\n\n\n\nG\n\n\n\na\n\n\nLength\n\n\nData\n\n\nPad\n\n\nData\n\n\nPad\n\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n\n\n\n\n\nFigure 16.3: A diagram showing a dynamic collection of data where the first 4 bits represent the length of the collection, and the next few bits are the collection of variables. Each variable will be separated by 2-bit padding.",
    "crumbs": [
      "Sequential Collections of Data",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Collections of Data</span>"
    ]
  },
  {
    "objectID": "12_collections_of_data_intro.html#memory-layout-and-program-performance",
    "href": "12_collections_of_data_intro.html#memory-layout-and-program-performance",
    "title": "16  Collections of Data",
    "section": "16.6 Memory Layout and Program Performance",
    "text": "16.6 Memory Layout and Program Performance\nThe manner in which data collections are stored in memory can have a significant impact on the performance of a program. For instance, accessing memory sequentially (in a pattern that matches the memory layout of the collection) allows the system to leverage cache lines, which are blocks of memory that are read into the CPU’s cache. Due to the way modern CPUs are designed, once a part of the memory is read, an entire cache line (usually 64 or 128 bytes) is loaded into the cache. Thus, sequential access often results in fewer cache misses, which improves the performance of your program.\nIn contrast, random access to memory can lead to frequent cache misses, as each access may require loading a different cache line into the CPU cache, slowing down your program. Therefore, understanding the memory layout of your data collections and designing your program to access data in a manner that complements this layout can lead to significant performance improvements.\nSo, as you delve deeper into the world of data collections, remember that your approach to handling memory can either elevate or hinder your program’s efficiency. The choice, as always, is in your hands!",
    "crumbs": [
      "Sequential Collections of Data",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Collections of Data</span>"
    ]
  },
  {
    "objectID": "12_collections_of_data_intro.html#summary-and-conclusion",
    "href": "12_collections_of_data_intro.html#summary-and-conclusion",
    "title": "16  Collections of Data",
    "section": "16.7 Summary and Conclusion",
    "text": "16.7 Summary and Conclusion\nThis chapter provided a comprehensive overview of data collections in programming, focusing mainly on their storage in computer memory. It began by explaining the fundamental understanding of memory in computing systems, portraying it as a vast grid where each cell stores a certain number of bits. This concept helped set the groundwork for understanding how data collections, which are groups of similar variables, are stored in memory.\nThe chapter then discussed two main types of data collections: fixed (static) and dynamic collections. While fixed collections have a constant number of variables and thus a constant memory size, dynamic collections have a variable size that can change over time. An illustration was given of both types to aid understanding.\nThe memory layout of these collections was then delved into, noting that how these collections are stored in memory can be influenced by the variables’ data type and size. The chapter also explained the strategies for accessing variables in both static and dynamic collections.\nMoreover, the chapter emphasized the significant impact of the memory layout of data collections on program performance. Sequential memory access in line with the memory layout could leverage cache lines and enhance the program’s performance, while random access could lead to frequent cache misses and slow down the program.\nIn conclusion, understanding data collections, how they are stored in memory, and how to access this data efficiently are critical aspects of programming. By considering these factors, programmers can significantly influence the performance of their programs. Thus, the mastery of data collections is not only essential for writing efficient code but is also a vital skill for optimizing the overall performance of a program.",
    "crumbs": [
      "Sequential Collections of Data",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Collections of Data</span>"
    ]
  },
  {
    "objectID": "13_operations_on_lists.html",
    "href": "13_operations_on_lists.html",
    "title": "17  Operations on Lists",
    "section": "",
    "text": "17.1 Adding Data to a List\nIn the prior lecture, we journeyed through the concept of collections of data and their manifestations in computer memory. Such collections were referred to as lists, a data structure central to understanding the organization and manipulation of data in programming. Our objective is to delve into the practical implementation of these lists in Java. However, before we embark on that, we must consider the types of operations we desire our lists to perform. These operations guide our implementation strategy and ensure that our list behaves in the ways we want it to. In this chapter, we’ll be investigating operations performed on a List Abstract Data Type (ADT).\nAs you may recall, a list is a collection of data elements. A key operation, therefore, is the ability to add data to this collection. Let’s visualize a simple list:\nThere are multiple ways to insert new data into this list. For instance, you might want to add a new element, say “X”, at the end:\nAlternatively, you could place “X” at the beginning:\nOr perhaps insert “X” somewhere in the middle:\nThere are three distinct methods to add data to our list, and each method may serve different needs in your program. Let’s discuss how these operations are implemented in the List ADT provided by the Java standard library, java.util.List.",
    "crumbs": [
      "Sequential Collections of Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Operations on Lists</span>"
    ]
  },
  {
    "objectID": "13_operations_on_lists.html#adding-data-to-a-list",
    "href": "13_operations_on_lists.html#adding-data-to-a-list",
    "title": "17  Operations on Lists",
    "section": "",
    "text": "1\n2\n3\n4\n5\n6\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\nX\n\n\n\n\n\n\n\n\n\nX\n1\n2\n3\n4\n5\n6\n\n\n\n\n\n\n\n\n\n1\n2\n3\nX\n4\n5\n6\n\n\n\n\n\n\n\n17.1.1 Appending an Element\nAppending an element to the end of a list is perhaps the most straightforward method of adding data. The add method provided by java.util.List is used for this operation. Here is how you might use it:\nlistObject.add(itemToAdd);\n\n\n17.1.2 Prepending an Element\nInserting an element at the beginning of a list is also a common operation, often called “prepending”. Though the java.util.List interface doesn’t provide a prepend method, we can use the overloaded add method, which accepts an index and an element. To prepend, we simply pass 0 as the index:\nlistObject.add(0, itemToAdd);\n\n\n17.1.3 Inserting an Element at a Specific Index\nAs hinted in the prepend operation, the add method in java.util.List allows for adding an element at any index in the list:\nlistObject.add(index, itemToAdd);\n\n\n17.1.4 Adding All Elements from Another List\nSometimes, you may want to merge one list into another. The addAll method allows you to add all the elements from another list to the end of the current list:\nlistObject.addAll(anotherList);",
    "crumbs": [
      "Sequential Collections of Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Operations on Lists</span>"
    ]
  },
  {
    "objectID": "13_operations_on_lists.html#the-role-of-abstract-data-types-adts",
    "href": "13_operations_on_lists.html#the-role-of-abstract-data-types-adts",
    "title": "17  Operations on Lists",
    "section": "17.2 The Role of Abstract Data Types (ADTs)",
    "text": "17.2 The Role of Abstract Data Types (ADTs)\nWe’ve mentioned the term “Abstract Data Type” (ADT) several times so far, but what does it really mean? An ADT is a high-level description of a collection of data and the operations that can be performed on that data. It is “abstract” in that it describes what operations are to be done but not how these operations will be implemented.\nIn the context of lists,\nthe List ADT defines a list’s behavior. For instance, we can append, prepend, or insert an element at a specific index, but we don’t care about how these operations are performed.\nWhy do we need ADTs? ADTs allow us to abstract away the details of the data structure’s implementation. They encapsulate the data and provide a well-defined interface to interact with it, ensuring that data remains consistent and operations on the data are predictable. Moreover, by using ADTs, different implementations of the same type of data structure can be swapped seamlessly, without changing the code that uses the data structure.\nIn your journey with Java so far, you’ve only used one kind of list – the ArrayList. As you progress, you’ll encounter other types of lists, like LinkedLists and Stacks, each with its own strengths, weaknesses, and suitable use cases. All these list types adhere to the same List ADT, allowing us to switch from one type to another depending on our requirements, while our code remains largely the same.\nIn the next sections of this course, we will dive deeper into different types of lists and their unique characteristics. For now, the important takeaway is that ADTs allow us to focus on what operations we want to perform without worrying about how they are implemented.\n\n17.2.1 Summary of Addition Operations\nHere is a summary of the adding operations available in the java.util.List ADT:\n\n\n\nModifier and Type\nMethod and Description\n\n\n\n\nboolean\nadd(E e) Appends the specified element to the end of this list (optional operation).\n\n\nvoid\nadd(int index, E element) Inserts the specified element at the specified position in this list (optional operation).\n\n\nboolean\naddAll(Collection&lt;? extends E&gt; c) Appends all of the elements in the specified collection to the end of this list, in the order that they are returned by the specified collection’s iterator (optional operation).\n\n\nboolean\naddAll(int index, Collection&lt;? extends E&gt; c) Inserts all of the elements in the specified collection into this list at the specified position (optional operation).",
    "crumbs": [
      "Sequential Collections of Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Operations on Lists</span>"
    ]
  },
  {
    "objectID": "13_operations_on_lists.html#removing-data-from-our-list",
    "href": "13_operations_on_lists.html#removing-data-from-our-list",
    "title": "17  Operations on Lists",
    "section": "17.3 Removing data from our List",
    "text": "17.3 Removing data from our List",
    "crumbs": [
      "Sequential Collections of Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Operations on Lists</span>"
    ]
  },
  {
    "objectID": "13_operations_on_lists.html#removing-data-from-a-list",
    "href": "13_operations_on_lists.html#removing-data-from-a-list",
    "title": "17  Operations on Lists",
    "section": "17.4 Removing Data from a List",
    "text": "17.4 Removing Data from a List\nHaving explored the addition of data to a list, let’s now shift our attention to its counterpart: removing data from the list. Remember our list from the previous section?\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n\nLet’s consider different ways to remove data from this list.\n\n17.4.1 Removing an Element at a Specific Index\nSometimes, you might need to remove an item from a specific position within the list. Java’s List ADT provides the remove(int index) method for this purpose. This method removes the element at the specified position:\nlistObject.remove(index);\n\n\n17.4.2 Removing the First Occurrence of an Element\nAt times, you might not know (or care about) the index of the element you want to remove, but you know the value of the element. In such cases, you can use the remove(Object o) method, which removes the first occurrence of the specified element from the list:\nlistObject.remove(objectToRemove);\n\n\n17.4.3 Removing All Elements from Another List\nConsider that you have two lists, and you want to remove all elements in the second list from the first one. You can use the removeAll(Collection&lt;?&gt; c) method, which removes from the current list all of its elements that are contained in the specified collection:\nlistObject.removeAll(anotherList);\n\n\n17.4.4 Removing All Elements from the List\nIn some scenarios, you might want to clear your list entirely. The clear() method comes in handy for this, as it removes all elements from the list:\nlistObject.clear();\nLike the addition operations, these removal operations offer different ways to manage the data in our list, providing flexibility based on the specific needs of our program.\n\n\n17.4.5 Summary of Removal Operations\nHere is a summary of the removal operations available in the java.util.List ADT:\n\n\n\nModifier and Type\nMethod and Description\n\n\n\n\nE\nremove(int index) Removes the element at the specified position in this list (optional operation).\n\n\nboolean\nremove(Object o) Removes the first occurrence of the specified element from this list, if it is present (optional operation).\n\n\nboolean\nremoveAll(Collection&lt;?&gt; c) Removes from this list all of its elements that are contained in the specified collection (optional operation).\n\n\nvoid\nclear() Removes all of the elements from this list (optional operation).",
    "crumbs": [
      "Sequential Collections of Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Operations on Lists</span>"
    ]
  },
  {
    "objectID": "13_operations_on_lists.html#searching-for-data-in-our-list",
    "href": "13_operations_on_lists.html#searching-for-data-in-our-list",
    "title": "17  Operations on Lists",
    "section": "17.5 Searching for data in our List",
    "text": "17.5 Searching for data in our List",
    "crumbs": [
      "Sequential Collections of Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Operations on Lists</span>"
    ]
  },
  {
    "objectID": "13_operations_on_lists.html#searching-in-a-list",
    "href": "13_operations_on_lists.html#searching-in-a-list",
    "title": "17  Operations on Lists",
    "section": "17.6 Searching in a List",
    "text": "17.6 Searching in a List\nBeyond just adding and removing data from our list, we often need to find or check for the existence of certain elements in our list. Java’s List ADT provides several methods for such search operations. Let’s dive into these operations using our familiar list:\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n\n\n17.6.1 Checking If an Element Exists in the List\nSometimes, all we need to know is whether a certain element exists in our list. The contains(Object o) method serves this purpose, returning true if this list contains the specified element, and false otherwise:\nboolean contains = listObject.contains(objectToCheck);\n\n\n17.6.2 Checking If All Elements of Another Collection Exist in the List\nIf you have a collection of elements and you want to check whether all these elements exist in your list, you can use the containsAll(Collection&lt;?&gt; c) method. It returns true if the list contains all of the elements of the specified collection:\nboolean containsAll = listObject.containsAll(anotherCollection);\n\n\n17.6.3 Finding the Index of an Element\nIf you want to find the position of a certain element in your list, Java provides the indexOf(Object o) method. This method returns the index of the first occurrence of the specified element in this list, or -1 if this list does not contain the element:\nint index = listObject.indexOf(objectToFind);\n\n\n17.6.4 Finding the Last Index of an Element\nIn a list with duplicate elements, you might be interested in finding the last occurrence of an element. In this case, you can use the lastIndexOf(Object o) method, which returns the index of the last occurrence of the specified element in this list, or -1 if this list does not contain the element:\nint lastIndex = listObject.lastIndexOf(objectToFind);\n\n\n17.6.5 Summary of Search Operations\nHere is a summary of the search operations available in the java.util.List ADT:\n\n\n\nModifier and Type\nMethod and Description\n\n\n\n\nboolean\ncontains(Object o) Returns true if this list contains the specified element.\n\n\nboolean\ncontainsAll(Collection&lt;?&gt; c) Returns true if this list contains all of the elements of the specified collection.\n\n\nint\nindexOf(Object o) Returns the index of the first occurrence of the specified element in this list, or -1 if this list does not contain the element.\n\n\nint\nlastIndexOf(Object o) Returns the index of the last occurrence of the specified element in this list, or -1 if this list does not contain the element.\n\n\n\nSearch operations are a fundamental part of manipulating lists. They allow us to locate and verify the presence of data, enhancing our ability to interact with our list and derive meaningful results.",
    "crumbs": [
      "Sequential Collections of Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Operations on Lists</span>"
    ]
  },
  {
    "objectID": "13_operations_on_lists.html#miscellaneous-operations-on-a-list",
    "href": "13_operations_on_lists.html#miscellaneous-operations-on-a-list",
    "title": "17  Operations on Lists",
    "section": "17.7 Miscellaneous Operations on a List",
    "text": "17.7 Miscellaneous Operations on a List\nIn addition to adding, removing, and searching elements in our list, Java’s List ADT provides a variety of other useful operations. These allow us to manipulate and inquire our list in a more sophisticated manner.\n\n17.7.1 Accessing an Element\nSometimes, we need to retrieve the element at a specific position in our list without removing it. The get(int index) method provides this functionality, returning the element at the specified position:\nE element = listObject.get(index);\n\n\n17.7.2 Modifying an Element\nWhat if we need to change an element at a specific position? Java provides the set(int index, E element) method. This method replaces the element at the specified position in this list with the specified element:\nlistObject.set(index, newElement);\n\n\n17.7.3 Determining the Size of the List\nWhen working with lists, it’s often necessary to know the number of elements present. The size() method returns the number of elements in this list:\nint size = listObject.size();\n\n\n17.7.4 Converting the List to an Array\nOn occasion, we may need to convert our list into an array. The toArray() method fulfills this purpose, returning an array containing all the elements in this list in proper sequence:\nObject[] array = listObject.toArray();\n\n\n17.7.5 Summary of Miscellaneous Operations\nHere is a summary of the miscellaneous operations available in the java.util.List ADT:\n\n\n\nModifier and Type\nMethod and Description\n\n\n\n\nE\nget(int index) Returns the element at the specified position in this list.\n\n\nE\nset(int index, E element) Replaces the element at the specified position in this list with the specified element (optional operation).\n\n\nint\nsize() Returns the number of elements in this list.\n\n\nObject[]\ntoArray() Returns an array containing all of the elements in this list in proper sequence (from first to last element).\n\n\n\nThese miscellaneous operations on a list help us to manipulate and utilize our lists effectively. In the subsequent sections, we will delve deeper into different types of lists and how these operations can be applied differently. Stay tuned!",
    "crumbs": [
      "Sequential Collections of Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Operations on Lists</span>"
    ]
  },
  {
    "objectID": "13_operations_on_lists.html#summary-and-conclusion",
    "href": "13_operations_on_lists.html#summary-and-conclusion",
    "title": "17  Operations on Lists",
    "section": "17.8 Summary and Conclusion",
    "text": "17.8 Summary and Conclusion\nThis chapter aimed to offer a comprehensive understanding of operations that can be performed on lists using the Java programming language. It introduced key manipulations including adding, removing, and searching for elements in a list, as well as several other useful list operations.\nWe began by delving into the process of adding elements to a list, where we considered various insertion methods such as appending, prepending, and inserting at a specific index. We also explored the ability to add all elements from another list into the current list. These addition operations provide flexibility and adaptability in managing data in our list as required by the program.\nNext, we moved on to the concept of removing elements from a list. We investigated removing elements at specific indices, removing the first occurrence of an element, and removing all elements from a list or all elements present in another list. These removal operations are just as vital in data management, providing diverse ways to regulate the contents of our list.\nSubsequently, we analyzed search operations within a list. These operations are crucial in data retrieval, determining the existence and position of data elements in our list. Methods like contains(Object o), containsAll(Collection&lt;?&gt; c), indexOf(Object o), and lastIndexOf(Object o) were discussed in depth.\nFurthermore, we examined miscellaneous operations that provide additional functionality in our list. These include accessing and modifying elements at specific indices, determining the size of the list, and converting the list into an array.\nThroughout our exploration, we highlighted the significant role of Abstract Data Types (ADTs). By standardizing the operations that can be performed on a list, the List ADT allows us to focus on what we want to accomplish without concerning ourselves with the underlying implementation details. This abstraction brings forth code consistency, reliability, and the potential for seamless integration of different list implementations.\nIn conclusion, understanding and utilizing list operations in Java is fundamental for effective data management in programming. The breadth of operations available affords us immense flexibility in manipulating lists to align with our program’s requirements. As we proceed in this course, we will delve deeper into the different types of lists and their unique characteristics, all the while, leveraging the power and versatility of the List ADT.",
    "crumbs": [
      "Sequential Collections of Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Operations on Lists</span>"
    ]
  },
  {
    "objectID": "15_implementing_linkedlists.html",
    "href": "15_implementing_linkedlists.html",
    "title": "19  Implementing Linked Lists",
    "section": "",
    "text": "19.1 Understanding References in Java\nIn this chapter, we’ll take a closer look at how objects interact in Java. To fully understand this interaction, we first need to grasp the concept of references in Java, memory allocation for Java objects, and how different objects can interact through references. Later, we’ll also examine how these principles apply to implementing linked lists in Java.\nIn Java programming, the statement that “all objects are references” is a cornerstone. Understanding what this statement means is a fundamental step in mastering Java.\nIn Java, when we say an object is a reference, it means that the object acts as an address pointer, referencing a specific location in memory. This location is where the actual data associated with that object is stored.\nConsider your computer’s memory as a large grid, as illustrated in Table 6.1. Each cell can store some data.\nFor any object in Java, it’s not the data of the object that is directly stored in the variable, but rather the reference to the data.",
    "crumbs": [
      "Sequential Collections of Data",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Implementing Linked Lists</span>"
    ]
  },
  {
    "objectID": "15_implementing_linkedlists.html#understanding-references-in-java",
    "href": "15_implementing_linkedlists.html#understanding-references-in-java",
    "title": "19  Implementing Linked Lists",
    "section": "",
    "text": "Representation of computer’s memory\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0",
    "crumbs": [
      "Sequential Collections of Data",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Implementing Linked Lists</span>"
    ]
  },
  {
    "objectID": "15_implementing_linkedlists.html#memory-allocation-for-java-objects",
    "href": "15_implementing_linkedlists.html#memory-allocation-for-java-objects",
    "title": "19  Implementing Linked Lists",
    "section": "19.2 Memory Allocation for Java Objects",
    "text": "19.2 Memory Allocation for Java Objects\nConsider the following example, in which we have a Dog class.\npublic class Dog {\n    String name;\n    int age;\n}\nInitially, the Dog class doesn’t occupy any memory space. It simply serves as a blueprint for creating Dog objects.\nNow, let’s create a new instance of Dog and assign it to the variable myDog:\nDog myDog = new Dog();\nThis operation performs two main actions:\n\nMemory is allocated for a new Dog object.\nThe address of that memory is stored in the myDog variable.\n\n\n\n\nDog Object Memory Allocation\n\n\nSo, myDog is essentially a pointer that points to the location in memory where the Dog object’s data is stored.",
    "crumbs": [
      "Sequential Collections of Data",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Implementing Linked Lists</span>"
    ]
  },
  {
    "objectID": "15_implementing_linkedlists.html#objects-interacting-through-references",
    "href": "15_implementing_linkedlists.html#objects-interacting-through-references",
    "title": "19  Implementing Linked Lists",
    "section": "19.3 6.3 Objects Interacting Through References",
    "text": "19.3 6.3 Objects Interacting Through References\nNow, let’s consider a scenario where we have two classes, Dog and HomeOwner. The Dog class is the same as before, but the HomeOwner class is a bit more complex.\npublic class HomeOwner\n\n {\n    String name;\n    Dog pet;\n}\nHere, the HomeOwner class has a pet field of type Dog. This means a HomeOwner can own a Dog.\nNow, suppose we have a constructor in the Dog class that accepts name and age as parameters, and a default constructor in the HomeOwner class:\npublic class Dog {\n    String name;\n    int age;\n    \n    public Dog(String name, int age) {\n        this.name = name;\n        this.age = age;\n    }\n}\n\npublic class HomeOwner {\n    String name;\n    Dog pet;\n\n    public HomeOwner() {\n        this.name = \"Timothy\";\n        this.pet = new Dog(\"Timothy Jr.\", 5);\n    }\n}\nWhen we create an instance of HomeOwner:\nHomeOwner owner = new HomeOwner();\n\n\n\nHomeOwner Object Creation\n\n\nA new Dog object is created and assigned to the pet field of the HomeOwner object. Importantly, the pet field stores the reference to the Dog object, not the actual Dog object data.\n\n\n\nDog Object Creation and Assignment\n\n\nThis creates a chain of references in memory, with the owner variable pointing to the HomeOwner object, which in turn has a reference to the Dog object.\n\n\n\nChain of References\n\n\nThis understanding of references and memory allocation sets the foundation for how linked lists work in Java, which we will explore in the upcoming sections.",
    "crumbs": [
      "Sequential Collections of Data",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Implementing Linked Lists</span>"
    ]
  },
  {
    "objectID": "15_implementing_linkedlists.html#implementing-linked-lists-in-java",
    "href": "15_implementing_linkedlists.html#implementing-linked-lists-in-java",
    "title": "19  Implementing Linked Lists",
    "section": "19.4 Implementing Linked Lists in Java",
    "text": "19.4 Implementing Linked Lists in Java\nAbsolutely! Last time, we used arrays to construct lists, but we can indeed build a list using the concept of references we have just discussed.\nWe’ll start by creating a class where we’ll store a reference to the first piece of data in our List. Then, our first piece of data will contain a reference to the next piece, and the next piece will point to the one following it, and so on.\n\n\n\nLinked List\n\n\nThis gives us the following structure:\nclass LinkedList {\n  firstPieceOfData head;\n}\n\nclass firstPieceOfData {\n  nextPieceOfData next;\n}\nHowever, there’s no actual data in this structure yet.\nSo, how about this?\nclass LinkedList {\n  firstPieceOfData head;\n}\n\nclass firstPieceOfData {\n  nextPieceOfData next;\n  actualData data;\n}\n\nclass nextPieceOfData {\n  nextPieceOfData next;\n  actualData data;\n}\nTo ensure our list is flexible and can store any type of data, we should make it generic:\nclass LinkedList&lt;T&gt; {\n  firstPieceOfData&lt;T&gt; head;\n}\n\nclass firstPieceOfData&lt;T&gt; {\n  nextPieceOfData&lt;T&gt; next;\n  T data;\n}\n\nclass nextPieceOfData&lt;T&gt; {\n  nextPieceOfData&lt;T&gt; next;\n  T data;\n}\nHere, &lt;T&gt; is a type parameter that allows us to define a class with placeholders for the types they use, which we can specify when we create an instance of the class.\nWe can also observe that firstPieceOfData and nextPieceOfData are structurally the same, as both classes store a reference to an object that holds data, and both hold some actual data. We can therefore simplify our structure by using a single class for both, like so:\nclass LinkedList&lt;T&gt; {\n  Node&lt;T&gt; head;\n}\n\nclass Node&lt;T&gt; {\n  Node&lt;T&gt; next;\n  T data;\n}\nThe list we’ve created is known as a singly linked list. In some algorithms, we may want to access the previous node. In that case, we can store a reference to the previous node as well, creating what’s known as a doubly linked list:\nclass LinkedList&lt;T&gt; {\n  Node&lt;T&gt; head;\n  Node&lt;T&gt; tail;\n}\n\nclass Node&lt;T&gt; {\n  Node&lt;T&gt; next;\n  Node&lt;T&gt; prev;\n  T data;\n}\nWhen we create a Node object, the constructor should initialize the next reference to null and data to the data we pass in. This is because at that point, we don’t yet know what the next node will be, but we do know what data we want to store.\nclass SinglyLinkedNode&lt;T&gt; {\n  private SinglyLinkedNode&lt;T&gt; next;\n  private T data;\n\n  public SinglyLinkedNode(T data) {\n    this.data = data;\n    this.next = null;\n  }\n}\nWe also renamed the Node class to SinglyLinkedNode to make it clear that this is for a singly-linked list.\nThe constructor for SinglyLinkedList will just set the head to null, initializing it as an empty list.\nclass SinglyLinkedList&lt;T&gt; {\n  private SinglyLinkedNode&lt;T&gt; head;\n\n  public SinglyLinkedList() {\n    this.head = null;\n  }\n}",
    "crumbs": [
      "Sequential Collections of Data",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Implementing Linked Lists</span>"
    ]
  },
  {
    "objectID": "15_implementing_linkedlists.html#adding-to-a-singly-linked-list",
    "href": "15_implementing_linkedlists.html#adding-to-a-singly-linked-list",
    "title": "19  Implementing Linked Lists",
    "section": "19.5 6.5 Adding to a Singly Linked List",
    "text": "19.5 6.5 Adding to a Singly Linked List\nThe SinglyLinkedNode\nclass needs getter and setter methods for accessing and modifying its private data and next fields.\nclass SinglyLinkedNode&lt;T&gt; {\n  private SinglyLinkedNode&lt;T&gt; next;\n  private T data;\n\n  public SinglyLinkedNode(T data) {\n    this.data = data;\n    this.next = null;\n  }\n\n  public SinglyLinkedNode&lt;T&gt; getNext() {\n    return this.next;\n  }\n\n  public void setNext(SinglyLinkedNode&lt;T&gt; next) {\n    this.next = next;\n  }\n\n  public T getData() {\n    return this.data;\n  }\n\n  public void setData(T data) {\n    this.data = data;\n  }\n}\nWith the getters and setters in place, we can now turn our attention to adding a new node to the list. Here’s what we need to do when the list is empty:\npublic class SinglyLinkedList&lt;T&gt; {\n  private SinglyLinkedNode&lt;T&gt; head;\n\n  public SinglyLinkedList() {\n    this.head = null;\n  }\n\n  public void add(T data) {\n    head = new SinglyLinkedNode&lt;T&gt;(data);\n  }\n}\nWhen we’re adding elements or nodes to our singly linked list, we’ve seen that for the first addition, we simply set the head to a new node with the provided data. This is essentially the creation of the first element or node in our list.\nLet’s take a moment to visualize this scenario:\nhead --&gt; [A]\nIn this ASCII diagram, the --&gt; represents the link from the head to the first node A.\nNow, when we’re adding a second element, we must be careful. We can’t simply overwrite the head again, as it contains our first piece of data. Instead, we want to add our new node at the next available position.\nWe achieve this by setting the next member of the head node to point to our new data, like so:\npublic void add(T data) {\n  if (head == null) {\n    head = new SinglyLinkedNode&lt;T&gt;(data);\n  } else {\n    head.setNext(new SinglyLinkedNode&lt;T&gt;(data));\n  }\n}\nThe updated state of the list is now:\nhead --&gt; [A] --&gt; [B]\nBut what if we want to add a third node? Using the code above, we would inadvertently overwrite the link from A to B, and instead, link A directly to the new node.\nWe have to revise our strategy. What if we checked whether the next of the head was null and then added our new node there?\npublic void add(T data) {\n  if (head == null) {\n    head = new SinglyLinkedNode&lt;T&gt;(data);\n  } else if (head.getNext() == null) {\n    head.setNext(new SinglyLinkedNode&lt;T&gt;(data));\n  } else {\n    head.getNext().setNext(new SinglyLinkedNode&lt;T&gt;(data));\n  }\n}\nThis would result in:\nhead --&gt; [A] --&gt; [B] --&gt; [C]\nThat seems to work, right? But this code has limitations; it only accommodates the addition of up to three nodes.\nLet’s observe what happens when we add a fourth node using this code. We’d end up with:\nhead --&gt; [A] --&gt; [B] --&gt; [D]\nThe third node, [C], disappears! That’s not what we want. We need a way to add nodes to our list, regardless of its current length. Can you see a pattern forming in how we’re adding nodes? How about a loop to iterate over the list until we find a node with a null next field?\npublic void add(T data) {\n  if (head == null) {\n    head = new SinglyLinkedNode&lt;T&gt;(data);\n  } else {\n    SinglyLinkedNode&lt;T&gt; current = head;\n\n    while (current.getNext() != null) {\n      current = current.getNext();\n    }\n\n    current.setNext(new SinglyLinkedNode&lt;T&gt;(data));\n  }\n}\nNow, each time we call add(), we start at the head and follow the next references until we find a node where next is null. We then set that node’s next to the new node, effectively adding it to the end of the list.\nAnd there you have it! We’ve cracked the code for adding nodes to a singly linked list, regardless of its size.",
    "crumbs": [
      "Sequential Collections of Data",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Implementing Linked Lists</span>"
    ]
  },
  {
    "objectID": "15_implementing_linkedlists.html#searching-a-singly-linked-list",
    "href": "15_implementing_linkedlists.html#searching-a-singly-linked-list",
    "title": "19  Implementing Linked Lists",
    "section": "19.6 Searching a Singly Linked List",
    "text": "19.6 Searching a Singly Linked List\nHaving explored the intricacies of adding elements to our singly linked list, it’s only logical to now ask, “How can we find an element in our list?” After all, what good is storing data if we can’t retrieve it?\nLet’s delve into the process of developing a search method that behaves similarly to the indexOf method in the Java standard library. Our search method should return the index of the first occurrence of a given element in the list and -1 if the element is not found.\nBefore we get started, there’s an important update we need to make to our generics. We’ll be comparing objects in our list to the search target, which means we need to ensure these objects are comparable. To do this, we’ll update T to T extends Comparable&lt;T&gt;. This ensures that the type T implements the Comparable interface, providing us with the ability to compare objects of this type.\nclass SinglyLinkedNode&lt;T extends Comparable&lt;T&gt;&gt; {\n  private SinglyLinkedNode&lt;T&gt; next;\n  private T data;\n\n  //...\n}\n\nclass SinglyLinkedList&lt;T extends Comparable&lt;T&gt;&gt; {\n  private SinglyLinkedNode&lt;T&gt; head;\n\n  //...\n}\nAlright, with this adjustment in place, let’s get to developing our search method!\nIf you reflect on how we traverse our linked list, it may become clear that the same process can be utilized for searching. We start at the head, and we progress through the list via the next pointers until we either find our target or reach the end of the list.\nHere’s a potential implementation for the search method:\npublic int search(T target) {\n  SinglyLinkedNode&lt;T&gt; current = head;\n  int index = 0;\n\n  while (current != null) {\n    if (current.getData().compareTo(target) == 0) {\n      return index;\n    }\n    index++;\n    current = current.getNext();\n  }\n\n  return -1;  // Target not found\n}\nLet’s examine this piece by piece. We start by creating a reference to the head of our list and initializing an index variable at 0. We then enter a while loop that will continue as long as current is not null, effectively iterating over the entire list.\nInside the loop, we compare the data of the current node to our target using the compareTo method, which is available to us thanks to our Comparable constraint. If the data matches our target (compareTo returns 0), we’ve found our target and we return the current index.\nIf the data does not match our target, we increment our index and move to the next node in the list. If we reach the end of the list without finding our target, the method returns -1, indicating the target is not in the list.\nAnd that’s how we develop a search method for our singly linked list, providing us with the means to locate and retrieve data efficiently!",
    "crumbs": [
      "Sequential Collections of Data",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Implementing Linked Lists</span>"
    ]
  },
  {
    "objectID": "15_implementing_linkedlists.html#removing-from-a-singly-linked-list",
    "href": "15_implementing_linkedlists.html#removing-from-a-singly-linked-list",
    "title": "19  Implementing Linked Lists",
    "section": "19.7 Removing from a Singly Linked List",
    "text": "19.7 Removing from a Singly Linked List\nJust as we learned how to add elements to a singly linked list, let’s now turn our attention to the process of removing elements. As with adding elements, we’ll start with a simpler case and gradually address more complex scenarios. We’ll design a removeLast method, which removes the last node from the list.\nThere are three scenarios we need to consider:\n\nThe list is empty.\nThe list contains only one element.\nThe list contains more than one element.\n\n\n19.7.1 Case 1: Empty Linked List\nThe simplest scenario to handle is an empty list. If the list is empty, we have nothing to remove. Here’s a simple starting point for our removeLast method:\npublic void removeLast() {\n  if (head == null) {\n    return; // Nothing to remove.\n  }\n}\nThis code handles an empty list by simply returning without doing anything.\n\n\n19.7.2 Case 2: Linked List with One Element\nNow, let’s consider the case where our list contains only one element.\nTo remove the only element from the list, we would set head to null, effectively removing the link to that node.\nHere’s how our removeLast method looks now:\npublic void removeLast() {\n  if (head == null) {\n    return; // Nothing to remove.\n  } else if (head.getNext() == null) {\n    head = null; // Remove the only node in the list.\n  }\n}\n\n\n19.7.3 Case 3: Linked List with More Than One Element\nWhen the list contains more than one element, we must find the last node and the node before it. Why? Because to remove the last node, we must set the next field of the node before it to null.\nLet’s think about how we might achieve this. We can’t simply set head.getNext() = null like we did with the one-element list, because this would leave us with only the first node.\nInstead, we could use a loop similar to the one in the add method, but with a twist. Instead of stopping when current.getNext() == null, which would leave us at the last node, we stop when current.getNext().getNext() == null, which will leave us at the second-to-last node.\npublic void removeLast() {\n  if (head == null) {\n    return; // Nothing to remove.\n  } else if (head.getNext() == null) {\n    head = null; // Remove the only node in the list.\n  } else {\n    SinglyLinkedNode&lt;T&gt; current = head;\n    while (current.getNext().getNext() != null) {\n      current = current.getNext();\n    }\n    current.setNext(null); // Remove the last node from the list.\n  }\n}\nNow, we have a removeLast method that handles any size list!\n\n\n19.7.4 Moving to remove(int i)\nRemoving the last node from a list is a good starting point, but what if we want to remove an arbitrary node at index i?\nFor this, we would need to iterate over the nodes until we reach the (i-1)th node (just before the node we want to remove), then update its next field to skip over the ith node and link to the (i+1)th node.\nHowever, this requires careful checking of edge cases, such as when i is 0 (requiring us to update the head), or when i is greater than the size of the list.\nThis will be our next challenge to tackle! Let’s see how we can go about creating a remove(int i) method, which will remove a node at a given index from our singly linked list.\nAs previously mentioned, our approach will be to traverse the list until we reach the (i-1)th node, and then adjust its next field to skip the ith node, effectively removing it from the list.\nHere’s a basic version of this method:\npublic void remove(int i) {\n  if (i == 0) {\n    head = head.getNext(); // If the node to remove is the head, move the head to the next node.\n  } else {\n    SinglyLinkedNode&lt;T&gt; current = head;\n    for (int j = 0; j &lt; i - 1; j++) {\n      current = current.getNext();\n    }\n    current.setNext(current.getNext().getNext());\n  }\n}\nThis is a simple implementation, but it lacks protection against edge cases. For instance, what happens if i is negative or if it’s larger than the size of the list?\nWe can address these cases by adding a few checks to our method:\npublic void remove(int i) {\n  if (i &lt; 0) {\n    return; // Do nothing for negative index.\n  } else if (i == 0) {\n    head = head.getNext(); // If the node to remove is the head, move the head to the next node.\n  } else {\n    SinglyLinkedNode&lt;T&gt; current = head;\n    for (int j = 0; j &lt; i - 1; j++) {\n      if (current.getNext() == null) {\n        return; // If the index is out of range, do nothing.\n      }\n      current = current.getNext();\n    }\n    if (current.getNext() != null) {\n      current.setNext(current.getNext().getNext());\n    }\n  }\n}\nThis updated method takes care of any negative index or index that’s out of range by simply returning without making any changes. If the index is zero, we update the head of the list to be the next node. If the index is within the range of the list, we find the node at (i-1) and update its next reference to skip over the ith node.\nAnd with that, we’ve covered the basics of how to add and remove nodes from a singly linked list!\n\n\n19.7.5 Summary and Conclusion\nIn this chapter, we learned how to implement linked lists in Java using references and objects. We saw how references can point to other objects in memory and how we can use them to create nodes that store data and link to other nodes. We also learned how to define a class for a singly linked list that contains a reference to the first node, called the head, and methods for adding, searching, and removing elements. We compared the advantages and disadvantages of linked lists with array-backed lists and discussed some applications of linked lists in real-world problems. We concluded that linked lists are a dynamic and flexible data structure that can grow and shrink as needed, but they also have some drawbacks such as extra memory overhead, lack of random access, and potential memory leaks.",
    "crumbs": [
      "Sequential Collections of Data",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Implementing Linked Lists</span>"
    ]
  },
  {
    "objectID": "16_stack.html",
    "href": "16_stack.html",
    "title": "20  Stack Data Structure",
    "section": "",
    "text": "20.1 Introduction\nA stack is a linear data structure that follows a last-in-first-out (LIFO) principle. That means the last element added to a stack is the first one to be removed. A stack has only one end, called the top, where elements can be inserted or deleted.\nYou can think of a stack as a pile of books. You can only add or remove books from the top of the pile. The book that you added last will be on top, and you have to remove it first before you can access any other book below it.\nStacks are useful for many applications that require reversing, backtracking, or undoing operations. For example, when you use an undo button in a text editor, you are using a stack to store your previous actions and revert them in reverse order.",
    "crumbs": [
      "Stacks and Queues",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Stack Data Structure</span>"
    ]
  },
  {
    "objectID": "16_stack.html#operations-on-a-stack",
    "href": "16_stack.html#operations-on-a-stack",
    "title": "20  Stack Data Structure",
    "section": "20.2 Operations on a Stack",
    "text": "20.2 Operations on a Stack\nThe two basic operations of a stack are push and pop. Push adds a new element to the top of the stack, while pop removes an element from the top of the stack. Both operations take constant time, that is O(1), because they only involve changing one pointer.\nAnother operation that can be useful for stacks is peek. Peek returns the top element of the stack without removing it. This can be helpful for checking what is on top of the stack before performing any other operation. Peek also takes constant time, O(1), because it only accesses one element.\nHere is an example of how these operations work on a stack:\n\nInitially, the stack is empty: []\nPush 10: [10]\nPush 20: [10, 20]\nPush 30: [10, 20, 30]\nPop: returns 30 and removes it from the stack: [10, 20]\nPeek: returns 20 but does not remove it from the stack: [10, 20]\nPop: returns 20 and removes it from the stack: [10]\nPop: returns 10 and removes it from the stack: []\nPop: returns an error or null because there is nothing to pop",
    "crumbs": [
      "Stacks and Queues",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Stack Data Structure</span>"
    ]
  },
  {
    "objectID": "16_stack.html#applications-of-a-stack",
    "href": "16_stack.html#applications-of-a-stack",
    "title": "20  Stack Data Structure",
    "section": "20.3 Applications of a Stack",
    "text": "20.3 Applications of a Stack\nStacks have many applications in various domains, such as expression evaluation, backtracking, function calls, undo/redo operations, browser history, etc. In this section, we will focus on one application: expression evaluation.\nExpression evaluation is the process of computing the value of an arithmetic or logical expression. For example, given an expression like 2 + 3 4 - 5 / (6 + 7), we want to find its value according to some rules of precedence and associativity.\nSo, for the problem of “Expression evaluation”, the input is a string like \"2 + 3 * 4 - 5 / (6 + 7)\", and the output is the value of the expression as an integer or a floating point number. This is much harder to do than it initially appears to be, particularly when we consider the rules of precedence and associativity.\nTo solve this problem, we can first convert an expression from infix notation (where operators are between operands) to postfix notation (where operators are after operands) using a stack.\nIt is useful to convert infix expressions to postfix expressions because postfix expressions are easier for computers to evaluate. Postfix expressions do not need parentheses or precedence rules to determine the order of operations. They can be evaluated from left to right using a stack.\n\nInfix: 2 + 3 * 4 (=&gt; 2 + 12 =&gt; 14)\nPostfix: 2 3 4 * + (=&gt; 2 12 + =&gt; 14)\nInfix: 2 + 3 * 4 - 5 / (6 + 7)\nPostfix: 2 3 4 * + 5 6 7 + / -\n\nFor example, to evaluate a postfix expression like 2 3 4 * +, we can use a stack as follows:\n\nScan the expression from left to right\nIf we encounter an operand (a number), we push it onto the stack\nIf we encounter an operator (+, -, *, /), we pop two operands from the stack, apply the operator on them, and push the result back onto the stack\nAt the end of the expression, there will be only one value on the stack, which is the final result.\n\nHere are the steps in action -\n\nScan the expression from left to right - 2 3 4 * +\nEncounter 2, push it onto the stack (stack: [2])\nEncounter 3, push it onto the stack (stack: [2, 3])\nEncounter 4, push it onto the stack (stack: [2, 3, 4])\nEncounter , pop two operands (4 and 3) from the stack and apply  on them. Push the result (12) onto the stack.(stack: [2, 12])\nEncounter +, pop two operands (12 and 2) from the stack and apply + on them. Push the result (14) onto the stack. (stack: [14])\nReach the end of the expression, pop the final result (14) from the stack and output it. (stack: [])\n\nAs for how you convert an infix expression to a postfix expression, you can use stacks again! Here are the steps:\n\nIf we encounter an operand (a number), we append it to the output string.\nIf we encounter an operator (+, -, *, /), we push it onto the stack if it has higher precedence than the top of the stack. Otherwise, we pop all operators with equal or higher precedence than it from the stack and append them to the output string. Then we push it onto the stack.\nIf we encounter a left parenthesis ‘(’, we push it onto the stack.\nIf we encounter a right parenthesis ‘)’, we pop all operators from the stack until we reach a left parenthesis ‘(’ and append them to the output string. Then we discard both parentheses.\nIf we reach the end of the input, pop all operators from the stack and append them to the output string.\nThe final output string is the postfix expression.\n\nFor example, to convert the infix expression 2 + 3 * 4 - 5 / (6 + 7) to postfix, we can use a stack as follows:\n\nScan the expression from left to right - 2 + 3 * 4 - 5 / (6 + 7)\nEncounter 2, append it to the output string (output: 2)\nEncounter +, push it onto the stack (stack: [+])\nEncounter 3, append it to the output string (output: 2 3)\nEncounter *, push it onto the stack (stack: [+, *])\nEncounter 4, append it to the output string (output: 2 3 4)\nEncounter -, push it onto the stack (stack: [+, *, -])\nEncounter 5, append it to the output string (output: 2 3 4 5)\nEncounter /, pop all operators with higher or equal precedence from the stack and append them to the output string. Then push / onto the stack (output: 2 3 4 5 * /, stack: [+, -])\nEncounter (, push it onto the stack (stack: [+, -, (])\nEncounter 6, append it to the output string (output: 2 3 4 5 * / 6)\nEncounter +, push it onto the stack (stack: [+, -, (, +])\nEncounter 7, append it to the output string (output: 2 3 4 5 * / 6 7)\nEncounter ), pop all operators from the stack until we reach the matching left parenthesis and append them to the output string. Discard both parentheses (output: 2 3 4 5 * / 6 7 + -, stack: [+])\nPop the remaining operator (+) from the stack and append it to the output string (output: 2 3 4 5 */ 6 7 + - +)\n\nThe final output string is 2 3 4* + 5 6 7 + / -, which is the postfix notation of the original infix expression.",
    "crumbs": [
      "Stacks and Queues",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Stack Data Structure</span>"
    ]
  },
  {
    "objectID": "17_queue.html",
    "href": "17_queue.html",
    "title": "21  Queue Data Structure",
    "section": "",
    "text": "21.1 Introduction\nA queue is a linear data structure that is open at both ends and follows a particular order for storing data. The order is First In First Out (FIFO). That means that the first element that is added to the queue is also the first one that is removed from it3. One can imagine a queue as a line of people waiting to receive something in sequential order which starts from the beginning of the line.\nSome real-life examples of queues are:\nQueues are useful for modeling situations where we need to process data elements one by one in the order they arrive.",
    "crumbs": [
      "Stacks and Queues",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Queue Data Structure</span>"
    ]
  },
  {
    "objectID": "17_queue.html#introduction",
    "href": "17_queue.html#introduction",
    "title": "21  Queue Data Structure",
    "section": "",
    "text": "Waiting lines at a bank, a restaurant, or an airport\nPrinter jobs that are processed in the order they are received\nCustomer service calls that are answered based on who called first",
    "crumbs": [
      "Stacks and Queues",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Queue Data Structure</span>"
    ]
  },
  {
    "objectID": "17_queue.html#operations-on-a-queue",
    "href": "17_queue.html#operations-on-a-queue",
    "title": "21  Queue Data Structure",
    "section": "21.2 Operations on a Queue",
    "text": "21.2 Operations on a Queue\nThe main operations that we can perform on a queue are:\n\nEnqueue: This operation adds an element to the rear end of the queue. We can only enqueue an element if the queue is not full.\nDequeue: This operation removes an element from the front end of the queue. We can only dequeue an element if the queue is not empty. The dequeued element is returned as the output of this operation.\nPeek or Front: This operation returns (but does not remove) the element at the front end of the queue without modifying it. We can only peek at an element if the queue is not empty.\nPoll: This operation is similar to dequeue, but it returns null instead of throwing an exception if the queue is empty.\n\n\nNote that the exact operations, their names and behavior often vary depending on the programming language and the implementation of the queue. You always need to check the documentation of the programming language you are using to know the exact operations and their behavior.",
    "crumbs": [
      "Stacks and Queues",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Queue Data Structure</span>"
    ]
  },
  {
    "objectID": "17_queue.html#applications-of-queues",
    "href": "17_queue.html#applications-of-queues",
    "title": "21  Queue Data Structure",
    "section": "21.3 Applications of Queues",
    "text": "21.3 Applications of Queues\nA queue data structure is generally used in scenarios where we need to follow a First In First Out (FIFO) approach for managing data elements. That means that we process data elements one by one in the order they arrive or are added to the queue.\nSome common applications of the queue data structure are:\n\nTask Scheduling: Queues can be used to schedule tasks based on priority or the order in which they were received. For example, a printer may use a queue to store print jobs that are waiting to be executed. The printer will print the documents in the order they were added to the queue, ensuring fairness and efficiency.\nResource Allocation: Queues can be used to manage and allocate resources, such as printers, CPU processing time, disk space, etc. For example, an operating system may use a queue to handle requests for CPU time from different processes. The operating system will grant CPU time to each process based on its position in the queue, ensuring that no process is starved or neglected.\nData Buffering: Queues can be used to buffer data between two components that operate at different speeds or have different capacities. For example, a keyboard may use a queue to store keystrokes that are entered by the user but not yet processed by the computer. The keyboard will enqueue each keystroke as it is typed, and the computer will dequeue each keystroke as it is ready to process it.\n\nOne application of queue data structure that we will focus on is CPU Scheduling.\nCPU scheduling is the problem of deciding which process should get access to the CPU at any given time. CPU scheduling is important for optimizing CPU utilization, throughput, response time, waiting time, etc. CPU scheduling algorithms use queues to store processes that are ready or waiting for execution.\nDifferent types of queues can be used for CPU scheduling:\n\nSingle Queue: This is a simple FIFO queue that stores all processes that are ready for execution. The CPU will execute processes in the order they arrive or are added to this queue. This type of queue ensures fairness but does not consider priority or burst time (the amount of time required by a process for execution) of processes. An example of this type of queue is FCFS (First Come First Serve) scheduling algorithm.\nMultiple Queues: This is a set of FIFO queues that store processes based on some criteria such as priority or burst time. The CPU will execute processes from different queues according to some rules such as round robin (each queue gets an equal share of CPU time), shortest job first (the queue with the shortest burst time gets higher preference), etc. This type of queue allows more flexibility and control over CPU scheduling but may introduce complexity and overheads. An example of this type of queue is MLFQ (Multi-Level Feedback Queue) scheduling algorithm.\nPriority Queue: This is a special type of queue that stores processes based on their priority values. The priority value can be static (assigned at creation) or dynamic (changed during execution). The CPU will execute processes with higher priority values before lower ones. This type of queue ensures urgency but may cause starvation (low-priority processes never get executed) or inversion (high-priority process waits for the low-priority process) problems.",
    "crumbs": [
      "Stacks and Queues",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Queue Data Structure</span>"
    ]
  },
  {
    "objectID": "18_recursion.html",
    "href": "18_recursion.html",
    "title": "22  Recursion",
    "section": "",
    "text": "22.1 Writing Recursive Functions\nRecursion is a computational technique in which a function refers to itself in its own definition to solve a problem. The essence of creating recursive functions is ensuring that the recursive call addresses a smaller version of the initial problem. This strategy enables the efficient and elegant decomposition of complex problems into more manageable sub-problems.\nLet’s consider some key characteristics of recursive functions:\nSome Advantages of Recursion -\nSome Disadvantages of Recursion -\nSome Examples of Recursive Problems -\nUnderstanding and implementing recursive functions often necessitates thinking of the broader problem in terms of smaller, similar instances. If you can decompose the main problem into subproblems that echo the original, you’re in a position to use recursion.\nFollow these general steps when designing a recursive function:\nLet’s delve into examples of recursive functions that exhibit diverse structures:",
    "crumbs": [
      "Recursion",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Recursion</span>"
    ]
  },
  {
    "objectID": "18_recursion.html#writing-recursive-functions",
    "href": "18_recursion.html#writing-recursive-functions",
    "title": "22  Recursion",
    "section": "",
    "text": "Identify the base case(s): Determine the simplest version of the problem that can be resolved directly without invoking recursion. This serves as the recursion’s termination point.\nReduce the problem: Consider how the main problem can be diminished to a smaller but similar subproblem.\nInvoke the recursive call: Initiate the recursive call on the smaller subproblem, passing any necessary parameters.\nConstruct the solution: Utilize the result of the recursive call to assemble the solution for the original problem.\nReturn the result: Once the solution is obtained, return the result.\n\n\n\n22.1.1 Single Base Case, Single Recursive Case\nThis is the most straightforward recursive structure featuring a solitary base case and a single recursive call on a reduced subproblem.\n// Calculate factorial of n\nint factorial(int n) {\n  // Base case\n  if (n == 0) {\n    return 1; \n  }\n  \n  // Recursive call\n  return n * factorial(n-1);\n}\nIn the above code, the base case is when n is zero, and the recursive call is made on n-1 (a smaller instance of the original problem).\n\n\n22.1.2 Multiple Base Cases\nSome problems might need multiple base cases to accommodate different elementary variants of the problem:\n// Count ways to climb n steps \n// Can climb 1 or 2 steps at a time\nint countWays(int n) {\n\n  // Base cases\n  if (n == 1) {\n    return 1;\n  }\n  if (n == 2) {\n    return 2;\n  }\n\n  // Recursive calls\n  return countWays(n-1) + countWays(n-2);\n\n}\nHere, there are two base cases (n == 1 and n == 2), reflecting the two possible ways of ascending a short staircase.\n\n\n22.1.3 Multiple Recursive Cases\nMore intricate problems might necessitate multiple recursive calls on diverse subproblems:\n// Fibonacci number\nint fib(int n) {\n  // Base case\n  if (n &lt;= 1) { \n    return n;\n  } \n  \n  // Recursive calls\n  return fib(n-1) + fib(n-2);\n}\nThe Fibonacci sequence computation involves two recursive calls, each on a different subproblem (n-1 and n-2).",
    "crumbs": [
      "Recursion",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Recursion</span>"
    ]
  },
  {
    "objectID": "18_recursion.html#writing-advanced-recursive-functions",
    "href": "18_recursion.html#writing-advanced-recursive-functions",
    "title": "22  Recursion",
    "section": "22.2 Writing Advanced Recursive Functions",
    "text": "22.2 Writing Advanced Recursive Functions\nWhile some recursive functions feature only a single base case and a single recursive call, it’s often the case that a function may have multiple base cases, multiple recursive cases, or a combination of both.\nHere’s the general template for such a recursive function in Java:\nif ( base case 1 ) {\n  // return some simple expression\n}\nelse if ( base case 2 ) {\n  // return some simple expression\n}\n...\nelse if ( recursive case 1 ) {\n  // some work before \n  // recursive call \n  // some work after \n }\n...\nelse { // last recursive case \n  // some work before \n  // recursive call \n  // some work after \n }\nEach base case returns a simple expression, whereas each recursive case performs some work before and after the recursive call.\nLet’s now explore some specific examples.\n\n22.2.1 Example 1: Prime Number Determination\nConsider a function that checks whether a given integer X is a prime number. Here, Y is a helper variable acting as the divisor. When the function is initially invoked, Y is set to X - 1.\nboolean prime(int x, int y) {\n  if (y == 1) {\n    return true;\n  }\n  else if (x % y == 0) {\n    return false; \n  }\n  else {\n    return prime(x, y-1);\n  }\n}\nLet’s understand how the prime function works. Remember, the purpose of this function is to check if a number X is prime.\nA prime number is a natural number greater than 1 that has no positive divisors other than 1 and itself. For instance, the first six prime numbers are 2, 3, 5, 7, 11, and 13.\nIn our prime function, we recursively check if X can be evenly divided by any number from X - 1 down to 2. If X can be evenly divided by any such number (i.e., x % y == 0 is true for some y), then X is not a prime number, and the function returns false.\nOn the other hand, if X cannot be evenly divided by any such number (i.e., we’ve checked all y from X - 1 down to 2 without finding an even divisor), then X is a prime number, and the function returns true.\nLet’s walk through the example of prime(7, 6):\n\nSince y is not 1, and 7 % 6 does not equal 0, we make the recursive call prime(7, 5).\nAgain, y is not 1, and 7 % 5 does not equal 0, so we make the recursive call prime(7, 4).\nThis pattern continues until we reach prime(7, 2), which again does not evenly divide 7, so we make the last recursive call prime(7, 1).\nNow y is 1, so we return true, unwinding the recursion and indicating that 7 is indeed a prime number.\n\nThis function correctly identifies whether a number X is prime. As with all recursive functions, it is important to understand the base case(s) and how the recursive calls are working towards reaching them. The process of “winding” (making recursive calls) and “unwinding” (returning from recursive calls) is central to how recursion works.\n\n\n22.2.2 Example 2: Subset Sum Problem\nThe following function, isSubsetSum, checks whether a subset of an integer array set can sum up to a given number sum. Here, n represents the number of array elements to consider. We don’t directly use set.length as recursive calls need to examine only part of the array.\nboolean isSubsetSum(int set[], int n, int sum) {\n  if (sum == 0) {\n    return true;\n  }\n  if ((n == 0) && (sum != 0)) {\n    return false;\n  }\n  if (set[n - 1] &gt; sum) {\n    return isSubsetSum(set, n - 1, sum);\n  }\n  return isSubsetSum(set, n - 1, sum) || isSubsetSum(set, n - 1, sum - set[n - 1]); \n}\nTo understand how this function works, we need to consider the problem it’s trying to solve: checking if there is a subset of set that sums up to sum. It does this by recursively examining each element in set and checking two cases:\n\nIs there a subset within the first n - 1 elements of set that sums to sum (i.e., the current element is not included in the sum)?\nIs there a subset within the first n - 1 elements that sums to sum - set[n - 1] (i.e., the current element is included in the sum)?\n\nBefore checking these two cases, we have three base cases:\n\nIf sum == 0, the function returns true. This is because an empty set always sums to 0.\nIf n == 0 (i.e., there are no more elements to consider) and sum is not zero, the function returns false as there are no more numbers to add up to sum.\nIf the current element (i.e., set[n - 1]) is larger than sum, we cannot include this element in the sum, so we only check for a subset within the first n - 1 elements that sum to sum.\n\nTo illustrate this, let’s trace isSubsetSum([3, 1, 5, 9, 12], 5, 9), which checks if there’s a subset of [3, 1, 5, 9, 12] that sums to 9.\nThe recursive tree will look like this:\nisSubsetSum([3, 1, 5, 9, 12], 5, 9)\n|____ isSubsetSum([3, 1, 5, 9, 12], 4, 9) \n|     |____ isSubsetSum([3, 1, 5, 9, 12], 3, 9) \n|     |     |____ isSubsetSum([3, 1, 5, 9, 12], 2, 9)\n|     |     |     |____ isSubsetSum([3, 1, 5, 9, 12], 1, 9)  =&gt; False\n|     |     |     |____ isSubsetSum([3, 1, 5, 9, 12], 1, 6)  =&gt; False\n|     |     |____ isSubsetSum([3, 1, 5, 9, 12], 2, 4)\n|     |           |____ isSubsetSum([3, 1, 5, 9, 12], 1, 4) =&gt; False\n|     |           |____ isSubsetSum([3, 1, 5, 9, 12], 1, 3) =&gt; True\n|     |____ isSubsetSum([3, 1, 5, 9, 12], 3, 0) =&gt; True\n|____ isSubsetSum([3, 1, 5, 9, 12], 4, -3)  =&gt; False\nSo, the function will return true as there exists a subset (3, 1, 5) that sums up to 9.\nBy recursively breaking down the problem, isSubsetSum can effectively and efficiently find whether there’s a subset that matches the desired sum. This illustrates how recursive functions can solve complex problems that might be non-intuitive or hard to solve with iterative methods.\n\n\n22.2.3 Example 3: Basketball Scoring Combinations\nIn this example, we create a function paths to calculate the number of ways to reach a given score in a basketball game, given that points can be accumulated in increments of 1, 2, or 3. If n = 3, for instance, paths will return 4, since there are four different ways to accumulate 3 points: 1+1+1, 1+2, 2+1, and 3.\nint paths(int n) {\n  if (n == 1) {\n    return 1;\n  }\n  if (n == 2) {\n    return 2;\n  }\n  if (n == 3) {\n    return 4;\n  }\n  return paths(n - 1) + paths(n - 2) + paths(n - 3);\n}\nThe function uses three base cases (n == 1, n == 2, n == 3) and three recursive calls, each corresponding to scoring 1, 2, or 3 points respectively. Let’s dive into how this function works. This recursive function is a little more complex than the previous factorial example because it has three base cases and makes three recursive calls. However, the principle is the same: we build up a complex calculation from simpler parts, and then start “unwinding” once we hit the base cases.\nThe paths function takes an integer n as its argument and returns the number of possible ways to reach n points in a basketball game. This is a classic combinatorial problem: we’re essentially counting the number of unique combinations of 1s, 2s, and 3s that can add up to n.\nLet’s trace the function call paths(4) as an example:\n\nFirst, the function checks if n is 1, 2, or 3 — these are our base cases. If n is one of these numbers, the function can immediately return the result. But in our case, n is 4, so we proceed to the recursive part of the function.\nNow the function makes three recursive calls: paths(n - 1), paths(n - 2), and paths(n - 3), or paths(3), paths(2), and paths(1). These represent the three ways we could reach 4 points: by scoring a point when we had 3, by scoring 2 points when we had 2, or by scoring 3 points when we had 1.\nEach of these calls will in turn make their own recursive calls, until they eventually hit one of the base cases. At that point, the function will start returning values back up the call stack.\n\nSo for paths(4), we would have:\n\npaths(4) = paths(3) + paths(2) + paths(1)\npaths(3) = 4 (from the base case)\npaths(2) = 2 (from the base case)\npaths(1) = 1 (from the base case)\nTherefore, paths(4) = 4 + 2 + 1 = 7\n\nSo there are seven ways to reach a score of 4 points.\nThis kind of tracing can help you understand how recursion builds up complex computations from simpler parts, and how the function’s call stack unwinds once it hits the base cases. Try applying this method of tracing to other recursive functions to get a feel for how they work.\nRemember to take the time to understand how each of these functions work. Recursion can seem complex at first, but with practice, it becomes a powerful tool in your programming arsenal.",
    "crumbs": [
      "Recursion",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Recursion</span>"
    ]
  },
  {
    "objectID": "18_recursion.html#tracing-recursive-code",
    "href": "18_recursion.html#tracing-recursive-code",
    "title": "22  Recursion",
    "section": "22.3 Tracing Recursive Code",
    "text": "22.3 Tracing Recursive Code\nWhile creating recursive functions, it is beneficial to approach the task in a top-down manner. Trust that the recursive call will correctly solve the subproblem, and then use that result, as you would with any other function, to solve the original problem.\nHowever, when analyzing or tracing a recursive function, you do need to understand how the function operates. Tracing a few recursive functions helps you grasp the behavior of recursion. With experience, you won’t need to trace through every detail. Your understanding of recursion will gradually solidify, and your confidence will grow.\nRecursive calls operate in two phases: the “winding” phase, where information is passed from one recursive call to the next, and the “unwinding” phase, where return values are passed back. These phases are sometimes overlooked, but they are essential to understanding recursion.\nIn the winding phase, any parameter passed through the recursive call travels forward until the base case is reached. In the unwinding phase, the function’s return value (if any) travels backward to the calling function.\nLet’s illustrate this with an example: a recursive function to compute the factorial of a number.\nint factorial(int n) {\n  if (n == 0) {\n    return 1;  // base case\n  } else {\n    return n * factorial(n - 1);  // recursive case\n  }\n}\nIn this case, the information (the decremented value of n) flows forward during the winding phase, and the computed factorial flows backward during the unwinding phase.\nA recursive function can also pass multiple parameters forward. For instance, a function that recursively sums the values in an array might pass the array and the current index in the winding phase, and return the cumulative sum during the unwinding phase.\nint arraySum(int[] arr, int n) {\n  if (n &lt;= 0) {\n    return 0;  // base case\n  } else {\n    return arr[n - 1] + arraySum(arr, n - 1);  // recursive case\n  }\n}\nIn this case, the array and the index (n - 1) are passed forward in the winding phase, while the summed value so far is passed back during the unwinding phase.\nTo better understand the idea of recursion, consider it as a domino effect where each recursive call is like tipping over a domino. These rules can guide your thinking:\n\nJust like the first domino has to be tipped manually, the base case solution in recursion is computed non-recursively.\nBefore any given domino can fall, all preceding dominos must have been tipped over. Similarly, before any recursive call can complete, all deeper recursive calls must complete.\n\nThrough this process of tracing and understanding, recursion becomes a powerful tool in problem-solving.",
    "crumbs": [
      "Recursion",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Recursion</span>"
    ]
  },
  {
    "objectID": "18_recursion.html#practice-exercises",
    "href": "18_recursion.html#practice-exercises",
    "title": "22  Recursion",
    "section": "22.4 Practice Exercises",
    "text": "22.4 Practice Exercises\nNow that we’ve laid out the theory of recursion and tracing recursive calls, let’s get some hands-on practice with some exercises. Remember, the more you practice, the better you’ll understand how recursion works.\n\n22.4.1 Exercise 1: Factorial Function\nConsider the recursive function for calculating the factorial of a number:\nint factorial(int n) {\n  if (n == 0) {\n    return 1;\n  }\n  else {\n    return n * factorial(n - 1);\n  }\n}\nTask: Trace the recursive calls made when factorial(4) is called.\n\n\n22.4.2 Exercise 2: Fibonacci Series\nConsider the recursive function for generating the Fibonacci series:\nint fibonacci(int n) {\n  if (n &lt;= 1) {\n    return n;\n  }\n  else {\n    return fibonacci(n - 1) + fibonacci(n - 2);\n  }\n}\nTask: Trace the recursive calls made when fibonacci(5) is called. Pay close attention to how the same sub-problems are solved multiple times.\n\n\n22.4.3 Exercise 3: Sum of Array Elements\nConsider the recursive function for finding the sum of all elements in an array:\nint sumArray(int[] arr, int n) {\n  if (n &lt;= 0) {\n    return 0;\n  }\n  else {\n    return sumArray(arr, n-1) + arr[n-1];\n  }\n}\nTask: Given the array {1, 2, 3, 4, 5}, trace the recursive calls made when sumArray(arr, 5) is called.\nTake your time working through these exercises and understand how each recursive call is made. After you’ve completed them, try creating your own recursive functions and tracing through their calls.\nIf you’re still finding it difficult to understand how the tracing is done, don’t hesitate to ask for help. Recursion is a difficult concept to grasp initially, but with practice and patience, it’ll become much clearer.",
    "crumbs": [
      "Recursion",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Recursion</span>"
    ]
  },
  {
    "objectID": "18_recursion.html#solution-walkthrough-factorial-function",
    "href": "18_recursion.html#solution-walkthrough-factorial-function",
    "title": "22  Recursion",
    "section": "22.5 Solution Walkthrough: Factorial Function",
    "text": "22.5 Solution Walkthrough: Factorial Function\nLet’s trace the recursive calls made when we calculate factorial(4) using our recursive factorial function. Here’s the function again for reference:\nint factorial(int n) {\n  if (n == 0) {\n    return 1;\n  }\n  else {\n    return n * factorial(n - 1);\n  }\n}\nWhen we call factorial(4), we’re not at the base case (n == 0), so we go to the else clause. Here, we calculate 4 * factorial(4 - 1), or 4 * factorial(3). Now, we need to calculate factorial(3) before we can continue. This process repeats for each recursive call:\n\nfactorial(4) = 4 * factorial(3)\nfactorial(3) = 3 * factorial(2)\nfactorial(2) = 2 * factorial(1)\nfactorial(1) = 1 * factorial(0)\n\nOnce we reach factorial(0), we’ve hit our base case, and the function returns 1:\n\nfactorial(0) = 1\n\nNow we can start unwinding the stack of calls:\n\nfactorial(1) = 1 * 1 = 1\nfactorial(2) = 2 * 1 = 2\nfactorial(3) = 3 * 2 = 6\nfactorial(4) = 4 * 6 = 24\n\nSo, factorial(4) returns 24. You can see how each recursive call builds on the previous one, and the base case provides the starting point for the calculation to unwind.\nThis same approach can be applied to other recursive functions to understand how they work. Try it out with the remaining exercises to practice tracing recursive calls.",
    "crumbs": [
      "Recursion",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Recursion</span>"
    ]
  },
  {
    "objectID": "19_binary_search.html",
    "href": "19_binary_search.html",
    "title": "23  Binary Search",
    "section": "",
    "text": "23.1 Recursive Linear Search\nLet’s imagine the following scenario: You’re in a well-organized library, each shelf categorized by the author’s last name in alphabetical order from A to Z. You’re tasked to find a book by your favorite author, say, ‘Margaret Atwood’.\nSo, you start at ‘A’ and proceed book by book, author by author, until you eventually find the book by Atwood. This, in essence, is a prime example of the linear search algorithm. Simple, right?\nLet’s formalize this process with Java:\nThis method receives an array arr and a key key. It uses a loop to traverse the array, checking if the current element is equal to the searched key. If it is, it returns the current index i. If it doesn’t find the key in the entire array, it returns -1.\nWe start the search at the beginning of the array, just like we started at the ‘A’ section in our library, and examine each element just like we looked at each book. When we find a match, we stop searching, just like we did when we found our Atwood’s book.\nDespite its simplicity, the linear search algorithm isn’t very efficient. For example, let’s say the library decided to include all authors worldwide. With the linear search strategy, you might end up examining books by millions of authors before you reach Margaret Atwood!\nThis brings us to our problem – How can we make this search more efficient?\nSince our last chapter was on recursion, let’s try to solve this problem using recursion. We’ll call our method linearSearchRecursive:\nThe linearSearchRecursive method is a recursive function that performs a linear search on an array of integers. A linear search is a simple algorithm that checks each element of the array in order until it finds the target value or reaches the end of the array. The method takes three parameters: arr, which is the array to be searched, key, which is the value to be found, and index, which is the current position in the array.\nThe method has three cases:\nThe data flows through the recursive calls via the parameters and return values. The parameters (arr, key, and index) help in propagating information forward through the recursive calls until reaching one of the base cases. The return values (-1 or index) relay information back from the base case to the original caller.\nLet’s illustrate this with an example: Suppose we want to search for 5 in the array {3, 1, 4, 2, 5} using linearSearchRecursive. We start by calling linearSearchRecursive(arr, 5, 0), where arr is {3, 1, 4, 2, 5}, key is 5, and index is 0. The recursive tree will look like this:\nAt each recursive call, we check if we have reached one of the base cases. If not, we make another recursive call with an incremented index. When we reach linearSearchRecursive(arr, 5, 4), we find that arr[4] is equal to key, so we return 4. This value is then passed back up the call stack until it reaches the original caller. So, linearSearchRecursive(arr, 5, 0) returns 4, indicating that 5 is found at index 4 in the array.",
    "crumbs": [
      "Recursion",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Binary Search</span>"
    ]
  },
  {
    "objectID": "19_binary_search.html#recursive-linear-search",
    "href": "19_binary_search.html#recursive-linear-search",
    "title": "23  Binary Search",
    "section": "",
    "text": "public int linearSearch(int[] arr, int key){\n    for(int i=0;i&lt;arr.length;i++){\n        if(arr[i] == key){\n            return i; \n        }\n    }\n    return -1; \n}\n\n\n\n\n\npublic int linearSearchRecursive(int[] arr, int key, int index){\n    if(index &gt;= arr.length){\n        return -1; \n    }\n    else if(arr[index] == key){\n        return index; \n    }\n    else{\n        return linearSearchRecursive(arr, key, index+1); \n    }\n}\n\n\n\nThe base case 1: If index is equal to or greater than the length of the array, it means that the end of the array has been reached and the key was not found. In this case, the method returns -1 to indicate that the search was unsuccessful.\nThe base case 2: If the element at arr[index] is equal to key, it means that the key has been found at the current position. In this case, the method returns index to indicate the location of the key in the array.\nThe recursive case: If neither of the base cases are true, it means that the key has not been found yet and there are more elements to check. In this case, the method makes a recursive call to itself with the same array and key, but with an incremented index (index+1). This way, the method moves forward in the array until it either finds the key or reaches the end.\n\n\n\nlinearSearchRecursive(arr, 5, 0)\n|____ linearSearchRecursive(arr, 5, 1)\n      |____ linearSearchRecursive(arr, 5, 2)\n            |____ linearSearchRecursive(arr, 5, 3)\n                  |____ linearSearchRecursive(arr, 5, 4) =&gt; returns 4",
    "crumbs": [
      "Recursion",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Binary Search</span>"
    ]
  },
  {
    "objectID": "19_binary_search.html#optimizing-linear-search",
    "href": "19_binary_search.html#optimizing-linear-search",
    "title": "23  Binary Search",
    "section": "23.2 Optimizing Linear Search",
    "text": "23.2 Optimizing Linear Search\nOur recursive linear search algorithm successfully finds a given element within an array. However, in each recursive call, we reduce our search space by just one element. It’s akin to taking one step at a time when we could be leaping. Wouldn’t it be nice if we could shrink this search space by more than a single element at each step?\nLet’s imagine again that we’re in our library from earlier, but this time, we have a partner. You start at ‘A’ and your partner starts at ‘Z’. Now you’re moving towards each other, each drawing closer to ‘M’, where Atwood’s books are likely to reside. If either of you find the book, you signal the other, and the process hinges to a stop.\nLet’s translate this idea into code:\npublic int optimizedLinearSearch(int[] arr, int left, int right, int key){\n    if(left&gt;right) {\n        return -1;\n    }\n    if(arr[left] == key){\n        return left; \n    }\n    if(arr[right] == key){ \n        return right;\n    }\n    return optimizedLinearSearch(arr, left + 1, right - 1, key); \n}\nThe optimizedLinearSearch method is another recursive function that performs a linear search on an array of integers, but with some optimizations. The method takes four parameters: arr, which is the array to be searched, left, which is the leftmost index of the subarray to be searched, right, which is the rightmost index of the subarray to be searched, and key, which is the value to be found.\nThe method has three cases:\n\nThe base case 1: If left is greater than right, it means that the subarray is empty and the key was not found. In this case, the method returns -1 to indicate that the search was unsuccessful.\nThe base case 2: If the element at arr[left] or arr[right] is equal to key, it means that the key has been found at one of the ends of the subarray. In this case, the method returns left or right to indicate the location of the key in the array.\nThe recursive case: If neither of the base cases are true, it means that the key has not been found yet and there are more elements to check. In this case, the method makes a recursive call to itself with the same array and key, but with a smaller subarray (left + 1 to right - 1). This way, the method eliminates two elements from the search space at each recursive call until it either finds the key or reaches an empty subarray.\n\nThe data flows through the recursive calls via the parameters and return values. The parameters (arr, left, right, and key) help in propagating information forward through the recursive calls until reaching one of the base cases. The return values (-1, left, or right) relay information back from the base case to the original caller.\nThe optimization in this method comes from checking both ends of the subarray at each recursive call, instead of just one element as in linearSearchRecursive. This reduces the number of recursive calls needed to find the key or reach an empty subarray. For example, if we want to search for 5 in the array {3, 1, 4, 2, 5} using optimizedLinearSearch, we start by calling optimizedLinearSearch(arr, 0, 4, 5), where arr is {3, 1, 4, 2, 5}, left is 0, right is 4, and key is 5. The recursive tree will look like this:\noptimizedLinearSearch(arr, 0, 4, 5)\n|____ optimizedLinearSearch(arr, 1, 3, 5)\n      |____ optimizedLinearSearch(arr, 2, 2, 5) =&gt; returns -1\nAt each recursive call, we check if we have reached one of the base cases. If not, we make another recursive call with a smaller subarray. When we reach optimizedLinearSearch(arr, 2, 2, 5), we find that neither arr[2] nor arr[2] is equal to key, and that left &gt; right. So we return -1. This value is then passed back up the call stack until it reaches the original caller. So, optimizedLinearSearch(arr, 0, 4, 5) returns -1, indicating that 5 is not found in the array.\nNote that this method works correctly only if there are no duplicates in the array. If there are duplicates, it may return a different index than expected or miss some occurrences of the key. For example, if we want to search for 4 in the array {3, 4, 4, 2}, using optimizedLinearSearch(arr, 0, 3, 4), we will get -1 as a result instead of 1 or 2. This is because both ends of the subarray are eliminated at each recursive call until an empty subarray is reached.\nThe worst case complexity analysis of both snippets is as follows:\n\nThe linearSearchRecursive method has a worst-case complexity of O(n), where n is the length of the array. This is because in the worst case, the key is not present in the array or is the last element of the array, and the method has to check every element of the array until it reaches the end. The number of recursive calls is equal to n in this case.\nThe optimizedLinearSearch method has a worst-case complexity of O(n/2) - because in the worst case, the key is not present in the array or is somewhere in the middle of the array, and the method has to check half of the elements of the array until it reaches an empty subarray - which ends up being also O(n) in Big-O notation.\n\nSo it looks like the worst-case scenario hasn’t vastly improved. We’d still potentially have to traverse half the array size if the element resides in the middle. In the worst-case scenario, the time complexity remains O(n).\nThis leads us to a realization: minor tweaks to our approach may not provide the drastic improvement we hope for. Is there some drastic change we can make to eliminate substantial amounts of data from our search space, not just one or two elements, with every recursive call?",
    "crumbs": [
      "Recursion",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Binary Search</span>"
    ]
  },
  {
    "objectID": "19_binary_search.html#divide-and-conquer",
    "href": "19_binary_search.html#divide-and-conquer",
    "title": "23  Binary Search",
    "section": "23.3 Divide and Conquer",
    "text": "23.3 Divide and Conquer\nWhen faced with a complex problem, one of the most effective strategies is often to break it down into smaller, more manageable parts. This is the essence of the ‘divide and conquer’ strategy. We divide the problem into several sub-problems that are similar to the original but smaller in size. We conquer the sub-problems by solving them recursively. Finally, we combine the solutions of the sub-problems to solve the original problem.\nThe divide and conquer strategy is especially effective when the sub-problems can be solved independently and the solutions can be combined efficiently. In the context of searching, we can apply this strategy by dividing the search space into smaller parts and searching each part separately. If we can eliminate large parts of the search space at each step, we can find the target value much faster.\nLet’s see how we can apply this to Search, and our original problem of looking for books in a library!\nThe journey to our favorite author’s book in the library hasn’t been as fast as we initially desired. Checking every book, or even every other book, can still be cumbersome in a large library.\nSo, let’s revisit our library metaphor and imagine a different scenario. Now, you have an idea. Instead of searching from ‘A’ to ‘Z’ linearly, you begin your search in the middle of the library, around ‘M.’ If Atwood falls in the latter half, ignore everything from ‘A to L.’ And voila! Half the library gets eliminated instantly.\nBut in order for this to work, we need to make an assumption – the library is sorted alphabetically. If it isn’t, we’ll have to sort it first, which is a whole other problem. But if it is, we can apply the divide and conquer strategy to our search.\nNow let’s put it into practice by creating a new method called drasticallyFasterSearch:\npublic int drasticallyFasterSearch(int[] arr, int left, int right, int key) {\n    int mid = left + (right - left) / 2;\n    if (arr[mid] == key)\n        return mid;\n    else if (arr[mid] &gt; key)\n        return optimizedLinearSearch(\n          java.utils.Arrays.copyOfRange(arr, 0, mid - 1), 0, mid - 1, key\n        );\n    else\n        return optimizedLinearSearch(\n          java.utils.Arrays.copyOfRange(arr, mid + 1, right), 0, right - mid - 1, key\n        );\n}\nThe drasticallyFasterSearch method is a hybrid function that combines our earlier optimizedLinearSearch to our new Divide and Conquer strategy. The method takes four parameters: arr, which is the array to be searched, left, which is the leftmost index of the subarray to be searched, right, which is the rightmost index of the subarray to be searched, and key, which is the value to be found.\nThe method has three cases:\n\nThe base case: If the element at the middle of the subarray (arr[mid]) is equal to key, it means that the key has been found at that position. In this case, the method returns mid to indicate the location of the key in the array.\nThe recursive case 1: If the element at the middle of the subarray (arr[mid]) is greater than key, it means that the key must be in the left half of the subarray. In this case, the method makes a recursive call to optimizedLinearSearch with a smaller subarray (arr[0] to arr[mid-1]). This way, the method eliminates half of the elements from the search space and then applies optimized linear search on the remaining elements.\nThe recursive case 2: If the element at the middle of the subarray (arr[mid]) is less than key, it means that the key must be in the right half of the subarray. In this case, the method makes a recursive call to optimizedLinearSearch with a smaller subarray (arr[mid+1] to arr[right]). This way, the method eliminates half of the elements from the search space and then applies optimized linear search on the remaining elements.\n\nThe data flows through the recursive calls via the parameters and return values. The parameters (arr, left, right, and key) help in propagating information forward through the recursive calls until reaching one of the base cases. The return values (mid, -1, left, or right) relay information back from the base case to the original caller.\nLet’s illustrate this with an example: Suppose we want to search for 5 in the sorted array {1, 2, 3, 4, 5} using drasticallyFasterSearch. We start by calling drasticallyFasterSearch(arr, 0, 4, 5), where arr is {1, 2, 3, 4, 5}, left is 0, right is 4, and key is 5. The recursive tree will look like this:\ndrasticallyFasterSearch(arr, 0, 4, 5)\n|____ drasticallyFasterSearch(arr, 3, 4, 5)\n      |____ optimizedLinearSearch(arr[3..4], 0 ,1 ,5) =&gt; returns 1\nAt each recursive call, we check if we have reached one of the base cases. If not, we make another recursive call with a smaller subarray. When we reach optimizedLinearSearch(arr[3..4], 0 ,1 ,5), we find that arr[4] is equal to key, so we return 1. This value is then added to left (which is 3) and passed back up the call stack until it reaches the original caller. So, drasticallyFasterSearch(arr, 0 ,4 ,5) returns (3 + 1) = 4, indicating that 5 is found at index 4 in the array.",
    "crumbs": [
      "Recursion",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Binary Search</span>"
    ]
  },
  {
    "objectID": "19_binary_search.html#even-faster-drasticallyfastersearch",
    "href": "19_binary_search.html#even-faster-drasticallyfastersearch",
    "title": "23  Binary Search",
    "section": "23.4 Even faster drasticallyFasterSearch",
    "text": "23.4 Even faster drasticallyFasterSearch\nBuilding on our previous exploration, we can see a pattern emerging. We are using our drasticallyFasterSearch to divide our problem, but when it comes to conquering, we are falling back to optimizedLinearSearch. This means we are not fully utilizing the strength of our divide and conquer approach. We have an opportunity for optimization here: what if, instead of resorting to linear search, we applied the same divide and conquer strategy again? This observation leads us to the Binary Search algorithm, a more efficient way to search sorted arrays, which would look like this:\npublic int drasticallyFasterSearch(int[] arr, int left, int right, int key) {\n    int mid = left + (right - left) / 2;\n    if (arr[mid] == key)\n        return mid;\n    else if (arr[mid] &gt; key)\n        return drasticallyFasterSearch(\n          java.utils.Arrays.copyOfRange(arr, 0, mid - 1), 0, mid - 1, key\n        );\n    else\n        return drasticallyFasterSearch(\n          java.utils.Arrays.copyOfRange(arr, mid + 1, right), 0, right - mid - 1, key\n        );\n}\nBut would this actually work? We need to use what we learnt about writing recursive functions to find out - what are the base cases? What are the recursive cases? How does the data flow through the recursive calls?\n\nThe analysis of the drasticallyFasterSearch method is left as an exercise for the reader.\n\nOne fatal flaw with the drasticallyFasterSearch function, as it is currently written, is that there is only one base case - when we find the element and return the mid index. If the element we’re searching for is not in the array, the function will enter into an infinite loop because it lacks a base case to handle this situation.\nSo, we need an additional base case to handle the scenario where the element is not in the array. This needs to be a condition that detects if we’ve run out of elements in the array that we haven’t yet searched.\nThis is where the skill of tracing recursive functions comes in handy. We can trace the function to see what happens when we run out of elements to search. Let’s trace the function for the example we used earlier: searching for 5 in the sorted array {1, 2, 3, 4, 5} results in the following recursive call sequence:\ndrasticallyFasterSearch(arr, 0, 4, 5)\n|____ drasticallyFasterSearch(arr, 3, 4, 5)\n      |____ drasticallyFasterSearch(arr, 4, 4, 5) \n            =&gt; returns 4\nBut what happens if the number we’re searching for isn’t in the array? What if we’re looking for 6 in the same array?\ndrasticallyFasterSearch(arr, 0, 4, 6)\n|____ drasticallyFasterSearch(arr, 3, 4, 6)\n      |____ drasticallyFasterSearch(arr, 4, 4, 6)\n            |____ drasticallyFasterSearch(arr, 5, 4, 6)\nThis last recursive call, drasticallyFasterSearch(arr, 5, 4, 6) is problematic because it makes no sense to start at an index higher than our end index. This means we’ve exhausted all potential elements in our search.\nThis suggests that we need an additional base case to handle when our left index exceeds our right index. This makes sense because there is no reason for left to be on the right of right! So, we should modify our search method to be:\npublic int drasticallyFasterSearch(int[] arr, int left, int right, int key) {\n    if (right &lt; left)\n        return -1;\n\n    int mid = left + (right - left) / 2;\n\n    if (arr[mid] == key)\n        return mid;\n    else if (arr[mid] &gt; key)\n        return drasticallyFasterSearch(\n          java.utils.Arrays.copyOfRange(arr, 0, mid - 1), 0, mid - 1, key\n        );\n    else\n        return drasticallyFasterSearch(\n          java.utils.Arrays.copyOfRange(arr, mid + 1, right), 0, right - mid - 1, key\n        );\n}\nNow, we can see that when right becomes less than left, the function will correctly return -1, signaling that our intended key isn’t present. Note that in the case where left equals right, we still want to check the arr[mid], so right &lt; left, not right &lt;= left, is our base case.\nAs we apply the divide and conquer methodology here, our drasticallyFasterSearch method is now a working implementation of the Binary Search algorithm. In each recursive call, we cut our search space in half, drastically reducing the number of elements searched. For a list of length n, in the worst-case, we’ll make roughly log(n) recursive calls, scaling our runtime far better than a linear search would.\nIndeed, this is a powerful tool for searching sorted data. By fully utilizing divide and conquer, we’ve been able to drastically speed up our search. This emphasizes the importance of understanding the fundamentals of recursion and problem decompositions as key building blocks in algorithm and data structure design.\nAs the reader (or student!) progresses, they may find more elegant ways to implement this algorithm, or variations of it, which solve complex problems in different contexts. Here is a variation of the this dearch algorithm that is more efficient and doesn’t require copying the array at each recursive call:\npublic int drasticallyFasterSearch(int[] arr, int left, int right, int key) {\n    if (right &gt;= left) {\n        int mid = left + (right - left) / 2;\n        if (arr[mid] == key)\n            return mid;\n        else if (arr[mid] &gt; key)\n            return drasticallyFasterSearch(arr, left, mid - 1, key);\n        else\n            return drasticallyFasterSearch(arr, mid + 1, right, key);\n    } else {\n        return -1;\n    }\n}",
    "crumbs": [
      "Recursion",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Binary Search</span>"
    ]
  },
  {
    "objectID": "19_binary_search.html#unveiling-binary-search",
    "href": "19_binary_search.html#unveiling-binary-search",
    "title": "23  Binary Search",
    "section": "23.5 Unveiling Binary Search",
    "text": "23.5 Unveiling Binary Search\nThe name “Binary Search” comes from the fact that this method divides the search space into two (binary) at each step of the algorithm. It is a fundamental technique used in computer science and a cornerstone in algorithm design.\nThe final implementation of the Binary Search algorithm is as follows:\npublic int binarySearch(int[] arr, int left, int right, int key) {\n    if (right &gt;= left) {\n        int mid = left + (right - left) / 2;\n        if (arr[mid] == key)\n            return mid;\n        else if (arr[mid] &gt; key)\n            return binarySearch(arr, left, mid - 1, key);\n        else\n            return binarySearch(arr, mid + 1, right, key);\n    } else {\n        return -1;\n    }\n}\nBy rethinking our approach and exploiting the sorted nature of our data, we devised a strategy that significantly reduces the time spent searching. This concludes the core of Binary Search, demonstrating how powerful a shift in perspective can be in problem-solving.",
    "crumbs": [
      "Recursion",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Binary Search</span>"
    ]
  },
  {
    "objectID": "20_trees.html",
    "href": "20_trees.html",
    "title": "24  Tree Data Structure",
    "section": "",
    "text": "24.1 Background\nWhen we first begin to learn about data structures, we often encounter linear structures such as arrays, linked lists, stacks, and queues. These structures, as the term ‘linear’ suggests, store data in a sequential manner (Figure 24.1). This makes them great for situations where the data can be naturally expressed as a sequence, for instance, a to-do list or a queue at a coffee shop.\nG\n\n\n\n1\n\n1\n\n\n\n2\n\n2\n\n\n\n1-&gt;2\n\n\n\n\n\n3\n\n3\n\n\n\n2-&gt;3\n\n\n\n\n\n4\n\n4\n\n\n\n3-&gt;4\n\n\n\n\n\n5\n\n5\n\n\n\n4-&gt;5\n\n\n\n\n\n\n\n\nFigure 24.1: A representation of a linked list structure, highlighting its linear nature.\nLet’s consider the linked list. Each node in the linked list has a reference to the next node, creating a chain of elements. This works well when the data in the structure has a natural sequential or linear relationship. However, as we delve deeper into more complex data problems, we begin to see that not all data fits neatly into a linear relationship.\nConsider an organizational hierarchy within a company. Each person (except the CEO) reports to exactly one person. However, each manager can have multiple direct reports. In this case, each node (person) might need to reference more than one node (Figure 24.2). Using a linked list to represent this data would be cumbersome, as it would not capture the multiple relationships that exist between a manager and their direct reports.\nG\n\n\n\nCEO\n\nCEO\n\n\n\nManager1\n\nManager1\n\n\n\nCEO-&gt;Manager1\n\n\n\n\n\nManager2\n\nManager2\n\n\n\nCEO-&gt;Manager2\n\n\n\n\n\nEmployee1\n\nEmployee1\n\n\n\nManager1-&gt;Employee1\n\n\n\n\n\nEmployee2\n\nEmployee2\n\n\n\nManager1-&gt;Employee2\n\n\n\n\n\nEmployee3\n\nEmployee3\n\n\n\nManager2-&gt;Employee3\n\n\n\n\n\nEmployee4\n\nEmployee4\n\n\n\nManager2-&gt;Employee4\n\n\n\n\n\n\n\n\nFigure 24.2: An example of an organizational hierarchy within a company, illustrating the non-linear relationships that exist.\nAnother example is a file system on your computer. Each folder can contain multiple files or other folders. But, each file or folder is contained within exactly one other folder. Once again, we see the need for a data structure that can capture these multiple relationships (Figure 24.3).\nG\n\n\n\nRoot\n\nRoot\n\n\n\nFolder1\n\nFolder1\n\n\n\nRoot-&gt;Folder1\n\n\n\n\n\nFile1\n\nFile1\n\n\n\nRoot-&gt;File1\n\n\n\n\n\nSubfolder1\n\nSubfolder1\n\n\n\nFolder1-&gt;Subfolder1\n\n\n\n\n\nFile2\n\nFile2\n\n\n\nFolder1-&gt;File2\n\n\n\n\n\nFile3\n\nFile3\n\n\n\nSubfolder1-&gt;File3\n\n\n\n\n\nFile4\n\nFile4\n\n\n\nSubfolder1-&gt;File4\n\n\n\n\n\n\n\n\nFigure 24.3: A representation of a simple file system, showing how folders can contain multiple other folders or files.\nThese examples bring to light a common pattern: there are situations where our data is not linear, but hierarchical. Hierarchies exist in various forms in the real world, from biological classifications to the layout of web pages, and modeling these hierarchies accurately in our programs allows us to reflect the reality more truthfully.\nEnter the concept of Trees.\nA tree in computer science is a hierarchical data structure that can model these kinds of relationships (Figure 24.4). This structure gets its name from a real-world tree, but with a twist - the tree data structure is often visualized with the root at the top and the leaves at the bottom, resembling an upside-down real-world tree.\nG\n\n\n\nRoot\n\nRoot\n\n\n\nChild1\n\nChild1\n\n\n\nRoot-&gt;Child1\n\n\n\n\n\nChild2\n\nChild2\n\n\n\nRoot-&gt;Child2\n\n\n\n\n\nChild3\n\nChild3\n\n\n\nRoot-&gt;Child3\n\n\n\n\n\nGrandchild1\n\nGrandchild1\n\n\n\nChild1-&gt;Grandchild1\n\n\n\n\n\nGrandchild2\n\nGrandchild2\n\n\n\nChild1-&gt;Grandchild2\n\n\n\n\n\nGrandchild3\n\nGrandchild3\n\n\n\nChild3-&gt;Grandchild3\n\n\n\n\n\n\n\n\nFigure 24.4: An illustration of a simple tree data structure, with key components labeled.\nThe tree structure is a fundamental shift from our previously understood data structures. It can open new doors to solving problems and provide an efficient way to organize and model complex relationships in our data.\nIn the following sections, we’ll take a deeper dive into the world of trees, understanding their properties, variations, and how they can revolutionize the way we think about hierarchical data relationships.",
    "crumbs": [
      "Trees and Graphs",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Tree Data Structure</span>"
    ]
  },
  {
    "objectID": "20_trees.html#introduction-to-trees",
    "href": "20_trees.html#introduction-to-trees",
    "title": "24  Tree Data Structure",
    "section": "24.2 Introduction to Trees",
    "text": "24.2 Introduction to Trees\nAfter establishing the need for a data structure that can handle more complex relationships, we can now formally introduce the concept of trees.\nA tree in computer science is an abstract data type that simulates a hierarchical structure with a set of interconnected nodes. The key characteristic that separates trees from other data structures is that no node in the tree may be connected to more than one parent node, thereby preventing any possibility of a loop in the data structure.\nLet’s get familiar with some fundamental tree terminologies (see Figure 24.5):\n\nNode: A unit or location in the tree where data is stored. It’s similar to a link in a linked list or an element in an array.\nRoot: The topmost node in the tree that doesn’t have a parent.\nEdge: The connection between two nodes.\nParent: A node which has one or more child nodes.\nChild: A node which has a parent node.\nLeaf: A node which does not have any child nodes.\nSubtree: A tree consisting of a node and its descendants.\nDegree: The number of children of a node.\nLevel: The distance from the root, measured in edges.\nHeight: The distance from the node to its furthest leaf.\nDepth: The distance from the node to the root.\n\n\n\n\n\n\n\n\n\nG\n\n\n\nA\n\nRoot (A)\nNode\n\n\n\nB\n\nChild (B)\nNode\nLevel: 1\nDepth: 1\nParent of: F, G\n\n\n\nA-&gt;B\n\n\n\n\n\nC\n\nChild (C)\nNode\nDegree: 2\n Parent of: D, E\n\n\n\nA-&gt;C\n\n\n\n\n\nF\n\nLeaf (F)\nNode\nDepth: 2\n\n\n\nB-&gt;F\n\n\n\n\n\nG\n\nLeaf (G)\nNode\nDepth: 2\n\n\n\nB-&gt;G\n\n\n\n\n\nD\n\nLeaf (D)\nNode\nDepth: 2\n\n\n\nC-&gt;D\n\n\n\n\n\nE\n\nLeaf (E)\nNode\nDepth: 2\n\n\n\nC-&gt;E\n\n\n\n\n\n\n\n\nFigure 24.5: A simple tree illustrating various tree terminologies. Node A is the root, nodes B and C are children of A. Node C, having two children, demonstrates the degree of a node. Nodes D, E, F, G are leaf nodes. The subtree consists of Node C and its descendants (D, E). The level is demonstrated by the level of Node B (Level 1). The depth of Node D is 2 (from Root A). The height of the tree is 2 (distance from the Root A to furthest leaf).\n\n\n\n\n\nThe concept of trees becomes truly interesting when we delve into the various types of trees. Different types of trees can be used to solve different types of problems, and it’s crucial to choose the right type for a given problem.\nThe simplest form of a tree is a General Tree, where each node can have any number of child nodes. An example of where a general tree might be useful is in the representation of a file system on a computer. In this system, each folder can contain any number of files or folders. We can illustrate this with a diagram (Figure 24.6).\n\n\n\n\n\n\n\n\nG\n\n\n\nRoot\n\nRoot\n\n\n\nFolder1\n\nFolder1\n\n\n\nRoot-&gt;Folder1\n\n\n\n\n\nFile1\n\nFile1\n\n\n\nRoot-&gt;File1\n\n\n\n\n\nSubfolder1\n\nSubfolder1\n\n\n\nFolder1-&gt;Subfolder1\n\n\n\n\n\nFile2\n\nFile2\n\n\n\nFolder1-&gt;File2\n\n\n\n\n\nFile3\n\nFile3\n\n\n\nSubfolder1-&gt;File3\n\n\n\n\n\nFile4\n\nFile4\n\n\n\nSubfolder1-&gt;File4\n\n\n\n\n\nFile5\n\nFile5\n\n\n\nSubfolder1-&gt;File5\n\n\n\n\n\n\n\n\nFigure 24.6: An example of a general tree representing a file system, where each node can have any number of child nodes.\n\n\n\n\n\nHowever, there are also more specialized types of trees, like Binary Trees, where each node can have at most two children, commonly referred to as left and right child. The binary tree can be visualized in the form of a family tree where each parent (node) can have at most two children (Figure 24.7).\n\n\n\n\n\n\n\n\nG\n\n\n\nParent\n\nParent\n\n\n\nChild1\n\nChild1\n\n\n\nParent-&gt;Child1\n\n\n\n\n\nChild2\n\nChild2\n\n\n\nParent-&gt;Child2\n\n\n\n\n\n\n\n\nFigure 24.7: A binary tree representing a family tree, where each parent can have at most two children.\n\n\n\n\n\nA further specialization is the Binary Search Tree (BST), where the nodes are arranged in a manner such that for every node, nodes in the left subtree have values less than the node, and nodes in the right subtree have values greater than the node. This property makes BSTs useful for search operations, such as finding a book in a library system, where books are organized based on their identifying information (Figure 24.8).\n\n\n\n\n\n\n\n\nG\n\n\n\n50\n\n50\n\n\n\n30\n\n30\n\n\n\n50-&gt;30\n\n\n\n\n\n70\n\n70\n\n\n\n50-&gt;70\n\n\n\n\n\n20\n\n20\n\n\n\n30-&gt;20\n\n\n\n\n\n40\n\n40\n\n\n\n30-&gt;40\n\n\n\n\n\n60\n\n60\n\n\n\n70-&gt;60\n\n\n\n\n\n80\n\n80\n\n\n\n70-&gt;80\n\n\n\n\n\n\n\n\nFigure 24.8: A binary search tree (BST) used in a library system for organizing books. The key property of BST is that nodes in the left subtree have values less than the parent node, and nodes in the right subtree have values greater.\n\n\n\n\n\nThere are also self-balancing trees like AVL Trees and Red-Black Trees that maintain their balance as new nodes are inserted or existing nodes are removed, which ensures that the tree remains efficient for operations like insertion, deletion, and search.\nAnother common type of tree is the B-tree, used in databases and filesystems. In a B-tree, each node can have more than two children, unlike in a binary tree. This structure allows for efficient access and modification of large blocks of data, which makes B-trees particularly suitable for use in disk-based storage systems, such as databases (Figure 24.9).\n\n\n\n\n\n\n\n\nG\n\n\n\nNode1\n\nNode1\n\n\n\nNode2\n\nNode2\n\n\n\nNode1--Node2\n\n\n\n\nNode3\n\nNode3\n\n\n\nNode1--Node3\n\n\n\n\nNode4\n\nNode4\n\n\n\nNode1--Node4\n\n\n\n\nLeaf1\n\nLeaf1\n\n\n\nNode2--Leaf1\n\n\n\n\nLeaf2\n\nLeaf2\n\n\n\nNode2--Leaf2\n\n\n\n\nLeaf3\n\nLeaf3\n\n\n\nNode3--Leaf3\n\n\n\n\nLeaf4\n\nLeaf4\n\n\n\nNode3--Leaf4\n\n\n\n\nLeaf5\n\nLeaf5\n\n\n\nNode3--Leaf5\n\n\n\n\nLeaf6\n\nLeaf6\n\n\n\nNode4--Leaf6\n\n\n\n\nLeaf7\n\nLeaf7\n\n\n\nNode4--Leaf7\n\n\n\n\n\n\n\nFigure 24.9: A representation of a B-tree used in a database. Each node can have more than two children, providing efficient access and modification of large data blocks.\n\n\n\n\n\nUnderstanding these types of trees and how they work is crucial to using them effectively. In the next sections, we will delve deeper into some of these tree types and explore their properties and uses.",
    "crumbs": [
      "Trees and Graphs",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Tree Data Structure</span>"
    ]
  },
  {
    "objectID": "20_trees.html#binary-tree",
    "href": "20_trees.html#binary-tree",
    "title": "24  Tree Data Structure",
    "section": "24.3 Binary Tree",
    "text": "24.3 Binary Tree\nAfter understanding the basic concept of trees, let’s now focus on a special kind of tree, the Binary Tree. As we have mentioned earlier, in a binary tree, a node can have at most two children - commonly referred to as the left child and the right child. This property leads to some interesting and useful characteristics that we’ll explore in this section.\nThe structure of a binary tree is relatively straightforward. Each node in a binary tree contains some data, a reference to the left child node, and a reference to the right child node. If a node has no left or right child, the corresponding reference is set to null.\nIn a binary tree, each level has twice as many nodes as the previous level, creating a rapidly expanding structure. This property enables binary trees to store large amounts of data in a relatively small number of levels, which can lead to efficient search and insertion operations, given the tree is balanced.\nLet’s take a look at some different types of binary trees:\n\nFull Binary Tree: In this tree, every node has either 0 or 2 children. This type of tree is useful when you know that every internal node in your tree will have exactly two children. For example, in certain types of decision trees used in machine learning, each decision leads to two subsequent decisions, forming a full binary tree.\n\n\n\n\n\n\n\n\n\nG\n\n\n\nA\n\nA\n\n\n\nB\n\nB\n\n\n\nA-&gt;B\n\n\n\n\n\nC\n\nC\n\n\n\nA-&gt;C\n\n\n\n\n\nD\n\nD\n\n\n\nB-&gt;D\n\n\n\n\n\nE\n\nE\n\n\n\nB-&gt;E\n\n\n\n\n\n\n\n\nFigure 24.10: A Full Binary Tree. Every node has either 0 or 2 children.\n\n\n\n\n\n\nComplete Binary Tree: Every level, except possibly the last, is fully filled, and all nodes are as far left as possible. Heaps are an example of complete binary trees. In a heap, all levels are fully filled except for the last level, which is filled from left to right.\n\n\n\n\n\n\n\n\n\nG\n\n\n\nA\n\nA\n\n\n\nB\n\nB\n\n\n\nA-&gt;B\n\n\n\n\n\nC\n\nC\n\n\n\nA-&gt;C\n\n\n\n\n\nD\n\nD\n\n\n\nB-&gt;D\n\n\n\n\n\nE\n\nE\n\n\n\nB-&gt;E\n\n\n\n\n\nF\n\nF\n\n\n\nC-&gt;F\n\n\n\n\n\n\n\n\nFigure 24.11: A Complete Binary Tree. Every level is fully filled except for the last, which is filled from left to right.\n\n\n\n\n\n\nPerfect Binary Tree: A perfect binary tree is both full and complete. Every level is fully filled. This kind of tree appears in some specialized data structures or algorithms, but is not common in general use.\n\n\n\n\n\n\n\n\n\nG\n\n\n\nA\n\nA\n\n\n\nB\n\nB\n\n\n\nA-&gt;B\n\n\n\n\n\nC\n\nC\n\n\n\nA-&gt;C\n\n\n\n\n\nD\n\nD\n\n\n\nB-&gt;D\n\n\n\n\n\nE\n\nE\n\n\n\nB-&gt;E\n\n\n\n\n\nF\n\nF\n\n\n\nC-&gt;F\n\n\n\n\n\nG\n\nG\n\n\n\nC-&gt;G\n\n\n\n\n\n\n\n\nFigure 24.12: A Perfect Binary Tree. Every level is fully filled.\n\n\n\n\n\n\nBalanced Binary Tree: The difference in height of left and right subtrees for every node is not more than k (mostly 1). Most of the tree algorithms require the tree to be balanced to maintain their optimal time complexity.\n\n\n\n\n\n\n\n\n\nG\n\n\n\nA\n\nA\n\n\n\nB\n\nB\n\n\n\nA-&gt;B\n\n\n\n\n\nC\n\nC\n\n\n\nA-&gt;C\n\n\n\n\n\nD\n\nD\n\n\n\nB-&gt;D\n\n\n\n\n\nE\n\nE\n\n\n\nB-&gt;E\n\n\n\n\n\nF\n\nF\n\n\n\nC-&gt;F\n\n\n\n\n\n\n\n\nFigure 24.13: A Balanced Binary Tree. The difference in height of left and right subtrees for every node is not more than 1.\n\n\n\n\n\n\nDegenerate (or pathological) Tree: Each parent node has only one child. The tree behaves like a linked list, and we lose the advantages of binary trees.\n\n\n\n\n\n\n\n\n\nG\n\n\n\nA\n\nA\n\n\n\nB\n\nB\n\n\n\nA-&gt;B\n\n\n\n\n\nC\n\nC\n\n\n\nB-&gt;C\n\n\n\n\n\nD\n\nD\n\n\n\nC-&gt;D\n\n\n\n\n\nE\n\nE\n\n\n\nD-&gt;E\n\n\n\n\n\n\n\n\nFigure 24.14: A Degenerate (or pathological) Tree. Each parent node has only one child.\n\n\n\n\n\n\n24.3.1 Implementing a Binary Tree\nIn this section, we will be implementing a simple Binary Tree structure using Java. We’ll start with the definition of the Node class.\n/**\n * This class represents a node in the binary tree.\n * @param &lt;T&gt; This is the type parameter which represents the type of value the node will hold.\n */\npublic class Node&lt;T&gt; {\n    T value;\n    Node&lt;T&gt; left;\n    Node&lt;T&gt; right;\n\n    /**\n     * Node class constructor.\n     * @param value This is the value that the node will hold.\n     */\n    public Node(T value) {\n        this.value = value;\n        this.left = null;\n        this.right = null;\n    }\n}\nIn the code above, we defined a generic Node class with a type parameter T. The Node class has three fields: value of type T representing the data the node holds, and left and right of type Node&lt;T&gt; representing the left and right children of the node.\nNext, let’s define our BinaryTree class.\n/**\n * This class represents a binary tree.\n * @param &lt;T&gt; This is the type parameter which represents the type of value the nodes in the tree will hold.\n */\npublic class BinaryTree&lt;T&gt; {\n    Node&lt;T&gt; root;\n\n    /**\n     * BinaryTree class constructor.\n     */\n    public BinaryTree() {\n        this.root = null;\n    }\n}\nIn the BinaryTree class, we defined a root of type Node&lt;T&gt; representing the root of the tree. Practical applications of binary trees usually include additional operations like adding a node, deleting a node, checking if a value exists in the tree, and various ways of traversing the tree (in-order, pre-order, post-order).",
    "crumbs": [
      "Trees and Graphs",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Tree Data Structure</span>"
    ]
  },
  {
    "objectID": "20_trees.html#common-operations-on-binary-trees",
    "href": "20_trees.html#common-operations-on-binary-trees",
    "title": "24  Tree Data Structure",
    "section": "24.4 Common Operations on Binary Trees",
    "text": "24.4 Common Operations on Binary Trees\n\n24.4.1 Traversals and Orderings\nTrees, being a nonlinear data structure, cannot be traversed in a single run like arrays, linked lists, or other linear data structures. Therefore, they require some specific methods to traverse their nodes, visit each node once, and perform some operations such as search, update, or accumulate.\nThree common types of depth-first traversal are in-order, pre-order, and post-order.\n\n24.4.1.1 In-Order Traversal\nIn in-order traversal, the process is as follows: 1. Traverse the left subtree 2. Visit the root node 3. Traverse the right subtree\nThis traversal technique is widely used, and in binary search trees, it retrieves data in ascending order.\n\n\n\n\n\n\n\n\nG\n\n\n\nA\n\n2\nA\n\n\n\nB\n\n1\nB\n\n\n\nA-&gt;B\n\n\n\n\n\nC\n\n3\nC\n\n\n\nA-&gt;C\n\n\n\n\n\n\n\n\nFigure 24.15: In-Order Traversal: Traverse left subtree, Visit root, Traverse right subtree. The numbers indicate the order of traversal.\n\n\n\n\n\nPseudocode for in-order traversal can look like this:\nfunction inOrderTraversal(node) {\n  if (node != null) {\n    inOrderTraversal(node.left)\n    visit(node)\n    inOrderTraversal(node.right)\n  }\n}\n\n\n24.4.1.2 Pre-Order Traversal\nIn pre-order traversal, the process is as follows: 1. Visit the root node 2. Traverse the left subtree 3. Traverse the right subtree\nThis method can be used to create a copy of the tree, or to get a prefix expression of an expression tree.\n\n\n\n\n\n\n\n\nG\n\n\n\nA\n\n1\nA\n\n\n\nB\n\n2\nB\n\n\n\nA-&gt;B\n\n\n\n\n\nC\n\n3\nC\n\n\n\nA-&gt;C\n\n\n\n\n\n\n\n\nFigure 24.16: Pre-Order Traversal: Visit root, Traverse left subtree, Traverse right subtree. The numbers indicate the order of traversal.\n\n\n\n\n\nPseudocode for pre-order traversal can look like this:\nfunction preOrderTraversal(node) {\n  if (node != null) {\n    visit(node)\n    preOrderTraversal(node.left)\n    preOrderTraversal(node.right)\n  }\n}\n\n\n24.4.1.3 Post-Order Traversal\nIn post-order traversal, the process is as follows: 1. Traverse the left subtree 2. Traverse the right subtree 3. Visit the root node\nThis method is used to delete or deallocate nodes of a tree, or to get a postfix expression of an expression tree.\n\n\n\n\n\n\n\n\nG\n\n\n\nA\n\n3\nA\n\n\n\nB\n\n1\nB\n\n\n\nA-&gt;B\n\n\n\n\n\nC\n\n2\nC\n\n\n\nA-&gt;C\n\n\n\n\n\n\n\n\nFigure 24.17: Post-Order Traversal: Traverse left subtree, Traverse right subtree, Visit root. The numbers indicate the order of traversal.\n\n\n\n\n\nPseudocode for post-order traversal can look like this:\nfunction postOrderTraversal(node) {\n  if (node != null) {\n    postOrderTraversal(node.left)\n    postOrderTraversal(node.right)\n    visit(node)\n  }\n}\nIn each of the traversal methods above, visit(node) is an operation that performs some computation on node, such as printing its value.\nIt’s important to note that traversal operations have a time complexity of O(n), where n is the number of nodes in the tree. This is because every node must be visited once and only once during the traversal.\nIn addition to traversal methods, other common operations on binary trees include insertion, deletion, and searching. These are vital operations for maintaining and interacting with the binary tree structure.",
    "crumbs": [
      "Trees and Graphs",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Tree Data Structure</span>"
    ]
  },
  {
    "objectID": "20_trees.html#binary-search-trees-bst",
    "href": "20_trees.html#binary-search-trees-bst",
    "title": "24  Tree Data Structure",
    "section": "24.5 Binary Search Trees (BST)",
    "text": "24.5 Binary Search Trees (BST)\nBefore delving into Binary Search Trees (BSTs), let’s take a step back and contemplate the significance of search operations in computer science. A search operation is fundamental to many applications, from looking up a contact in your smartphone to searching for a specific book in a library database. Therefore, making search operations efficient has always been a central challenge in computer science.\nTraditional search approaches like linear search, which checks each item one by one, can be time-consuming especially when dealing with large datasets, as it has a time complexity of O(n). A considerable improvement is the binary search algorithm, which works by repeatedly dividing the sorted list of elements in half until the target element is found, giving it a time complexity of O(log n). However, binary search requires the data to be sorted, and maintaining a sorted list of elements can be expensive for insert and delete operations.\nThis is where Binary Search Trees come in. A Binary Search Tree is a special type of binary tree that maintains a specific ordering property — for each node, all elements in the left subtree are less than the node, and all elements in the right subtree are greater than the node (see Figure 24.18). This property of BSTs enables us to perform search, insert, and delete operations efficiently, maintaining a time complexity of O(log n) in an ideal situation where the tree is balanced. Let’s explore these concepts in more detail.\n\n24.5.1 Definition, Properties, and Structure of BSTs\nA Binary Search Tree (BST) is a binary tree where for every node:\n\nThe values in its left subtree are less than the node’s value.\nThe values in its right subtree are greater than the node’s value.\n\n\n\n\n\n\n\n\n\nG\n\n\n\n5\n\n5\n\n\n\n3\n\n3\n\n\n\n5-&gt;3\n\n\n\n\n\n7\n\n7\n\n\n\n5-&gt;7\n\n\n\n\n\n2\n\n2\n\n\n\n3-&gt;2\n\n\n\n\n\n4\n\n4\n\n\n\n3-&gt;4\n\n\n\n\n\n6\n\n6\n\n\n\n7-&gt;6\n\n\n\n\n\n8\n\n8\n\n\n\n7-&gt;8\n\n\n\n\n\n\n\n\nFigure 24.18: The Binary Search Tree property: for every node, all elements in the left subtree are less than the node, and all elements in the right subtree are greater than the node.\n\n\n\n\n\nThis property ensures that the “in-order” traversal of a BST results in a sorted sequence of values, which is the key to efficient search, insertion, and deletion operations.\n\n\n24.5.2 Advantages of BSTs over other tree structures\nThe BST property provides some advantages over other tree structures:\n\nEfficient search: If the tree is balanced, we can find, insert, or delete entries in O(log n) time where n is the total number of nodes. This is significantly faster than linear search, and it doesn’t require the maintenance of a sorted list like binary search.\nSorted traversal: In-order traversal of a BST yields the nodes in sorted order, which can be useful in various applications.\nFlexibility: Unlike an array, BSTs do not have a fixed size. They can grow and shrink during the execution of the program, allowing efficient use of memory.\n\n\n\n24.5.3 Detailed description of BST operations: insertion, deletion, searching\nBSTs support three fundamental operations: search, insertion, and deletion, all of which leverage the BST property to operate efficiently.\nSearch: Starting from the root, if the target value is less than the current node, we move to the left child; if it’s greater, we move to the right child. We repeat this process until we either find the target value or reach a null where the target value should be if it were in the tree (see Figure 24.19).\n\n\n\n\n\n\n\n\nG\n\n\n\n5\n\n5\n\n\n\n3\n\n3\n\n\n\n5-&gt;3\n\n\n\n\n\n7\n\n7\n\n\n\n5-&gt;7\n\n\n\n\n\n2\n\n2\n\n\n\n3-&gt;2\n\n\n\n\n\n4\n\n4\n\n\n\n3-&gt;4\n\n\n\n\n\n6\n\n6\n\n\n\n7-&gt;6\n\n\n\n\n\n8\n\n8\n\n\n\n7-&gt;8\n\n\n\n\n\n\n\n\nFigure 24.19: BST search operation: Starting from the root, the algorithm compares the target value with the current node and moves left or right based on the comparison. The search ends when the target is found or when a null is encountered.\n\n\n\n\n\nInsertion: Similar to search, but when we reach a null location, we create a new node with the target value at that spot (see Figure 24.20).\n\n\n\n\n\n\n\n\nG\n\n\n\n5\n\n5\n\n\n\n3\n\n3\n\n\n\n5-&gt;3\n\n\n\n\n\n7\n\n7\n\n\n\n5-&gt;7\n\n\n\n\n\n2\n\n2\n\n\n\n3-&gt;2\n\n\n\n\n\n4\n\n4\n\n\n\n3-&gt;4\n\n\n\n\n\n6\n\n6\n\n\n\n7-&gt;6\n\n\n\n\n\n8\n\n8\n\n\n\n7-&gt;8\n\n\n\n\n\n1\n\n1\n\n\n\n2-&gt;1\n\n\n\n\n\n\n\n\nFigure 24.20: BST insertion operation: Similar to the search operation, the algorithm moves down the tree starting from the root. When a null location is encountered, a new node is created at that spot.\n\n\n\n\n\nDeletion: A bit more complicated as we need to maintain the BST property after removing a node. The process varies based on the number of children the node to be deleted has\n, similar to deletion in a binary tree (see Figure 24.21).\n\n\n\n\n\n\n\n\nG\n\n\n\n5\n\n5\n\n\n\n3\n\n3\n\n\n\n5-&gt;3\n\n\n\n\n\n7\n\n7\n\n\n\n5-&gt;7\n\n\n\n\n\n2\n\n2\n\n\n\n3-&gt;2\n\n\n\n\n\n4\n\n4\n\n\n\n3-&gt;4\n\n\n\n\n\n6\n\n6\n\n\n\n7-&gt;6\n\n\n\n\n\n8\n\n8\n\n\n\n7-&gt;8\n\n\n\n\n\n1\n\n1\n\n\n\n2-&gt;1\n\n\n\n\n\n\n\n\nFigure 24.21: BST deletion operation: Deletion in a BST can be complex as it must maintain the BST property after removing a node. The process varies depending on whether the node to be deleted has no children, one child, or two children.\n\n\n\n\n\nAbsolutely. Let’s break down the implementation of BST operations into both recursive and iterative approaches using Java.\n\n\n24.5.4 Recursive and Iterative Implementations in Java\nWhile recursion offers a straightforward and elegant approach to implementing BST operations, the iterative method can often provide better performance in terms of space complexity. Let’s look at both methods for each of the main BST operations: search, insertion, and deletion.\n\n\n24.5.5 Search Operation\nThe objective of the search operation in a binary search tree (BST) is to find a node that contains a specified value. If the value is present in the BST, the function will return that node. Otherwise, it will return null.\nTo find a node in a BST, we can use one of two main approaches: recursion or iteration. Both have their advantages and disadvantages, which will depend on the situation at hand.\n\n24.5.5.1 Recursive Approach\nConsider the following Java code that uses a recursive method to search a node in a BST.\npublic TreeNode search(TreeNode node, int value) {\n    if (node == null || node.val == value)\n        return node;\n\n    if (node.val &gt; value)\n        return search(node.left, value);\n    \n    return search(node.right, value);\n}\nThe method search receives a TreeNode node (that represents a current node in the BST) and an integer value (that is the value to be searched).\nThe search operation begins by checking whether the node is null (which indicates that we’ve traversed a path in the tree where the value does not exist) or the value of the current node is equal to the desired value. If either condition is true, the node is returned.\nIf the value to be found is less than the current node’s value, the function recursively calls itself, continuing the search on the left subtree (node.left).\nConversely, if the value is greater than the current node’s value, the function calls itself to continue the search on the right subtree (node.right).\n\n\n24.5.5.2 Iterative Approach\nNow, let’s look at an iterative approach to search a node in a BST. The iterative approach can be more space-efficient as it does not require stack space for recursive calls.\npublic TreeNode search(TreeNode root, int value) {\n    while (root != null) {\n        if (root.val &gt; value)\n            root = root.left;\n        else if (root.val &lt; value)\n            root = root.right;\n        else\n            return root;\n    }\n    return null;\n}\nIn the iterative approach, we begin with the root node and enter a while loop that continues as long as the current node is not null.\nIf the value of the current node (root.val) is greater than the desired value, we move to the left child of the current node (root = root.left).\nIf the value of the current node is less than the desired value, we move to the right child (root = root.right).\nThe loop breaks and the current node is returned if the value of the current node is equal to the desired value.\nIf the value is not found in the tree, the function will return null.\n\n\n24.5.5.3 Visualizing the Search Operation\nWe’ll use a simple binary search tree with the values 2, 1, and 3, with 2 being the root node, 1 the left child, and 3 the right child. We’ll search for the value 3.\n\n\n\n\n\n\n\n\nBST\n\n\n\n2\n\n2\n\n\n\n1\n\n1\n\n\n\n2-&gt;1\n\n\n\n\n\n3\n\n3\n\n\n\n2-&gt;3\n\n\n\n\n\n\n\n\nFigure 24.22: Illustration of the search operation process in a binary search tree, comparing recursive and iterative approaches. The recursive path is shown in blue, while the iterative path is shown in green.\n\n\n\n\n\nHere, the search operation starts at node 2 (the root node). In both recursive and iterative approaches, the search function compares the value to be found (3) with the current node’s value (2). As 3 is greater than 2, the function moves to the right child (node 3) for both recursive and iterative approaches. This search path is highlighted in blue for recursive and green for iterative.\nThe search function now finds that the value of the current node (3) is equal to the desired value (3). Therefore, the function returns this node.\nPlease note that in this simple example, the recursive and iterative paths are identical. This won’t always be the case, especially in larger trees, but the key takeaway is that both methods are following the same fundamental logic, albeit through different operational mechanics.\n\n\n\n24.5.6 Insertion Operation\nInsertion operation in a Binary Search Tree (BST) aims to add a new node with a specific value to the tree. The position of the new node is determined by the fundamental property of BST: for any given node, all nodes in its left subtree have values less than its value, and all nodes in its right subtree have values greater than its value.\n\n24.5.6.1 Recursive Approach\nLet’s start with the recursive approach to insert a node in a BST. Here is a snippet of Java code implementing this approach:\npublic TreeNode insert(TreeNode node, int value) {\n    if (node == null) {\n        return new TreeNode(value);\n    }\n\n    if (value &lt; node.val) {\n        node.left = insert(node.left, value);\n    } else if (value &gt; node.val) {\n        node.right = insert(node.right, value);\n    }\n\n    return node;\n}\nThis insert function takes a TreeNode node (representing the current node in the BST) and an integer value (the value to be inserted).\nThe function first checks if the node is null. If it is (indicating that we’ve reached an empty spot in the tree where a new node should be inserted), it creates a new TreeNode with the given value and returns it.\nIf the node is not null, it checks if the value to be inserted is less than the value of the current node. If so, it recursively calls the insert function to insert the value into the left subtree (node.left).\nIf the value to be inserted is greater than the value of the current node, it recursively calls insert to insert the value into the right subtree (node.right).\nAfter the value is inserted, it returns the (potentially modified) current node, maintaining the structure of the tree.\n\n\n24.5.6.2 Iterative Approach\nNow let’s consider an iterative approach to insert a node in a BST, which can be more space-efficient as it doesn’t require stack space for recursive calls:\npublic TreeNode insert(TreeNode root, int value) {\n    TreeNode node = new TreeNode(value);\n    if (root == null) {\n        return node;\n    }\n\n    TreeNode parent = null, current = root;\n    while (current != null) {\n        parent = current;\n        if (current.val &gt; value) {\n            current = current.left;\n        } else {\n            current = current.right;\n        }\n    }\n\n    if (parent.val &gt; value) {\n        parent.left = node;\n    } else {\n        parent.right = node;\n    }\n\n    return root;\n}\nIn the iterative insert function, we start by creating a new TreeNode node with the specified value.\nIf the BST is empty (i.e., the root is null), we return this new node as the root of the BST.\nIf the BST is not empty, we start from the root and iterate down the tree to find the correct position for the new node.\nWe maintain two pointers during the traversal: parent (representing the parent of the current node) and current (the current node in the traversal). We update parent to be current before moving current in each iteration.\nWhen the current node becomes null, we’ve found the correct spot for the new node. If the new value is less than the parent’s value, we insert the new node as the left child of the parent; otherwise, we insert it as the right child.\nThe function then returns the root of the (now modified) BST.\n\n\n24.5.6.3 Visualizing the Insertion Operation\nConsider a simple binary search tree with values 2, 1, and 3, where 2 is the root node, 1 is the left child, and 3 is the right child. Let’s try to insert a new node with the value 4 into this tree.\n\n\n\n\n\n\n\n\nBST\n\n\n\n2\n\n2\n\n\n\n1\n\n1\n\n\n\n2-&gt;1\n\n\n\n\n\n3\n\n3\n\n\n\n2-&gt;3\n\n\n\n\n\n4\n\n4\n\n\n\n3-&gt;4\n\n\n\n\n\n\n\n\nFigure 24.23: Illustration of the node insertion process in a binary search tree, showcasing both recursive and iterative approaches. The path followed by the recursive approach is shown in blue, while the path followed by the iterative approach is shown in green.\n\n\n\n\n\nIn this diagram, we start at node 2 (the root node) and want to insert a node with the value 4 (shown in red). In both the recursive and iterative methods, we start by comparing the value to be inserted (4) with the value of the current node (2). Since 4 is greater than 2, we move to the right child (node 3).\nWe then compare 4 with 3. Since 4 is greater than 3, and node 3 doesn’t have a right child, we add the new node as the right child of node 3. The paths taken by the recursive and iterative approaches are shown in blue and green respectively.\nJust like the search operation, in this simple example, the recursive and iterative paths for the insertion operation are identical. But in a larger tree, the paths may differ depending on the current state of the tree and the value to be inserted. However, both methods adhere to the same principle - the new node is added in such a way that it maintains the property of the binary search tree.\n\n\n\n24.5.7 Deletion Operation\nDeleting or removing a node from a binary tree is a slightly complex operation compared to insertion. This is because when we remove a node, we need to ensure that the remaining nodes still form a binary tree. The process differs depending on the number of children the node has:\n\nNo child: If the node is a leaf node (i.e., it does not have any children), we can directly remove the node.\n\n\n\n\n\n\n\n\n\nG\n\n\n\nA\n\nA\n\n\n\nB\n\nB\n\n\n\nB-&gt;A\n\n\nremove\n\n\n\nC\n\nC\n\n\n\nB-&gt;C\n\n\n\n\n\n\n\n\nFigure 24.24: Removing a node with no children. The node ‘C’ is simply removed from the tree.\n\n\n\n\n\n\nOne child: If the node has a single child, we can replace the node with its subtree.\n\n\n\n\n\n\n\n\n\nG\n\n\n\nA\n\nA\n\n\n\nB\n\nB\n\n\n\nB-&gt;A\n\n\nremove\n\n\n\nC\n\nC\n\n\n\nB-&gt;C\n\n\n\n\n\nD\n\nD\n\n\n\nC-&gt;D\n\n\n\n\n\n\n\n\nFigure 24.25: Removing a node with one child. The node ‘B’ is replaced by its subtree rooted at ‘C’.\n\n\n\n\n\n\nTwo children: This is the most complex case. If the node has two children, we generally seek a node from the tree to replace the node to be deleted. The replacement node is typically the in-order predecessor or the in-order successor of the node. These are the node’s immediate previous and next nodes in in-order traversal (see Figure 24.15).\n\nIn-order predecessor: This is the node with the highest value that is smaller than the value of the node to be removed. It can be found by moving one step to the left (to the left child), and then as far right as possible.\nIn-order successor: This is the node with the smallest value that is larger than the value of the node to be removed. It can be found by moving one step to the right (to the right child), and then as far left as possible.\n\n\n\n\n\n\n\n\n\n\nG\n\n\n\nA\n\nA\n\n\n\nB\n\nB\n\n\n\nB-&gt;A\n\n\n\n\n\nC\n\nC\n\n\n\nB-&gt;C\n\n\n\n\n\n\n\n\nFigure 24.26: Removing a node with two children. The node ‘B’ is replaced by its in-order predecessor ‘A’ or in-order successor ‘C’. The replacement node is then recursively removed from its original position.\n\n\n\n\n\nOnce we find the in-order predecessor (or successor), we replace the node to be deleted with the predecessor (or successor) and then recursively delete the predecessor (or successor) from its original position.\nThe reason we choose the in-order predecessor or successor is to maintain the binary search tree property, i.e., for every node, nodes in the left subtree have values less than the node, and nodes in the right subtree have values greater than the node.\nHere is a basic outline of the removal process in pseudocode:\nfunction remove(node, key) {\n  if (node is null) {\n    return node\n  }\n  if (key &lt; node.key) {\n    node.left = remove(node.left, key)\n  } else if (key &gt; node.key) {\n    node.right = remove(node.right, key)\n  } else {\n    if (node.left is null) {\n      return node.right\n    } else if (node.right is null) {\n      return node.left\n    }\n    node.key = minValue(node.right)\n    node.right = remove(node.right, node.key)\n  }\n  return node\n}\n\nfunction minValue(node) {\n  current = node\n  while (current.left is not null) {\n    current = current.left\n  }\n  return current.key\n}\nIn this pseudocode, remove() is a recursive function that deletes the node with the specified key from the tree and returns the new root of the subtree, and minValue() is a helper function that returns the minimum value in a non-empty binary search tree.\nThe time complexity for deletion in a binary tree can range from O(log n) to O(n), where n is the number of nodes. In a balanced tree, the time complexity is O(log n) because we would be traversing the height of the tree, which is logarithmic in a balanced tree. However, in the worst-case scenario, such as when the tree is a degenerate or skewed tree, the time complexity would be O(n) because each removal could potentially involve traversing all the nodes of the tree.\n\n24.5.7.1 Recursive Approach\npublic TreeNode delete(TreeNode root, int value) {\n    if (root == null) {\n        return root;\n    }\n\n    if (value &lt; root.val) {\n        root.left = delete(root.left, value);\n    } else if (value &gt; root.val) {\n        root.right = delete(root.right, value);\n    } else {\n        if (root.left == null)\n            return root.right;\n        else if (root.right == null)\n            return root.left;\n\n        root.val = findMinValue(root.right);\n        root.right = delete(root.right, root.val);\n    }\n\n    return root;\n}\n\nprivate int findMinValue(TreeNode root) {\n    int min = root.val;\n    while (root.left != null) {\n        min = root.left.val;\n        root = root.left;\n    }\n    return min;\n}\nPlease note that the iterative implementation of delete operation is relatively complex and can detract from the understanding of how the delete operation fundamentally works. Therefore, we generally prefer the recursive approach for teaching purposes. However, once you are comfortable with the recursive implementation, it’s worthwhile to try to implement the iterative version on your own for practice.\nThe presented examples illustrate how different BST operations can be performed in both recursive and iterative manner. Each has its own pros and cons - recursive methods are often easier to comprehend and write, whereas iterative methods might provide better performance.\n\n\n\n24.5.8 Performance and Time Complexity\nThe time complexity of binary search tree operations such as search, insert, and delete depends on the height of the binary search tree. In the best-case scenario, the tree is perfectly balanced, and its height is log(n), where n is the number of nodes. In this case, search, insert, and delete operations can be performed in O(log n) time. However, in the worst-case scenario, the tree can become skewed, which means it resembles a linked list more than a tree. In this case, the height of the tree is n, and operations can take up to O(n) time.\nLet’s see how this plays out for each operation:\nSearch Operation\n\nBest-case performance: O(log n) – This is when the tree is balanced, and we can eliminate half of the nodes from our search at each step.\nWorst-case performance: O(n) – This happens when the tree is skewed, and our search path includes most or all nodes in the tree.\n\nInsert Operation\n\nBest-case performance: O(log n) – When the tree is balanced, the location for a new node is found by traversing from the root to the appropriate leaf node.\nWorst-case performance: O(n) – In a skewed tree, the new node could end up being a child of the deepest leaf node.\n\nDelete Operation\n\nBest-case performance: O(log n) – In a balanced tree, we find the node to delete quickly. If the node has two children, we also need time to find the in-order predecessor or successor.\nWorst-case performance: O(n) – In a skewed tree, deletion can involve traversing most of the tree.\n\nOne thing to note is that the best-case scenario, a balanced tree, is not the average case. Unless measures are taken to balance the tree, binary search trees can become skewed from sequences of insertions and deletions.\nThe next topic that we’ll discuss, self-balancing binary search trees, will address this limitation of the basic binary search tree by ensuring that the tree remains approximately balanced at all times. As a result, self-balancing binary search trees can guarantee that the time complexity of major operations is always O(log n), which is a significant improvement in the worst-case scenario.",
    "crumbs": [
      "Trees and Graphs",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Tree Data Structure</span>"
    ]
  },
  {
    "objectID": "20_trees.html#balanced-binary-search-trees",
    "href": "20_trees.html#balanced-binary-search-trees",
    "title": "24  Tree Data Structure",
    "section": "24.6 Balanced Binary Search Trees",
    "text": "24.6 Balanced Binary Search Trees\nBinary search trees (BSTs) are powerful data structures that support efficient search, insertion, and deletion operations. However, if the tree becomes unbalanced, these operations could degrade to linear time complexity. One effective way to mitigate this issue is by employing balanced binary search trees, such as AVL Trees and Red-Black Trees. These trees aim to maintain balance, ensuring efficient performance regardless of the input sequence.\nThe concept of a “balanced” tree might sound abstract, so let’s make it tangible with an example (Figure 24.27). A balanced binary tree is one where the difference between the heights of the left and right subtrees of every node is either -1, 0, or 1. This balance ensures that the tree doesn’t lean too heavily on one side, optimizing the path to every node and promoting efficiency.\n\n\n\n\n\n\n\n\nG\n\n\n\n6\n\n6\n\n\n\n4\n\n4\n\n\n\n6-&gt;4\n\n\n\n\n\n8\n\n8\n\n\n\n6-&gt;8\n\n\n\n\n\n2\n\n2\n\n\n\n4-&gt;2\n\n\n\n\n\n5\n\n5\n\n\n\n4-&gt;5\n\n\n\n\n\n7\n\n7\n\n\n\n8-&gt;7\n\n\n\n\n\n9\n\n9\n\n\n\n8-&gt;9\n\n\n\n\n\n1\n\n1\n\n\n\n2-&gt;1\n\n\n\n\n\n3\n\n3\n\n\n\n2-&gt;3\n\n\n\n\n\n\n\n\nFigure 24.27: A balanced binary search tree. The numbers show the height of each node.\n\n\n\n\n\nConsider a balance scale, with nodes as the weights. When equally balanced, the scale maintains equilibrium. However, adding or subtracting weights (or nodes) disturbs this balance, causing the scale to lean towards the heavier side. AVL Trees and Red-Black Trees work similarly - they maintain equilibrium by redistributing the weights, which we call “rotations” in the context of trees.\nAVL Trees, named after their inventors Adelson-Velsky and Landis, adjust their balance through a process called rotation. This operation preserves the in-order property of the tree. If the balance factor of a node in an AVL tree becomes 2 or -2, a rotation is performed to regain balance. Let’s illustrate this with a simple example (Figure 24.31).\nConsider inserting 1, 2, 3 into an initially empty AVL tree.\n\nInserting 1 gives us:\n\n\n\n\n\n\n\n\n\nG\n\n\n\n1\n\n1\n\n\n\n\n\n\nFigure 24.28: Inserting 1 into an empty AVL tree.\n\n\n\n\n\n\nInserting 2 gives us:\n\n\n\n\n\n\n\n\n\nG\n\n\n\n1\n\n1\n\n\n\n2\n\n2\n\n\n\n1-&gt;2\n\n\n\n\n\n\n\n\nFigure 24.29: Inserting 2 into the AVL tree.\n\n\n\n\n\n\nInserting 3 unbalances the tree:\n\n\n\n\n\n\n\n\n\nG\n\n\n\n1\n\n1\n\n\n\n2\n\n2\n\n\n\n1-&gt;2\n\n\n\n\n\n3\n\n3\n\n\n\n2-&gt;3\n\n\n\n\n\n\n\n\nFigure 24.30: Inserting 3 into the AVL tree results in an unbalanced tree.\n\n\n\n\n\nThis structure is unbalanced and equivalent to a linked list. The balance factor for node 1 is -2, indicating a required rotation.\n\nWe perform a left rotation around the root (node 1), resulting in a balanced tree:\n\n\n\n\n\n\n\n\n\nG\n\n\n\n2\n\n2\n\n\n\n1\n\n1\n\n\n\n2-&gt;1\n\n\n\n\n\n3\n\n3\n\n\n\n2-&gt;3\n\n\n\n\n\n\n\n\nFigure 24.31: A left rotation around the root balances the AVL tree.\n\n\n\n\n\nThe AVL Tree is now balanced, and all operations are guaranteed logarithmic time complexity.\nRed-Black Trees offer another solution, painting each node one of two colors – red or black – and maintaining balance by adhering to red-black properties. Although the rules and operations are more complex, they also ensure the tree remains approximately balanced during additions and deletions. A Red-Black Tree would be great to visualize, but given its complexity, it’s better to explore it interactively or in more advanced courses.\nThese tree structures may seem complicated, but they highlight how well-understood tools like a binary tree can be further optimized for efficiency. While we’ve only scratched the surface here, you’ll delve deeper into these and other types of self-balancing trees in advanced data structures courses. The goal is not to memorize every detail but to appreciate the breadth of tools available for different use cases.\nIn your journey as a computer scientist, you’ll encounter various real-world scenarios where maintaining efficiency is vital. From databases to file systems, different applications require different tools. Understanding the principles behind these tools, such as the importance of balance in BSTs, will empower you to make informed decisions in your work.\nWith this, we conclude our journey into the world of binary trees and binary search trees. We hope that the concepts, examples, and code shared in this chapter help illuminate these fundamental data structures. Remember that learning is a process - with every step, you’re building a stronger foundation in computer science. Keep practicing, keep questioning, and keep exploring.",
    "crumbs": [
      "Trees and Graphs",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Tree Data Structure</span>"
    ]
  },
  {
    "objectID": "21_graphs.html",
    "href": "21_graphs.html",
    "title": "25  Graphs",
    "section": "",
    "text": "25.1 Introduction\nConsider a typical day scrolling through Instagram. You tap on a photo from a friend, then another from an influencer you admire, followed by a click on the profile of someone who commented, leading you to yet another profile you find interesting. This scenario illustrates the complex web of connections that is characteristic of social networks. In order to analyze and make sense of such connections, we need an appropriate data structure to model them.\nTake, for instance, your Instagram followers. We could start by simply representing you and your followers as entities, or “vertices,” connected by lines or “edges.” The diagram below (Figure 25.1) shows such a simple representation of your followers.\nAt first glance, this might appear to be a tree structure, where you are the root, and your followers are the branches. However, this isn’t the most accurate representation. In reality, your followers can also follow other people, and those individuals can have their own followers, creating a multi-tiered and mutual relationship that cannot be represented using a tree structure. Tree structures are fundamentally hierarchical, with each node having one parent at most, making them inadequate to model the complex interconnections found in social networks (Figure 25.2).\nTo accurately capture these complex relationships, we use a graph data structure. Graphs consist of a set of vertices (or nodes) and a set of edges that connect them. In the context of Instagram followers, the vertices represent the users, and the edges represent the connections between them.\nFurthermore, the relationships between Instagram users are not always reciprocal. You might follow a celebrity who does not follow you back. This creates a directed relationship, represented by an edge with a specific direction. Such relationships can be modeled using directed graphs. Figure 25.3 shows a graph where the edges represent the direction of follower relationships.\nUnderstanding and modeling such intricate networks is critical to analyzing social media behavior. From detecting trends and predicting future connections to understanding influence within a network, the applications of this model are vast and significant. In the subsequent sections, we will delve deeper into how graph data structures work, how to implement them, and how to analyze the interesting problems they can help solve.\nImagine standing at a subway station, tracing the colorful lines on the map that depict the various routes. Each line represents a different subway route, while each station serves as a meeting point for different lines. The interplay of lines and stations illustrates the essence of a graph - a non-linear data structure that consists of vertices (stations) and edges (subway routes) connecting these vertices.\nAdjacency and incidence are two fundamental terms in the realm of graphs. Two vertices, say station A and station B, are considered adjacent if a subway line (edge) directly connects them. On the other hand, incidence refers to a station (vertex) being served by a subway line (edge).\nGraphs are not merely abstract mathematical constructs but have tangible real-world applications. They model social networks, where vertices are people, and edges signify their connections. In transportation networks, vertices represent locations interconnected by edges symbolizing routes. In the era of a global pandemic, graphs can even represent Coronavirus transmission networks, where vertices symbolize individuals, and edges depict transmission paths.",
    "crumbs": [
      "Trees and Graphs",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Graphs</span>"
    ]
  },
  {
    "objectID": "21_graphs.html#graph-terminology",
    "href": "21_graphs.html#graph-terminology",
    "title": "25  Graphs",
    "section": "25.2 Graph Terminology",
    "text": "25.2 Graph Terminology\nTo navigate through the expansive world of graphs, one must be acquainted with its language. Let’s traverse through the fundamental terms and properties that define graphs.\n\n25.2.1 Basic Terms and Properties\nImagine a city map representing connections between various landmarks - each landmark is a vertex, and the roads linking them are the edges. The connections established are not always two-way streets. While some roads may be traversed in both directions, symbolizing an undirected graph, others might be one-way streets, indicative of a directed graph. Here, edges have a specific direction, indicating a unidirectional relationship between vertices. The diagrams Figure 25.4 and Figure 25.5 vividly illustrate this concept.\n\n\n\n\n\n\n\n\nG\n\n\n\nA\n\nA\n\n\n\nB\n\nB\n\n\n\nA--B\n\n\n\n\nC\n\nC\n\n\n\nB--C\n\n\n\n\nC--A\n\n\n\n\n\n\n\nFigure 25.4: Example of an undirected graph. Vertices A, B, and C are interconnected, demonstrating bi-directional relationships.\n\n\n\n\n\n\n\n\n\n\n\n\n\nG\n\n\n\nA\n\nA\n\n\n\nB\n\nB\n\n\n\nA-&gt;B\n\n\n\n\n\nC\n\nC\n\n\n\nB-&gt;C\n\n\n\n\n\nC-&gt;A\n\n\n\n\n\n\n\n\nFigure 25.5: Example of a directed graph. The arrowheads (highlighted in orange) demonstrate the direction of the relationships between vertices A, B, and C.\n\n\n\n\n\nIn some cities, not all roads are created equal. Some roads might be longer or more congested, carrying different costs for traversal. This scenario reflects a weighted graph where edges are not equally important but carry different weights. On the contrary, in an unweighted graph, all roads carry the same weight - a short stroll in the park or a long cross-country journey holds equal significance. The illustrations Figure 25.6 and Figure 25.7 demonstrate this.\n\n\n\n\n\n\n\n\nG\n\n\n\nA\n\nA\n\n\n\nB\n\nB\n\n\n\nA--B\n\n\n\n\nC\n\nC\n\n\n\nB--C\n\n\n\n\nC--A\n\n\n\n\n\n\n\nFigure 25.6: Example of an unweighted graph. The edges between vertices A, B, and C carry no weights.\n\n\n\n\n\n\n\n\n\n\n\n\n\nG\n\n\n\nA\n\nA\n\n\n\nB\n\nB\n\n\n\nA--B\n\n2\n\n\n\nC\n\nC\n\n\n\nB--C\n\n3\n\n\n\nC--A\n\n1\n\n\n\n\n\n\nFigure 25.7: Example of a weighted graph. The weights (2, 3, 1) assigned to the edges between vertices A, B, and C differentiate their importance or cost.\n\n\n\n\n\nAs we traverse through the city, we encounter different traffic configurations. Some intersections allow only a single connection between landmarks, reflecting a simple graph with no self-loops. However, other intersections might have multiple routes connecting the same pair of landmarks or even allow U-turns, creating self-loops. This scenario portrays a multigraph. The diagrams Figure 25.8 and Figure 25.9 encapsulate these ideas.\n\n\n\n\n\n\n\n\nG\n\n\n\nA\n\nA\n\n\n\nB\n\nB\n\n\n\nA--B\n\n\n\n\nC\n\nC\n\n\n\nB--C\n\n\n\n\nC--A\n\n\n\n\n\n\n\nFigure 25.8: Example of a simple graph. Vertices A, B, and C are interconnected by simple edges.\n\n\n\n\n\n\n\n\n\n\n\n\n\nG\n\n\n\nA\n\nA\n\n\n\nA--A\n\nself-loop\n\n\n\nB\n\nB\n\n\n\nA--B\n\ne1\n\n\n\nA--B\n\ne2\n\n\n\nC\n\nC\n\n\n\nB--C\n\n\n\n\nC--A\n\n\n\n\n\n\n\nFigure 25.9: Example of a multigraph. Vertex A is connected to vertex B through two edges (a solid and a dashed line) and to itself via a self-loop (in orange).\n\n\n\n\n\nIn the city of graphs, the popularity of a landmark (vertex) might be judged by the number of roads (edges) leading to it, a concept known as the degree of a vertex. A busy intersection with roads pouring in and out signifies a high-degree vertex. (See Figure 25.10 for an example.)\n\n\n\n\n\n\n\n\nG\n\n\n\nA\n\nA - degree 3\n\n\n\nB\n\nB - degree 2\n\n\n\nA--B\n\n\n\n\nD\n\nD - degree 1\n\n\n\nA--D\n\n\n\n\nC\n\nC - degree 2\n\n\n\nB--C\n\n\n\n\nC--A\n\n\n\n\n\n\n\nFigure 25.10: Example graph with vertex degrees. The degree of each vertex is indicated by the number in parentheses in the labels on the edges.\n\n\n\n\n\nTo travel from one landmark to another, one must choose a path, a sequence of landmarks connected by roads. Occasionally, this path might lead back to the starting point, creating a cycle, a closed path where the journey starts and ends at the same landmark. (See Figure 25.11 and Figure 25.12 for examples.)\n\n\n\n\n\n\n\n\nG\n\n\n\nA\n\nA\n\n\n\nB\n\nB\n\n\n\nA--B\n\n\n\n\nC\n\nC\n\n\n\nB--C\n\n\n\n\nD\n\nD\n\n\n\nC--D\n\n\n\n\n\n\n\nFigure 25.11: Example graph with a path from A to D.\n\n\n\n\n\n\n\n\n\n\n\n\n\nG\n\n\n\nA\n\nA\n\n\n\nB\n\nB\n\n\n\nA--B\n\n\n\n\nC\n\nC\n\n\n\nB--C\n\n\n\n\nC--A\n\n\n\n\n\n\n\nFigure 25.12: Example graph with a cycle. (A-B-C)\n\n\n\n\n\nCities can be busy, bustling with interconnected landmarks, or they can be quiet towns with isolated points of interest. A city with a path between every pair of landmarks is a connected graph, while a city with isolated landmarks, unreachable from others, is a disconnected graph. This concept is illustrated in Figure 25.13 and Figure 25.14.\n\n\n\n\n\n\n\n\nG\n\n\n\nA\n\nA\n\n\n\nB\n\nB\n\n\n\nA--B\n\n\n\n\nC\n\nC\n\n\n\nB--C\n\n\n\n\nC--A\n\n\n\n\n\n\n\nFigure 25.13: Example of a connected graph.\n\n\n\n\n\n\n\n\n\n\n\n\n\nG\n\n\n\nA\n\nA\n\n\n\nB\n\nB\n\n\n\nA--B\n\n\n\n\nC\n\nC\n\n\n\nB--C\n\n\n\n\nD\n\nD\n\n\n\nE\n\nE\n\n\n\nD--E\n\n\n\n\n\n\n\nFigure 25.14: Example of a disconnected graph.\n\n\n\n\n\nUnderstanding these terms and properties provides the vocabulary to talk about graphs, setting the stage for exploring more advanced concepts and applications. As we delve deeper, we will start seeing how these properties interplay and shape the complex world of graphs.\n\n\n25.2.2 Graph Notation\nIn mathematical terms, a graph can be represented by the notation \\(G(V, E)\\), where \\(G\\) stands for the graph, \\(V\\) is the set of vertices (or nodes), and \\(E\\) is the set of edges (or connections). This notation provides a concise, abstract representation of the graph, which aids in understanding and analyzing its structure.\n\n\n25.2.3 Special Types of Graphs\nCertain types of graphs, distinguished by specific properties or structures, have earned unique classifications. Three such types are: complete graphs, bipartite graphs, and trees.\n\nComplete Graph: A complete graph, denoted by \\(K_n\\) where \\(n\\) is the number of vertices, is a simple graph where every pair of distinct vertices is connected by a unique edge. In a complete graph, each vertex is directly connected to every other vertex, signifying a fully interconnected network. This type of graph is commonly seen in problems concerning network connectivity and graph coloring. See Figure 25.15.\n\n\n\n\n\n\n\n\n\nG\n\n\n\nA\n\nA\n\n\n\nB\n\nB\n\n\n\nA--B\n\n\n\n\nC\n\nC\n\n\n\nA--C\n\n\n\n\nD\n\nD\n\n\n\nA--D\n\n\n\n\nB--C\n\n\n\n\nB--D\n\n\n\n\nC--D\n\n\n\n\n\n\n\nFigure 25.15: Example of a complete graph. In this graph, every pair of distinct vertices (A, B, C, D) is connected by a unique edge (shown in orange), demonstrating a fully interconnected network.\n\n\n\n\n\n\nBipartite Graph: A bipartite graph can be partitioned into two distinct sets, where vertices within the same set share no edges. Each edge connects a vertex from one set to a vertex in the other set, and never two vertices within the same set. This special class of graphs has extensive applications, including matching problems, scheduling problems, and in the study of molecular structures. See Figure 25.16.\n\n\n\n\n\n\n\n\n\nG\n\n\n\nA\n\nA\n\n\n\n1\n\n1\n\n\n\nA--1\n\n\n\n\n2\n\n2\n\n\n\nA--2\n\n\n\n\nB\n\nB\n\n\n\nB--1\n\n\n\n\n3\n\n3\n\n\n\nB--3\n\n\n\n\nC\n\nC\n\n\n\nC--2\n\n\n\n\nC--3\n\n\n\n\n\n\n\nFigure 25.16: Example of a bipartite graph. The graph can be partitioned into two distinct sets (shown in lightblue and lightgreen), with each edge connecting a vertex from one set to a vertex in the other set. No edges connect vertices within the same set.\n\n\n\n\n\n\nTree: A tree is a special type of graph that holds a significant place in computer science due to its simplicity and versatility. A tree is an undirected, connected graph with no cycles. Often, one vertex is designated as the root, and the other vertices are arranged in a parent-child relationship, forming a hierarchical structure. Trees are utilized in various areas such as organizing hierarchical data, structuring web pages, and in algorithms like searches and sorts. See Figure 25.17.\n\n\n\n\n\n\n\n\n\nG\n\n\n\nA\n\nA\n\n\n\nB\n\nB\n\n\n\nA--B\n\n\n\n\nC\n\nC\n\n\n\nA--C\n\n\n\n\nD\n\nD\n\n\n\nB--D\n\n\n\n\nE\n\nE\n\n\n\nB--E\n\n\n\n\nF\n\nF\n\n\n\nC--F\n\n\n\n\nG\n\nG\n\n\n\nC--G\n\n\n\n\n\n\n\nFigure 25.17: Example of a tree. A hierarchical structure with a designated root (A, in red) and parent-child relationships (edges in orange). The other vertices are arranged in parent-child relationships, forming a hierarchical structure.",
    "crumbs": [
      "Trees and Graphs",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Graphs</span>"
    ]
  },
  {
    "objectID": "21_graphs.html#graph-representation",
    "href": "21_graphs.html#graph-representation",
    "title": "25  Graphs",
    "section": "25.3 Graph Representation",
    "text": "25.3 Graph Representation\nAs we use graphs to model various complex scenarios in computer science, it’s crucial to understand how to translate the abstract concept of graphs into concrete data structures. This will enable us to manipulate graphs within a programming environment. In this section, we’ll explore two prominent methods for graph representation: the adjacency list and the adjacency matrix.\n\n25.3.1 Adjacency List\nIn an adjacency list representation, each vertex in the graph is associated with a list of its neighboring vertices. Depending on the specific scenario, the adjacency list can be implemented via arrays of lists or as a hash table. In either case, each vertex’s key (or index) corresponds to a list of its adjacent vertices.\n\n\n\n\n\n\n\n\nG\n\n\n\nA\n\nA\n\n\n\nB\n\nB\n\n\n\nA--B\n\n\n\n\nC\n\nC\n\n\n\nA--C\n\n\n\n\nD\n\nD\n\n\n\nB--D\n\n\n\n\nC--D\n\n\n\n\n\n\n\nFigure 25.18: An undirected graph depicted as an example for the adjacency list. Each vertex (A in red, B in green, C in blue, D in yellow) is linked to its neighboring vertices. In the adjacency list representation, each of these vertices points to their respective neighbors.\n\n\n\n\n\nAdjacency list representation for the graph Figure 25.18 is as follows:\n\n\n\nListing 25.1: Adjacency list representation.\n\n\nA: [B, C]\nB: [A, D]\nC: [A, D]\nD: [B, C]\n\n\n\nThe adjacency list is particularly effective for sparse graphs, where the number of edges is far less than the total possible number of edges. This method only stores the vertices that are directly connected, conserving memory and allowing faster iteration over the neighbors of a given vertex.\n\n\n25.3.2 Adjacency Matrix\nAn adjacency matrix is a two-dimensional grid representation, where the cell at the i-th row and j-th column denotes the connection between the i-th and j-th vertices. In an unweighted graph, the cells contain a 1 (indicating an edge between the vertices) or a 0 (indicating no edge). In a weighted graph, the cells hold the weights of the respective edges.\n\n\n\n\n\n\n\n\nG\n\n\n\nA\n\nA\n\n\n\nB\n\nB\n\n\n\nA--B\n\n\n\n\nC\n\nC\n\n\n\nA--C\n\n\n\n\nD\n\nD\n\n\n\nB--D\n\n\n\n\nC--D\n\n\n\n\n\n\n\nFigure 25.19: A graph for adjacency matrix example\n\n\n\n\n\nAdjacency matrix representation (unweighted) for the graph in Figure 25.19 is:\n\n\n\nListing 25.2: Adjacency matrix representation.\n\n\n  A B C D\nA 0 1 1 0\nB 1 0 0 1\nC 1 0 0 1\nD 0 1 1 0\n\n\n\nThe adjacency matrix proves advantageous when working with dense graphs, where the number of edges approaches the total possible number of edges. It’s also useful when one needs to rapidly determine whether an edge exists between two vertices. However, for large sparse graphs, this method can be memory-intensive as it stores the state for every possible pair of vertices.\n\n\n25.3.3 Converting Between Representations\n\nConverting from a graph diagram or notation to adjacency list/matrix\n\n\nStart by identifying all vertices and edges in the graph.\nFor the adjacency list, create an empty list or hash table for each vertex. As you enumerate the edges, add the corresponding vertices to the lists of their connected vertices.\nFor the adjacency matrix, create a square matrix with dimensions equal to the number of vertices. As you enumerate the edges, set the cell at the intersection of the two connected vertices to 1 (or the weight of the edge in the case of a weighted graph).\n\n\nConverting from adjacency list/matrix to a graph diagram or notation\n\n\nStart by identifying all vertices from the keys (in an adjacency list) or indices (in an adjacency matrix).\nFor the adjacency list, traverse each list and for each pair of vertices, draw an edge between them.\nFor the adjacency matrix, traverse the matrix and for each non-zero cell, draw an edge between the corresponding vertices. In the case of a weighted graph, the cell value corresponds to the weight of the edge.\n\nLearning these methods to represent graphs in code and being able to switch between them equips you with the flexibility to choose the most efficient representation based on the specific requirements of your problem.",
    "crumbs": [
      "Trees and Graphs",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Graphs</span>"
    ]
  },
  {
    "objectID": "21_graphs.html#graph-traversal",
    "href": "21_graphs.html#graph-traversal",
    "title": "25  Graphs",
    "section": "25.4 Graph Traversal",
    "text": "25.4 Graph Traversal\nImagine a challenge where you need to determine the average age of all Facebook users. Considering Facebook’s user base reaches into the billions, it’s untenable to hold the entire graph of the friend network in memory. Thus, a solution would involve addressing each user’s data individually, as needed. This requirement brings us to the concept of graph traversal algorithms.\nGraph traversal algorithms enable us to visit each node (user, in the Facebook case), accumulate their age, and eventually calculate the average. For instance, we could load the data of a single user, push all their friends to a stack, and then continually pop from this stack, requesting each friend’s data from Facebook. As we receive data, we mark each user as ‘visited’ to avoid duplicating their age in our count. The friends of every loaded user are added to our stack, and this process repeats until our stack is empty.\nThis scenario underscores the crucial role of graph traversal, a foundational operation in graph theory. Graphs, due to their versatility and power, have found applications across diverse domains. Whether it’s modeling social networks, computer networks, transportation systems, or web pages, or even solving complex problems in games, graphs form an indispensable tool. The process of graph traversal allows us to navigate these graphs in various ways, leading to numerous applications like searching for specific nodes, identifying the shortest path between nodes, and analyzing the overall structure of a graph.\nIn the context of graph traversal, we generally commence from a start node and strive to visit all the remaining nodes. This task brings with it a host of challenges, such as dealing with unreachable nodes, avoiding revisiting nodes, and choosing the next node to visit from several potential options. Graph traversal algorithms tackle these challenges by leveraging different strategies and data structures to track visited nodes and those pending to be visited. The two most frequently used algorithms for this purpose are the Breadth-First Search (BFS) and Depth-First Search (DFS). These algorithms primarily differ in the sequence in which they visit nodes.\nIn many real-world scenarios, we may not have access to the entire graph upfront. Instead, we may only have information about a specific node and its adjacent nodes. For example, in the Facebook use case mentioned earlier, we don’t know the entire user network from the outset. But, we can employ graph traversal algorithms to solve problems associated with such large and dynamic graphs by visiting each node and examining their data individually, as required.\nNow, let’s delve into two commonly used methods to traverse a graph:\n\nBreadth-First Search (BFS): This method is akin to exploring a graph layer by layer. We start at the root node (or any other chosen node), explore all the neighboring nodes at the current depth level before moving onto nodes at the next depth level.\nDepth-First Search (DFS): In contrast to BFS, DFS digs as deep as possible into the graph’s structure before backtracking. In essence, it explores an arbitrary path as far as possible before retracing steps.\n\nBy understanding these graph traversal methods and knowing how to implement them, you will be equipped to efficiently navigate and manipulate intricate graphs, enabling you to solve a wide variety of problems. Furthermore, the choice of graph traversal method can often be influenced by the representation of the graph (as an adjacency list or adjacency matrix), which we discussed in the previous section. Having a good grasp of graph representation techniques will thus aid in effective graph traversal.\n\n25.4.1 Breadth-First Search (BFS)\nBreadth-First Search (BFS) is a widely-used algorithm for graph traversal and pathfinding. The defining characteristic of BFS is that it explores a graph in ‘layers’. It begins at a chosen ‘start’ vertex and explores all the neighboring vertices at the current depth prior to moving on to nodes at the next depth level. BFS uses a queue as its core data structure, which provides an inherent ‘First In, First Out’ (FIFO) characteristic - it first visits the nodes that were introduced earlier in the process, thus ensuring that it traverses level by level in the graph.\nThe BFS algorithm’s layer-wise exploration makes it particularly well-suited for problems where the shortest path or minimum number of steps is required, such as navigating through a maze or finding the shortest route in a transportation network. However, it’s important to note that BFS can consume a significant amount of memory because it needs to store all the vertices of the current level.\nTo illustrate how BFS works, let’s consider a step-by-step BFS traversal on the following graph:\n\n\n\n\n\n\n\n\nG\n\n\n\nA\n\n1. A\n\n\n\nB\n\n2. B\n\n\n\nA--B\n\n\n\n\nC\n\n3. C\n\n\n\nA--C\n\n\n\n\nD\n\n4. D\n\n\n\nB--D\n\n\n\n\nC--D\n\n\n\n\nE\n\n5. E\n\n\n\nC--E\n\n\n\n\n\n\n\nFigure 25.20: An example graph for BFS traversal. The numbers represent the order in which the vertices are visited.\n\n\n\n\n\nHere’s how BFS traversal, starting from vertex A, would proceed:\n\nVisit A and add its neighbors B and C to the queue: [B, C]\nVisit B and add its unvisited neighbor D to the queue: [C, D]\nVisit C and add its unvisited neighbor E to the queue: [D, E]\nVisit D: [E]\nVisit E: []\n\nTherefore, the BFS traversal order from A would be A, B, C, D, E.\nWe can encapsulate the BFS algorithm with the following pseudocode:\nBFS(graph, start):\n  Initialize an empty queue Q\n  Mark start as visited\n  Enqueue start into Q\n  \n  while Q is not empty:\n    vertex = Dequeue(Q)\n    Visit vertex\n    \n    for each neighbor of vertex:\n      if neighbor is not visited:\n        Mark neighbor as visited\n        Enqueue neighbor into Q\nThis pseudocode presents the core logic of BFS. It starts with the ‘start’ vertex, explores its neighbors, and marks them as visited. Then it continues with the next node from the queue (the ‘oldest’ unvisited node) and repeats the process until all reachable nodes have been visited. It’s important to note that BFS will only visit nodes reachable from the ‘start’ node; any isolated nodes or nodes in a separate connected component will not be visited by this BFS execution.\n\n\n\n\n\n\n\n\nBFS\n\n\n\nstart\n\nStart\n\n\n\ninitQ\n\nInitialize an empty queue Q\n\n\n\nstart-&gt;initQ\n\n\n\n\n\nmarkStart\n\nMark start as visited\n\n\n\ninitQ-&gt;markStart\n\n\n\n\n\nenqueueStart\n\nEnqueue start into Q\n\n\n\nmarkStart-&gt;enqueueStart\n\n\n\n\n\nQempty\n\nIs Q empty?\n\n\n\nenqueueStart-&gt;Qempty\n\n\n\n\n\ndequeue\n\nDequeue a vertex from Q\n\n\n\nQempty-&gt;dequeue\n\n\nNo\n\n\n\nend\n\nEnd\n\n\n\nQempty-&gt;end\n\n\nYes\n\n\n\nforLoop\n\nFor each neighbor of this vertex, if it is unvisited, mark it as visited and enqueue it into Q\n\n\n\ndequeue-&gt;forLoop\n\n\n\n\n\nforLoop-&gt;Qempty\n\n\n\n\n\n\n\n\nFigure 25.21: Flowchart depicting the steps of the BFS algorithm.\n\n\n\n\n\nBy understanding the BFS algorithm and its application, you can efficiently solve a variety of problems involving layers or levels, shortest path, or minimal steps.\n\n\n25.4.2 Depth-First Search (DFS)\nDepth-First Search (DFS) is another essential technique for graph traversal and pathfinding. DFS explores a graph by visiting a vertex and its neighbors as deeply as possible before backtracking. This characteristic of DFS, going as deep as possible from a node before backtracking, is what distinguishes it from BFS. DFS can be implemented using recursion or an explicit stack data structure. The choice between recursion and stack implementation depends on the problem’s requirements and the size of the graph.\nTo illustrate how DFS works, consider the following graph:\n\n\n\n\n\n\n\n\n\n\n\nA\n\n1\nA\n\n\n\nB\n\n2\nB\n\n\n\nA--B\n\n\n\n\nC\n\n4\nC\n\n\n\nA--C\n\n\n\n\nD\n\n3\nD\n\n\n\nB--D\n\n\n\n\nC--D\n\n\n\n\nE\n\n5\nE\n\n\n\nC--E\n\n\n\n\n\n\n\nFigure 25.22: Example graph for DFS traversal. The numbers and colors indicate the order and depth of traversal, respectively.\n\n\n\n\n\nThe DFS traversal, starting from vertex A, would proceed as follows:\n\nVisit A and recurse on its first neighbor B\nVisit B and recurse on its first neighbor D\nVisit D and backtrack (no unvisited neighbors)\nBacktrack to A and recurse on its next neighbor C\nVisit C and recurse on its first neighbor E\nVisit E and backtrack (no unvisited neighbors)\n\nThis leads to a DFS traversal order of A, B, D, C, E.\nDFS can be implemented either recursively or iteratively. Here are the pseudocodes for both methods:\nRecursive implementation:\nDFS(graph, vertex):\n  Mark vertex as visited\n  Visit vertex\n  \n  for each neighbor of vertex:\n    if neighbor is not visited:\n      DFS(graph, neighbor)\n\n\n\n\n\n\n\n\nDFS\n\n\n\nstart\n\nStart\n\n\n\nmarkVertex\n\nMark vertex as visited\n\n\n\nstart-&gt;markVertex\n\n\n\n\n\nvisitVertex\n\nVisit vertex\n\n\n\nmarkVertex-&gt;visitVertex\n\n\n\n\n\nforLoop\n\nFor each neighbor of vertex, if it is not visited, call DFS recursively\n\n\n\nvisitVertex-&gt;forLoop\n\n\n\n\n\nforLoop-&gt;markVertex\n\n\nRecursive call\n\n\n\nend\n\nEnd\n\n\n\nforLoop-&gt;end\n\n\n\n\n\n\n\n\nFigure 25.23: Flowchart depicting the recursive implementation of the DFS algorithm.\n\n\n\n\n\nIterative implementation (with a stack):\nDFS(graph, start):\n  Initialize an empty stack S\n  Mark start as visited\n  Push start onto S\n  \n  while S is not empty:\n    vertex = Pop(S)\n    Visit vertex\n    \n    for each neighbor of vertex:\n      if neighbor is not visited:\n        Mark neighbor as visited\n        Push neighbor onto S\n\n\n\n\n\n\n\n\nDFS\n\n\n\nstart\n\nStart\n\n\n\ninitStack\n\nInitialize an empty stack S\n\n\n\nstart-&gt;initStack\n\n\n\n\n\nmarkStart\n\nMark start as visited\n\n\n\ninitStack-&gt;markStart\n\n\n\n\n\npushStart\n\nPush start onto S\n\n\n\nmarkStart-&gt;pushStart\n\n\n\n\n\nSempty\n\nIs S empty?\n\n\n\npushStart-&gt;Sempty\n\n\n\n\n\npop\n\nPop a vertex from S\n\n\n\nSempty-&gt;pop\n\n\nNo\n\n\n\nend\n\nEnd\n\n\n\nSempty-&gt;end\n\n\nYes\n\n\n\nforLoop\n\nFor each neighbor of vertex, if it is not visited, mark it as visited and push it onto S\n\n\n\npop-&gt;forLoop\n\n\n\n\n\nforLoop-&gt;Sempty\n\n\n\n\n\n\n\n\nFigure 25.24: Flowchart depicting the iterative implementation of the DFS algorithm.\n\n\n\n\n\n\n\n25.4.3 Applications and Variations of BFS and DFS\nBFS and DFS are versatile tools in graph theory with numerous applications. Understanding their core principles and the variations that can be implemented, provides the foundation for solving a wide range of graph-related problems:\n\nShortest path: BFS can find the shortest path between two vertices in an unweighted graph due to its level-wise exploration characteristic. BFS can be modified to not only keep track of the path length but also trace the actual path taken.\nConnected components: Both BFS and DFS can identify the connected components of an undirected graph. This ability is particularly important in network analysis, where one might want to identify groups of interconnected nodes.\nTopological sorting: DFS is excellent for topological sorting of a directed acyclic graph (DAG). Topological ordering is beneficial in task scheduling problems where certain tasks must be completed before others can start.\nBipartite graph check: A graph is bipartite if its vertices can be split into two independent sets, U and V, such that every edge connects a vertex in U to one in V. BFS or DFS can be adapted to color vertices during the traversal. If at any point during the traversal, two adjacent vertices have the same color, the graph is not bipartite.\nGraph cycle detection: DFS can detect cycles in a graph. This feature is essential in dependency networks where cycles can lead to deadlocks.\n\nIn summary, BFS and DFS are powerful techniques for navigating the complex structures of graphs. By understanding these algorithms, their pros and cons, and their numerous applications, we can better analyze and extract meaningful information from various domains, including computer networks, social networks, transportation systems, and many others.",
    "crumbs": [
      "Trees and Graphs",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Graphs</span>"
    ]
  },
  {
    "objectID": "22_hashing.html",
    "href": "22_hashing.html",
    "title": "26  Hashing, Hash Tables, and Hash Maps",
    "section": "",
    "text": "26.1 Background and Motivation",
    "crumbs": [
      "Hashing",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Hashing, Hash Tables, and Hash Maps</span>"
    ]
  },
  {
    "objectID": "22_hashing.html#background-and-motivation",
    "href": "22_hashing.html#background-and-motivation",
    "title": "26  Hashing, Hash Tables, and Hash Maps",
    "section": "",
    "text": "26.1.1 The Power and Constraints of Indexing\nIndexing allows for direct access to individual elements within an array, using an integer as a specific reference, referred to as an ‘index’. When called with the same index, an array always returns the same element, provided the array itself hasn’t been altered. For instance, to retrieve the 12th element of an array arr, you would use arr[11].\nMathematically, the formula to find the memory address of an array element is given by:\nbaseAddress + (index * sizeOfElement)\nWhile this concept is powerful and efficient, it encounters certain limitations when extended beyond its original design. Let’s delve into one of these constraints.\n\n\n26.1.2 The Challenge of Non-numeric Indexing\nImagine a scenario where we want to access an array element using a string as an index, for example, arr[\"dhruv\"]. What’s the issue here?\nThe problem lies in the arithmetic: baseAddress + (\"dhruv\" * sizeOfElement) is not feasible because the index, “dhruv”, is not a numeric value. Without the ability to perform this operation, we’re unable to directly use strings as indices in an array.\n\n\n26.1.3 Mapping Strings to Numbers: A Key to Overcoming the Constraint\nConsider an array of strings where we map numeric indices to string values:\n0 -&gt; \"Alice\"\n1 -&gt; \"Bob\"\n2 -&gt; \"Charlie\"\nWith an array allowing us to map a number to a string, could we use a similar mechanism to map strings to numbers? Absolutely!\nOne approach involves a linear search for a string in the array, noting its index. For instance, let’s assume we find the string “Dhruv” at index 5:\n0 -&gt; \"Alice\"\n1 -&gt; \"Bob\"\n2 -&gt; \"Charlie\"\n...\n5 -&gt; \"Dhruv\"\nThe index where we locate the string “Dhruv” (5, in this case) can serve as a key in another array, enabling access to data associated with “Dhruv”. Nevertheless, this method, while feasible, isn’t very efficient. Linear searching can become significantly slow when dealing with large datasets, begging the question - is there a better solution? We’ll explore the answer to this question in the upcoming sections.\n\n\n\n\n\n\n\n\nG\n\n\n\narray\n\n0: Alice\n\n1: Bob\n\n2: Charlie\n\n...\n\n5: Dhruv\n\n\n\n0\n0\n\n\n\n0--array:0\n\n\n\n\n1\n1\n\n\n\n1--array:1\n\n\n\n\n2\n2\n\n\n\n2--array:2\n\n\n\n\n5\n5\n\n\n\n5--array:5\n\n\n\n\n\n\n\nFigure 26.1: Mapping strings to numbers using array indices. The index of an array corresponds to a specific string value, such as ‘Dhruv’ at index 5. Each box represents an array element.\n\n\n\n\n\n\n\n26.1.4 Hashing: A Key to Efficient Mapping\nThis brings us to hashing, an ingenious method that enables us to map non-numeric keys, such as strings, to numeric indices, achieving efficiency even with a large volume of data. At the core of this mechanism is a hash function, which transforms a given key into a number that serves as an index within an array.\nA hash function ingests a key and excretes an index, pointing to a location within the hash table’s array. An effective hash function should meet the following criteria:\n\nUniform distribution: The function should scatter keys evenly across the array, thereby reducing the likelihood of collisions (where multiple keys map to the same index).\nMinimal collisions: It should minimize the occurrence of hash collisions.\nSwift computation: The function should quickly compute the hash, thereby enabling rapid data insertion, retrieval, and deletion.\nDeterministic output: The function should consistently produce the same output for a given input.\n\nConsider a rudimentary hash function that transforms the first character of a string into its ASCII code:\nhash(\"dhruv\") = ASCII('d') = 100\nThe hash function yields 100, which we can utilize as an index in an array to store or retrieve data related to “dhruv”. This illustrates how hashing enables us to use strings (and other non-numeric keys) as indices, thus offering a practical solution to efficient mapping.",
    "crumbs": [
      "Hashing",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Hashing, Hash Tables, and Hash Maps</span>"
    ]
  },
  {
    "objectID": "22_hashing.html#hash-functions",
    "href": "22_hashing.html#hash-functions",
    "title": "26  Hashing, Hash Tables, and Hash Maps",
    "section": "26.2 Hash Functions",
    "text": "26.2 Hash Functions\n\n26.2.1 Introduction\nIn our earlier discussion, we accomplished mapping the string “D” to certain data, akin to mapping an index to specific data in an array. The resulting data structure, capable of mapping any string to any data, is termed a hash table. The function that conducts this mapping from strings to data is, as you’ve guessed, the hash function. The procedure of associating “D” to an index 3, where we locate the corresponding value, can be referred to as hashing “D”.\nAs we step further into the realm of hashing, we’ll explore how it enables mapping any object (referred to as a ‘key’) to any other object (termed the ‘record’ or ‘value’). Picture your student ID as a key, while the record stores all relevant data about you attached to this ID.\nFigure 2: At this juncture, a diagram illustrating the concept of hash functions and their role in a hash table would be helpful. The diagram could visualize a hash function mapping different keys (string, numeric or otherwise) to different indices in an array (hash table), which in turn point to corresponding data or records.\n\n\n\n\n\n\n\n\nG\n\n\n\nhashTable\n\nIndex 0:\n\nIndex 1:\n\nIndex 2:\n\n...\n\nIndex 100: Dhruv's Data\n\n\n\nkey1\n\nKey 1\n\n\n\nkey1-&gt;hashTable\n\n\nhash(Key 1)\n\n\n\nkey2\n\nKey 2\n\n\n\nkey2-&gt;hashTable\n\n\nhash(Key 2)\n\n\n\nkey3\n\nKey 3\n\n\n\nkey3-&gt;hashTable\n\n\nhash(Key 3)\n\n\n\ndhruvKey\n\nKey: 'dhruv'\n\n\n\ndhruvKey-&gt;hashTable\n\n\nhash('dhruv')\n\n\n\n\n\n\nFigure 26.2: Hash function mapping keys to indices in a hash table. Each key is hashed to a specific index, which points to the corresponding data or record.\n\n\n\n\n\n\n\n26.2.2 Hashing\nWe can conceptualize hashing as a mechanism for storing and retrieving records from a database, similar to an adept librarian who knows exactly where to fetch a book or where to replace it on the vast shelves. Given a search key value, hashing facilitates the insertion, deletion, and search for records. The speed and efficiency of these operations is remarkable when the hashing is properly implemented, often examining merely one or two records for each operation. This outperforms the average cost of \\(O(log n)\\) required to execute a binary search on a sorted array of n records, or to conduct an operation on a binary search tree.\nDespite its simplicity in concept, the implementation of hashing can be surprisingly complex, requiring careful attention to every detail to ensure a correctly working system. A hash system stores records in an array known as a hash table, denoted as HT in our discussions. Hashing operates by computing a search key K to pinpoint the location in HT that houses the record with key K. This computation is performed by the hash function, denoted as h.\nGiven that hashing schemes position records in the table based on the needs of the address calculation, records aren’t sorted by value. A position in the hash table is often referred to as a slot. The number of slots in the hash table HT is denoted by the variable M, with slots numbered from 0 to M−1. The ultimate aim of a hashing system is to ensure that for any key value K and a hash function h, \\(i=h(K)\\) identifies a slot in the table such that \\(0 \\leq i &lt; M\\), with the key of the record at HT[i] being equal to K.\nHowever, hashing isn’t the panacea for all applications. It stumbles in scenarios where multiple records with the same key value are allowed. Nor is it effective in addressing range searches, i.e., locating all records whose key values fall within a particular range. It also falters when it comes to finding the record with the minimum or maximum key value, or traversing the records in key order. Hashing excels at answering the question, ‘Which record, if any, has the key value K?’ It’s most appropriate for exact-match queries, and proves to be incredibly efficient when implemented correctly.\nGiven the large range of key values, hashing generally takes records and stores them in a table with a relatively small number of slots. This inevitably leads to situations where a hash function maps two keys to the same slot, an event known as a collision.\nTo illustrate, imagine a classroom of students. What is the likelihood that a pair of students share the same birthday (same day of the year, not necessarily the same year)? With 23 students and 365 “slots” or possible days for a birthday, the odds seem slim. However, as the number of students grows, so does the probability of a collision, i.e., two students sharing a birthday. A database utilizing hashing must be careful not to create a hash table so large it wastes space.\nAn ideal hash function should distribute keys to slots such that every slot in the hash table has an equal probability of being filled, given the actual set of keys being used. Unfortunately, we typically have no control over the distribution of key values for the actual records in a given database or collection. Hence, the effectiveness of any particular hash function depends on the actual distribution of the keys used within the allowable key range.\nIn some cases, incoming data are well distributed across their key range. For instance, if the input is a set of random numbers selected uniformly from the key range, any hash function that assigns the key range so that each slot in the hash table receives an equal share of the range will likely also distribute the input records uniformly within the table.\nHowever, there are numerous scenarios where incoming records are highly clustered or otherwise poorly distributed. It can be challenging to devise a hash function that successfully scatters the records throughout the table, especially if the input distribution is unknown in advance. For example, if the input is a collection of English words, the beginning letter will be poorly distributed. A dictionary of words mapped to their frequencies is often used in basic natural language processing algorithms.\nIn summary, while any function can serve as a hash function (i.e., mapping a value to an index), not all can qualify as a good hash function. A function that constantly returns the index 0 is a hash function, albeit a subpar one as it maps everything to 0. A common hash function is the modulus operator. For instance, in N-sized hash tables, it’s common to use the modulus of N as a hash function. If N is \\(20\\), data for 113 will be hashed to index \\(113 \\% 20 = 13\\).\nBut what happens when multiple pieces of data map to the same index using the modulus operator? For instance, \\(53 \\% 20 = 13\\), \\(73 \\% 20 = 13\\), etc. The solution lies in the use of nested data structures, allowing us to store everything at index 13. We will explore this in more detail later.\n\n\n26.2.3 Simple Hash Functions\n\n\n\n\n\n\n\n\nG\n\n\ncluster_1\n\nNested Data Structure\n(Linked List)\n\n\ncluster_0\n\nHash Table\n\n\n\nhashTable\n\nIndex 0:\n\nIndex 1:\n\n...\n\nIndex 13:\n\n...\n\nIndex 19:\n\n\n\nnested\n\n113\n\n53\n\n73\n\n\n\nhashTable:f13-&gt;nested\n\n\nstore at index 13\n\n\n\nkeys\n\nKeys: 113, 53, 73\n\n\n\nhashFunction\n\nHash Function\n(% 20)\n\n\n\nkeys-&gt;hashFunction\n\n\napply\n\n\n\nhashFunction-&gt;hashTable:f13\n\n\nhash to index 13\n\n\n\n\n\n\nFigure 26.3: An illustration of hash collision and its resolution using nested data structures. Multiple keys hash to the same index (13), triggering a collision. A nested data structure (linked list) at index 13 accommodates all these colliding keys.\n\n\n\n\n\nLet’s envision hashing as a real-world scenario. A teacher has a stack of students’ tests, and she wants to categorize them by the last digit of their respective student IDs. In this case, the “bucket” will be the bins labeled with numbers from 0 to 9, and the teacher will place each test paper in the bin that matches the last digit of the student’s ID. This process is analogous to a hash function, and the simple function in this example is equivalent to the modulo operation.\nGiven a hash table of size 5, we can use this hash function to assign keys to the appropriate index:\nHashTable size: 5\nHashFunction: key % size\n\nKeys: 15, 28, 47, 10, 33\n\nIndices:\n15 % 5 = 0\n28 % 5 = 3\n47 % 5 = 2\n10 % 5 = 0\n33 % 5 = 3\nThis quick mapping of keys to indices highlights the simplicity and efficiency of a well-chosen hash function. However, not all hash functions are created equal, and their effectiveness can be dependent on the data they’re being applied to. Let’s explore a few other hash functions that are commonly used in different scenarios.\n\n\n26.2.4 Various Hash Functions and Their Applications\n\n26.2.4.1 Direct Hashing\nDirect hashing is as straightforward as it gets. If we consider the item’s key as its index, this constitutes direct hashing. It’s like walking into a classroom and asking students to sit according to their roll numbers, which are unique to each student. So, a student with roll number 15 would sit at desk number 15.\nThis simplicity, however, comes with its own limitations:\n\nKeys must be non-negative integers.\nThe size of the hash table must equal the maximum key value plus 1, which can lead to a large table and inefficient use of space if the keys are large.\n\n\n\n26.2.4.2 Modulo Hash\nThe modulo hash function, as seen in our example above, utilizes the remainder of the key divided by the table size M to determine the index. It’s a simple and effective way to convert a larger key range into a manageable index range.\nh(K) = K % M\n\n\n26.2.4.3 Mid-Square Hash\nMid-square hashing takes a unique approach. The key is squared, and a portion of the resulting squared value is used as the index. This technique can be particularly useful when keys are not uniformly distributed.\nh(K) = middle_digits(K^2)\n\n\n26.2.4.4 Mid-Square Hash with Base 2\nThis variation of mid-square hashing squares the key and extracts the middle bits of the binary representation of the squared value as the index. This technique can be particularly useful for binary keys.\nh(K) = middle_bits(K^2)\n\n\n26.2.4.5 Multiplicative String Hashing\nMultiplicative string hashing offers a way to handle string keys. It treats the characters in the string as numbers and combines them using multiplication and a constant. This approach can help achieve a good distribution of string keys in the hash table.\nh(K) = (c1 * a^(n-1) + c2 * a^(n-2) + ... + cn) % M\nHere, c1, c2, ..., cn are the character codes of the string, a is a constant, n is the length of the string, and M is the size of the hash table.\nApplying our hash function (modulo 5) to the keys from the example above, we end up with a hash table represented in ASCII as follows:\nIndex | Key\n-------------\n  0   | 15\n  1   | -\n  2   | 47\n  3   | 28\n  4   | -\nHere, we observe that the keys 15 and 10, as well as 28 and 33, have resulted in collisions, as they both map to the same indices (0 and 3, respectively). The presence of collisions brings us to another important aspect of hashing: collision resolution strategies, which we will explore in the upcoming sections.\n\n\n\n\n\n\n\n\nG\n\n\n\nhashTable\n\nIndex 0:\n\nIndex 1:\n\nIndex 2:\n\nIndex 3:\n\nIndex 4:\n\n\n\nkeys1\n\n15\n\n10\n\n\n\nkeys1-&gt;hashTable\n\n\n\ncollision (at index 0)\n\n\n\nhashFunction\n\nHash Function\n(% 5)\n\n\n\nkeys1-&gt;hashFunction\n\n\napply\n\n\n\nkeys3\n\n28\n\n33\n\n\n\nkeys3-&gt;hashTable\n\n\n\ncollision (at index 3)\n\n\n\nkeys3-&gt;hashFunction\n\n\napply\n\n\n\nkey2\n\n47\n\n\n\nkey2-&gt;hashFunction\n\n\napply\n\n\n\nhashFunction-&gt;hashTable\n\n\nhash to indices\n\n\n\n\n\n\nFigure 26.4: Illustration of a hash table with keys and collisions. The keys 15 and 10, as well as 28 and 33, have collided, mapping to the same indices (0 and 3, respectively).\n\n\n\n\n\n\n\n\n26.2.5 Comparing Different Hash Functions\nVarious hash functions exist, each offering a distinct balance between computational speed and uniform key distribution, thus leading to different degrees of collision occurrence. The effectiveness of a hash function can significantly influence the overall performance of a hash table.\n\n26.2.5.1 Efficiency versus Complexity Trade-off\nFor instance, a simple hash function, such as modulo operation, is computationally efficient but tends to distribute keys unevenly across the table. This non-uniform distribution increases the probability of collisions, resulting in deteriorated performance due to the need for collision resolution techniques.\nOn the other hand, more sophisticated hash functions, including cryptographic hash functions, yield a more uniform distribution of keys across the table. However, their complexity leads to slower computation times, impacting the speed of operations.\nIn real-world applications, the choice of a hash function is largely influenced by the specific demands of the task and the nature of the data being processed. The primary goal is to strike a balance between achieving a uniform key distribution, minimal collisions, fast computation, and deterministic output.",
    "crumbs": [
      "Hashing",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Hashing, Hash Tables, and Hash Maps</span>"
    ]
  },
  {
    "objectID": "22_hashing.html#collision-resolution-in-hashing-chaining-and-open-addressing",
    "href": "22_hashing.html#collision-resolution-in-hashing-chaining-and-open-addressing",
    "title": "26  Hashing, Hash Tables, and Hash Maps",
    "section": "26.3 Collision Resolution in Hashing: Chaining and Open Addressing",
    "text": "26.3 Collision Resolution in Hashing: Chaining and Open Addressing\n\n26.3.1 Hash Collisions\nHash collisions, a scenario where multiple keys map to the same index, are an inevitable outcome in hash tables due to the pigeonhole principle, which states that if there are more pigeons than pigeonholes, at least one pigeonhole must contain more than one pigeon. In the context of hashing, the keys represent pigeons, and the indices of the hash table are the pigeonholes. This analogy aptly describes how more keys than available indices will result in collisions. Such collisions can have a detrimental effect on the efficiency of hash table operations, increasing access times for insertions, deletions, and retrievals.\nTwo prominent strategies for collision resolution include chaining and open addressing.\n\n\n26.3.2 Chaining in Hash Collisions\nChaining employs an auxiliary data structure, such as a linked list, at each index to accommodate multiple key-value pairs. This strategy effectively creates a ‘chain’ of entries at the index where a collision occurs.\n\n26.3.2.1 Process for Insertion, Search, and Deletion\nThe following steps outline the handling of key-value pairs during insertion, search, and deletion when using chaining:\n\nInsertion: The hash function calculates the index for the key. If the index is empty, a new data structure is initialized, and the key-value pair is inserted. In case of a non-empty index, the key-value pair is added to the existing data structure.\nSearch: Upon calculating the index using the hash function, if the index is empty, it indicates the absence of the key in the hash table. If not empty, the data structure at the index is searched to find the key.\nDeletion: Similar to the search operation, the index is determined using the hash function. If the index is empty, it means the key does not exist in the hash table. If the index is non-empty, the data structure is searched to locate the key, which is then removed.\n\n\n\n\n\n\n\n\n\nG\n\n\n\nhashTable\n\nIndex 0:\n\nIndex 1:\n\nIndex 2:\n\nIndex 3:\n\nIndex 4:\n\n\n\nchain0\n\n15\n\n10\n\n\n\nhashTable-&gt;chain0\n\n\n\ncollision resolved (at index 0)\n\n\n\nchain3\n\n28\n\n33\n\n\n\nhashTable-&gt;chain3\n\n\n\ncollision resolved (at index 3)\n\n\n\nkeys1\n\n15\n\n10\n\n\n\nhashProcess\n\n\n\n\nkeys1-&gt;hashProcess\n\n\n\n\nkeys3\n\n28\n\n33\n\n\n\nkeys3-&gt;hashProcess\n\n\n\n\nkey2\n\n47\n\n\n\nkey2-&gt;hashProcess\n\n\n\n\nhashFunction\n\nHash Function\n\n\n\nhashFunction-&gt;hashProcess\n\n\n\n\nhashProcess-&gt;hashTable\n\n\nhash to indices\n\n\n\n\n\n\nFigure 26.5: Illustration of collision resolution using Chaining. Keys 15 and 10, and 28 and 33, have collided at indices 0 and 3, respectively, resulting in linked lists (chains) at those indices.\n\n\n\n\n\n\n\n26.3.2.2 Advantages and Disadvantages of Chaining\nChaining presents several benefits and drawbacks:\n\nAdvantages:\n\nEase of implementation: The use of pre-existing data structures, like linked lists, simplifies the implementation of chaining.\nDynamic size management: The data structure at each index can grow or shrink as necessary, enabling efficient space utilization.\n\nDisadvantages:\n\nIncreased space requirements: Chaining necessitates extra space to store the data structure at each index, contributing to memory overhead.\nVariable access time: The time taken to access key-value pairs is contingent on the length of the data structure at the index, which can vary.\n\n\nDespite the memory overhead and potentially variable access times, chaining remains a widely used method for resolving hash collisions due to its simplicity and capacity to dynamically manage memory.\n\n\n\n26.3.3 Open Addressing in Hash Collisions\nOpen addressing presents an alternative strategy for collision resolution. It entails finding a substitute index for a key-value pair if the original index is already occupied. This method employs a technique known as probing to find the next available index in the event of a collision. Common probing techniques include linear probing, quadratic probing, and double hashing.\n\n26.3.3.1 Probing Techniques\n\nLinear probing: If a collision is encountered, the hash table is scanned sequentially (one index at a time) until an empty slot is discovered.\nQuadratic probing: During a collision, the hash table is searched quadratically (by incrementing the index by the square of the probe number) until a vacant slot is identified.\nDouble hashing: In the event of a collision, a secondary hash function is utilized to determine a new index for the key-value pair. This process is repeated until an empty slot is found.\n\n\n\n\n\n\n\n\n\nG\n\n\n\nhashTable\n\nIndex 0:\n\nIndex 1:\n\nIndex 2:\n\nIndex 3:\n\nIndex 4:\n\n\n\nrelocatedKey\n\n33\n\n\n\nhashTable-&gt;relocatedKey\n\n\n\ncollision resolved (at index 4)\n\n\n\nkeys1\n\n15\n\n10\n\n\n\nhashProcess\n\n\n\n\nkeys1-&gt;hashProcess\n\n\n\n\nkeys2\n\n28\n\n33\n\n\n\nkeys2-&gt;hashProcess\n\n\n\n\nkey2\n\n47\n\n\n\nkey2-&gt;hashProcess\n\n\n\n\nhashFunction\n\nHash Function\n\n\n\nhashFunction-&gt;hashProcess\n\n\n\n\nhashProcess-&gt;hashTable\n\n\nhash to indices\n\n\n\n\n\n\nFigure 26.6: Illustration of collision resolution using Open Addressing. Here, key 33 has been relocated to index 4 due to a collision at index 3.\n\n\n\n\n\n\n\n26.3.3.2 Insertion, Search, and Deletion\nWhen interacting with a hash table operating under open addressing, three main operations emerge: insertion, search, and deletion. The performance of these operations deeply intertwines with the hash function and probing technique utilized:\n\nInsertion: To insert a key-value pair, begin by computing the hash function to determine the initial index. If the index remains unoccupied, place the key-value pair there. However, if a collision occurs and the index is occupied, leverage the probing technique to navigate the terrain of the hash table and locate an alternative index. There, you will find your new home for the key-value pair.\nSearch: To find a key, repeat the hashing process, leading to the first potential residence of the key-value pair. If an empty index greets you, the key does not exist in the hash table. If an occupied index is found, compare the stored key with the desired one. A match indicates a successful search; a mismatch compels the probing technique to guide you towards the next probable index. Repeat this process until the key reveals itself or an empty index signals the end of the journey.\nDeletion: To remove a key-value pair, initiate the same hashing and searching process. If the key is located, excise the key-value pair and mark the index as a grave—a formerly occupied place. However, a solitary deletion might disrupt the balance of the hash table if the removed pair was part of a cluster, requiring continued probing to rectify the disarray.\n\n\n\n26.3.3.3 Advantages and Disadvantages\nLike all data structures, open addressing presents both advantages and disadvantages, a balance one must consider carefully:\n\nAdvantages:\n\nMemory-Efficient: Open addressing demands no extra storage for additional data structures at each index, presenting a more frugal option.\nDefined Size: The static size of the hash table allows for predictable memory usage, a boon when memory resources are constrained.\n\nDisadvantages:\n\nClustering Phenomenon: Certain probing techniques can induce clusters of key-value pairs, hindering efficient access.\nDeletion Dilemmas: The removal of key-value pairs can disrupt the structure, creating “holes” within clusters that must be addressed.\n\n\nWhile open addressing provides a viable alternative for resolving hash collisions and may outperform chaining in memory-constrained situations, it’s not the panacea for all use cases. One must consider clustering phenomena and deletion challenges, which could tip the scales unfavorably.",
    "crumbs": [
      "Hashing",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Hashing, Hash Tables, and Hash Maps</span>"
    ]
  },
  {
    "objectID": "22_hashing.html#evaluating-complexity-and-load-factor-in-hash-tables",
    "href": "22_hashing.html#evaluating-complexity-and-load-factor-in-hash-tables",
    "title": "26  Hashing, Hash Tables, and Hash Maps",
    "section": "26.4 Evaluating Complexity and Load Factor in Hash Tables",
    "text": "26.4 Evaluating Complexity and Load Factor in Hash Tables\nIn order to properly assess the complexity of hash functions and hash tables, it’s important to understand the time required to perform fundamental operations such as search, insertion, and deletion. These operations typically consist of two essential steps:\n\nComputation of the hash function for the given key.\nTraversal of the list of key-value pairs at the computed index.\n\n\n26.4.1 Analyzing Time Complexity in Hash Computation\nIn the first step, the time complexity depends on both the key and the hash function used. Let’s take an example where the key is a string, such as “abcd”. In this case, the complexity of its hash function might depend on the length of the string. However, when we’re dealing with an exceptionally large number of entries n in the map, the length of the keys becomes almost negligible compared to n. Therefore, we can treat the computation of the hash function as a constant time operation, i.e., O(1).\n\n\n26.4.2 Investigating Time Complexity in List Traversal\nFor the second step, which involves traversing the list of key-value pairs at the computed index, the time complexity can be quite variable. In the worst-case scenario, where all the n entries end up at the same index, the time complexity escalates to O(n). However, thanks to the substantial amount of research dedicated to designing hash functions that distribute keys uniformly in the array, this situation occurs very infrequently.\n\n\n26.4.3 Understanding the Load Factor\nThe load factor, represented by the symbol λ, plays a pivotal role in the performance of our hash table. It’s defined as n/b, where n represents the number of entries and b stands for the size of the array. Hence, it refers to the average number of entries at each index:\nλ = n/b\nIt’s crucial to keep this load factor low in order to limit the number of entries at a single index, thus keeping the complexity close to a constant, i.e., O(1).\n\n\n26.4.4 The Interplay Between Load Factor and Complexity\nIn order to maintain the load factor within an acceptable range, we can resize the hash table when the load factor surpasses a certain predefined threshold. This action effectively helps to maintain the complexity of the hash table operations around O(1) by ensuring a uniform distribution of the keys across a larger array.\nIn conclusion, a comprehensive understanding of the complexity and load factor of hash functions is key to designing efficient hash tables. By carefully selecting an appropriate hash function and managing the load factor, we can aim to achieve a near-constant time complexity for various operations in a hash table.",
    "crumbs": [
      "Hashing",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Hashing, Hash Tables, and Hash Maps</span>"
    ]
  },
  {
    "objectID": "22_hashing.html#rehashing",
    "href": "22_hashing.html#rehashing",
    "title": "26  Hashing, Hash Tables, and Hash Maps",
    "section": "26.5 Rehashing",
    "text": "26.5 Rehashing\nRehashing—quite literally, hashing anew—is a mechanism that helps manage the load factor when it swells beyond a certain threshold. This predefined value often defaults to 0.75. If the load factor rises above this threshold, the complexity of hash table operations increases, thereby prompting a need for rehashing.\nDuring rehashing, the size of the array (typically doubling) is augmented, and all existing values are rehashed, subsequently being placed into the new, more capacious array. This enlargement of the array helps in maintaining a lower load factor and thereby, lower complexity.\n\n\n\n\n\n\n\n\nG\n\n\n\nBeforeRehash\n\nBeforeRehash\n\n\n\nrehashProcess\n\nRehashing\n(Resize and Redistribute)\n\n\n\nBeforeRehash-&gt;rehashProcess\n\n\n\n\nAfterRehash\n\nAfterRehash\n\n\n\nAfterRehash-&gt;rehashProcess\n\n\n\n\nBeforeEntries\n\n \n\n \n\nK1\n\n \n\nK2\n\n \n\n\n\nAfterEntries\n\n \n\nK1\n\n \n\nK2\n\n \n\n \n\n \n\nK3\n\n \n\n\n\nrehashProcess-&gt;BeforeEntries\n\n\nBefore Rehashing\n\n\n\nrehashProcess-&gt;AfterEntries\n\n\nAfter Rehashing\n\n\n\n\n\n\nFigure 26.7: The process of rehashing in a hash table. Upon insertion of a new key, if the load factor crosses a certain threshold, the hash table increases its size and redistributes the keys uniformly across the new, larger array, maintaining an efficient time complexity.\n\n\n\n\n\n\n26.5.1 The Need for Rehashing\nAs key-value pairs accumulate in the map, they contribute to an increasing load factor. As previously detailed, a rising load factor implies an escalating time complexity, which could potentially compromise the desirable time complexity of O(1). Consequently, to restore the balance, rehashing is employed, expanding the size of the bucketArray to dial down both the load factor and time complexity.\n\n\n26.5.2 The Mechanics of Rehashing\nRehashing in a hash table typically unfolds as follows:\n\nMonitor the load factor with each addition of a new entry to the map.\nIf the load factor surpasses its predefined value (or defaults to 0.75 if not specified), initiate rehashing.\nAs part of the rehashing process, construct a new array that’s double the size of the previous one and assign it as the new bucketArray.\nIterate through each element in the former bucketArray and employ the insert() method for each, allowing it to find its place in the new, enlarged bucketArray.\n\nThe diagram below offers a visual representation of this rehashing process:\nInitial bucketArray (size = 4):\n+---+---+---+---+\n|   | K1|   | K2|\n+---+---+---+---+\n\nUpon insertion of a new key K3 (load factor &gt; 0.75):\n\nNew bucketArray (size = 8):\n+---+---+---+---+---+---+---+---+\n|   | K1|   | K2|   |   |   | K3|\n+---+---+---+---+---+---+---+---+\nBy applying rehashing, a hash table can retain its desired time complexity of O(1) even as the element count increases. It’s noteworthy that rehashing, while an effective mechanism, can be a costly operation—especially if the hash table houses a large number of elements. Nevertheless, since rehashing is triggered infrequently and only when the load factor breaches a certain threshold, the amortized cost of rehashing remains low. This allows hash table operations to sustain their near-constant time complexity.",
    "crumbs": [
      "Hashing",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Hashing, Hash Tables, and Hash Maps</span>"
    ]
  },
  {
    "objectID": "22_hashing.html#hash-tables-and-hash-maps",
    "href": "22_hashing.html#hash-tables-and-hash-maps",
    "title": "26  Hashing, Hash Tables, and Hash Maps",
    "section": "26.6 Hash Tables and Hash Maps",
    "text": "26.6 Hash Tables and Hash Maps\nIt’s common for individuals, even computer science students and practitioners, to use the terms hash table and hash map interchangeably. However, these are distinct constructs in computing, each with its unique characteristics, capabilities, and preferred applications.\nHash tables and hash maps, while sharing similarities, diverge significantly in their operational mechanisms:\n\nA hash table implements direct hashing, necessitating that the key either be an integer or directly convertible to one, such as a string of numerals. This integer then serves as the determinant for the index in the hash table.\nA hash map, in contrast, embraces indirect hashing, where keys of any data type are permitted. This necessitates an auxiliary hash function that transforms the key into an index within the hash table.\n\nWhen making the choice between a hash table or a hash map, one must critically examine the problem domain, particularly the data type of the keys:\n\nIn scenarios where keys are integers or easily translatable into integers, a hash table might be a more fitting solution. A case in point is when handling student IDs as keys; in such a context, a hash table would be ideal.\nIf the keys, however, belong to other data types or resist easy conversion into integers, a hash map is often the superior choice. For instance, if you are dealing with strings like usernames or URLs as keys, a hash map tends to be the better fit.",
    "crumbs": [
      "Hashing",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Hashing, Hash Tables, and Hash Maps</span>"
    ]
  },
  {
    "objectID": "22_hashing.html#hashmaps-in-java",
    "href": "22_hashing.html#hashmaps-in-java",
    "title": "26  Hashing, Hash Tables, and Hash Maps",
    "section": "26.7 HashMaps in Java",
    "text": "26.7 HashMaps in Java\nThe HashMap class is a part of the Java Collection Framework and encapsulates the functionality of a hash table. As a key-value pair repository, each key within a HashMap is unique, and the ordering of keys is not maintained.\nHere’s how to utilize a HashMap in Java:\n\nImport the HashMap class: First, you must import the HashMap class from the java.util package to use it within your Java code:\nimport java.util.HashMap;\nInstantiate a HashMap: To create a new HashMap instance, employ the following syntax:\nHashMap&lt;String, Integer&gt; myMap = new HashMap&lt;String, Integer&gt;();\nInserting elements: To add key-value pairs to the HashMap, make use of the put() method:\nmyMap.put(\"apple\", 3);\nmyMap.put(\"banana\", 5);\nmyMap.put(\"orange\", 2);\nRetrieving elements: To fetch the value associated with a specific key, use the get() method:\nint apples = myMap.get(\"apple\"); // returns 3\nint oranges = myMap.get(\"orange\"); // returns 2\nRemoving elements: To delete a key-value pair from the HashMap, apply the remove() method:\nmyMap.remove(\"banana\");\nVerifying key existence: To verify whether a particular key resides within the HashMap, call the containsKey() method:\nboolean hasApple = myMap.containsKey(\"apple\"); // returns true\nboolean hasGrape = myMap.containsKey(\"grape\"); // returns false\nIterating over keys: To traverse the keys in a HashMap, you can utilize a for-each loop in conjunction with the keySet() method:\nfor (String fruit : myMap.keySet()) {\n    System.out.println(fruit + \": \" + myMap.get(fruit));\n}\nIterating over values: If you wish to iterate over the values within a HashMap, a for-each loop alongside the values() method can be used:\nfor (Integer count : myMap.values()) {\n    System.out.println(count);\n}\nIterating over key-value pairs: To cycle through the key-value pairs in a HashMap, employ a for-each loop with the entrySet() method:\nfor (HashMap.Entry&lt;String, Integer&gt; entry : myMap.entrySet()) {\n    System.out.println(entry.getKey() + \": \" + entry.getValue());\n}\n\nThe HashMap proves to be a handy data structure when it comes to efficiently storing key-value pairs. With constant-time performance for common operations such as put, get, and remove, it emerges as an invaluable asset for a wide array of applications.\nThis is an excellent first draft that is very detailed. I’ve noticed a few opportunities for improvement that will make your descriptions more consistent and add a bit more depth to your explanations.",
    "crumbs": [
      "Hashing",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Hashing, Hash Tables, and Hash Maps</span>"
    ]
  },
  {
    "objectID": "22_hashing.html#hashtables-in-java",
    "href": "22_hashing.html#hashtables-in-java",
    "title": "26  Hashing, Hash Tables, and Hash Maps",
    "section": "26.8 HashTables in Java",
    "text": "26.8 HashTables in Java\nIn Java, a HashTable is a type of collection that adheres to the Map interface, utilizing a hash table for storage. It shares similarities with HashMap, however, it distinguishes itself through its inherent synchronization which confers thread-safety. The HashTable class stores unique keys and their corresponding values, much like HashMap, it does not maintain any ordering of these keys.\nHere is how one might put HashTable to use in Java:\n\nImporting the HashTable Class: The HashTable class can be integrated into your Java code via importing from the java.util package:\nimport java.util.Hashtable;\nInstantiating a HashTable: The creation of a new HashTable can be achieved with the following syntax:\nHashtable&lt;String, Integer&gt; myTable = new Hashtable&lt;String, Integer&gt;();\nInserting Elements: Key-value pairs can be added to the HashTable using the put() method:\nmyTable.put(\"apple\", 3);\nmyTable.put(\"banana\", 5);\nmyTable.put(\"orange\", 2);\nAccessing Elements: The value associated with a specific key can be retrieved using the get() method:\nint apples = myTable.get(\"apple\"); // 3\nint oranges = myTable.get(\"orange\"); // 2\nDeleting Elements: To expunge a key-value pair from the HashTable, make use of the remove() method:\nmyTable.remove(\"banana\");\nKey Existence Verification: To ascertain the existence of a key in the HashTable, the containsKey() method is handy:\nboolean hasApple = myTable.containsKey(\"apple\"); // true\nboolean hasGrape = myTable.containsKey(\"grape\"); // false\nIteration Over Keys: Iteration through the keys in a HashTable can be achieved with a for-each loop in conjunction with the keySet() method:\nfor (String fruit : myTable.keySet()) {\n    System.out.println(fruit + \": \" + myTable.get(fruit));\n}\nIteration Over Values: To traverse through the values in a HashTable, a for-each loop combined with the values() method is efficient:\nfor (Integer count : myTable.values()) {\n    System.out.println(count);\n}\nIteration Over Key-Value Pairs: To iterate through the key-value pairs in a HashTable, the entrySet() method can be used in combination with a for-each loop:\nfor (Hashtable.Entry&lt;String, Integer&gt; entry : myTable.entrySet()) {\n    System.out.println(entry.getKey() + \": \" + entry.getValue());\n}\n\nThe HashTable class proves useful in scenarios where key-value pairs need to be stored with a requirement for thread-safe operations. However, due to the performance overhead resulting from synchronization, if thread safety isn’t a primary concern, a HashMap would generally be a more efficient alternative.",
    "crumbs": [
      "Hashing",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Hashing, Hash Tables, and Hash Maps</span>"
    ]
  },
  {
    "objectID": "22_hashing.html#hashsets-in-java",
    "href": "22_hashing.html#hashsets-in-java",
    "title": "26  Hashing, Hash Tables, and Hash Maps",
    "section": "26.9 HashSets in Java",
    "text": "26.9 HashSets in Java\nA HashSet in Java is a collection class implementing the Set interface, and leverages a hash table for its storage mechanism. Contrary to hash tables and hash maps, it doesn’t store key-value pairs. Instead, it stores unique elements only. These elements aren’t stored in any particular order, and the class doesn’t allow for duplicate values.\nThe usage of a HashSet in Java can be illustrated as follows:\n\nImporting the HashSet Class: To integrate the HashSet class into your Java code, import it from the java.util package:\nimport java.util.HashSet;\nCreating a HashSet: A new HashSet can be instantiated using the following syntax:\nHashSet&lt;String&gt; mySet = new HashSet&lt;String&gt;();\nAdding Elements: Elements can be added to the HashSet using the add() method:\nmySet.add(\"apple\");\nmySet.add(\"banana\");\nmySet.add(\"orange\");\nRemoving Elements: To remove an element from the HashSet, employ the remove() method:\nmySet.remove(\"banana\");\nElement Existence Verification: To verify the existence of an element in the HashSet, the contains() method is quite useful:\nboolean hasApple = mySet.contains(\"apple\"); // true\nboolean hasGrape = mySet.contains(\"grape\"); // false\nIteration Over Elements: To traverse the elements stored in a HashSet, a for-each loop is effective:\nfor (String fruit : mySet) {\n    System.out.println(fruit);\n}\n\nThe HashSet class is particularly beneficial when there’s a need to store a unique set of elements without any specific order. Given its provision of constant-time performance for frequent operations such as add, remove, and contains, it emerges as an efficient choice for a wide range of applications.\n\n\n\n\n\n\n\n\nG\n\n\n\nHashMap\n\nHashMap\n\nKey-Value Pairs\n\nAdd: O(1)\n\nRemove: O(1)\n\nContains: O(1)\n\n\n\nHashTable\n\nHashTable\n\nKey-Value Pairs\n\nAdd: O(1)\n\nRemove: O(1)\n\nContains: O(1)\n\n\n\nHashMap-&gt;HashTable\n\nThread-Safe\nNo Ordering\n\n\n\nHashSet\n\nHashSet\n\nUnique Values\n\nAdd: O(1)\n\nRemove: O(1)\n\nContains: O(1)\n\n\n\nHashTable-&gt;HashSet\n\nNo Key-Value Pairs\nNo Ordering\n\n\n\nHashSet-&gt;HashMap\n\nKey-Value Pairs\nNo Thread-Safety\n\n\n\n\n\n\nFigure 26.8: Comparative Analysis of HashMap, HashTable, and HashSet. This diagram depicts the similarities and differences between HashMap, HashTable, and HashSet in terms of their structure and performance characteristics. It helps visualize how each of them stores data and their efficiency for common operations.",
    "crumbs": [
      "Hashing",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Hashing, Hash Tables, and Hash Maps</span>"
    ]
  },
  {
    "objectID": "22_hashing.html#hashcode-and-equals-in-java",
    "href": "22_hashing.html#hashcode-and-equals-in-java",
    "title": "26  Hashing, Hash Tables, and Hash Maps",
    "section": "26.10 hashCode and equals in Java",
    "text": "26.10 hashCode and equals in Java\nIn the heart of Java’s Object Oriented structure, lies the Object class. It plays the role of the ultimate superclass for all Java classes. Within this class, two methods, hashCode and equals, form the fundamental basis for the interaction of objects in various Java collections. These methods interplay crucially in object comparison and are instrumental in the design and function of hash-based collections such as HashSet, HashMap, and HashTable.\n\n26.10.1 Understanding hashCode\nThe hashCode method provides a means for generating hash codes, which are integers symbolizing the memory address of an object. The method signature is as follows:\npublic int hashCode()\nBy default, the method returns a hash code derived from the object’s memory address. However, subclasses can override this behavior to offer custom hash code generation. For the efficient functioning of hash-based collections, and to maintain object-to-hashcode consistency, a well-defined hashCode method should adhere to these principles:\n\nIf equals() perceives two objects as equal, their hash codes must be identical.\nThe inverse, however, isn’t true: objects with matching hash codes are not necessarily equal according to their equals() method.\nUnless data influencing the equals() method alters, the object’s hash code should remain consistent.\n\n\n\n26.10.2 Overriding the hashCode Method\nFor custom classes, overriding hashCode becomes essential if the equals() method is also overridden. Ensuring that both these methods are in sync upholds the general hashCode contract, and thereby guarantees the smooth operation of hash-based data structures.\nBelow is an instance of a custom Person class, which overrides both equals() and hashCode() methods:\npublic class Person {\n    private String name;\n    private int age;\n\n    // Constructor, getters, and setters\n\n    @Override\n    public boolean equals(Object obj) {\n        if (this == obj) {\n            return true;\n        }\n        if (obj == null || getClass() != obj.getClass()) {\n            return false;\n        }\n        Person person = (Person) obj;\n        return age == person.age && Objects.equals(name, person.name);\n    }\n\n    @Override\n    public int hashCode() {\n        return Objects.hash(name, age);\n    }\n}\nHere, the equals() method verifies if two Person objects share the same name and age. Meanwhile, the hashCode() method leverages the utility function Objects.hash(), which generates a hash code based on the name and age fields.\n\n\n\n\n\n\n\n\nG\n\n\ncluster_0\n\nOverriding hashCode and equals\n\n\ncluster_1\n\nhashCode in HashSet\n\n\n\nPerson1\n\nPerson\n\nname=x\n\nage=y\n\n\n\nPerson2\n\nPerson\n\nname=x\n\nage=y\n\n\n\nPerson1-&gt;Person2\n\n\nequals()\n\n\n\nHashTable\nHashSet Structure\n\n\n\nPerson\n\nPerson\n\nname=x\n\nage=y\n\n\n\nIndex\n\nIndex\n\nhashCode\n\n\n\nPerson-&gt;Index\n\n\nhashCode()\n\n\n\nIndex-&gt;HashTable\n\n\nStores at Index\n\n\n\n\n\n\nFigure 26.9: Understanding hashCode and equals in Java. The first part (top) of this diagram showcases the need for synchronizing hashCode and equals methods in custom classes. The second part (bottom) illuminates the role of hashCode in storing and retrieving objects in hash-based collections like HashSet.\n\n\n\n\n\n\n\n26.10.3 hashCode in the Context of Java Collections\nIn hash-based collections like HashSet, HashMap, and HashTable, the hashCode method takes center stage, boosting their performance through efficient storage and retrieval of objects based on hash codes.\nMaintaining an aptly implemented hashCode method for the objects being stored is paramount when working with these collections. Missteps could result in compromised performance or erroneous behavior.\nIn essence, the hashCode method in Java, which stems from the Object class, provides a default blueprint for hash code generation. While creating custom classes, it is critical to override the hashCode method in sync with the equals() method. This symbiosis between the two methods ensures a seamless operation of hash-based data structures like HashSet and HashMap.\nWhile this section provides an understanding of hashCode and equals in Java, the next chapter will further delve into comparing objects in Java and the intricacies involved therein.",
    "crumbs": [
      "Hashing",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Hashing, Hash Tables, and Hash Maps</span>"
    ]
  }
]